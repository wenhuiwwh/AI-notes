#     Hadoop

##  1. 大数据处理架构Hadoop

### 1.1 概述

#### 1.1.1 Hadoop简介

* Hadoop是Apache软件基金会旗下的一个开源分布式计算平台，为用户提供了系统底层细节透明的分布式基础架构，Hadoop是基于Java语言开发的，具有很好的跨平台特性，并且可以部署在廉价的计算机集群中
* Hadoop的核心是分布式文件系统HDFS（Hadoop Distributed File System）和MapReduce

#### 1.1.2 Hadoop发展简史

1. Hadoop最初是由Apache Lucene项目的创始人Doug Cutting开发的文本搜索库，Hadoop源自始于2002年的Apache Nutch项目——一个开源的网络搜索引擎并且也是Lucene项目的一部分
2. 在2004年，Nutch项目也模仿GFS开发了自己的分布式文件系统NDFS (Nutch Distributed File System) ，也就是HDFS的前身
3. 2005年，Nutch开源实现了谷歌的MapReduce

#### 1.1.3 Hadoop的特性

* 高可用性
* 高效性
* 高扩展性
* 高容错性
* 成本低
* 运行在Linux平台上
* 支持多种编程语言

#### 1.1.4 Hadoop的应用现状

国内采用Hadoop的公司主要有百度、淘宝、网易、华为、中国移动等，其中，淘宝的Hadoop集群比较大

 <img src="imgs/1570935751619.png" alt="1570935751619" style="zoom:50%;" />

### 1.2 Hadoop项目结构

 <img src="imgs/1570935862668.png" alt="1570935862668" style="zoom:80%;" />

### 1.3 Hadoop的安装与使用

[《大数据技术原理与应用 第二章 大数据处理架构Hadoop 学习指南》](http://dblab.xmu.edu.cn/blog/285)

**创建Hadoop用户**

如果安装 Ubuntu 的时候不是用的 “hadoop” 用户，那么需要增加一个名为 hadoop 的用户

```shell
$ sudo useradd –m hadoop –s /bin/bash
```

接着使用如下命令设置密码，可简单设置为 hadoop，按提示输入两次密码：

```shell
$ sudo passwd hadoop
```

可为 hadoop 用户增加管理员权限，方便部署，避免一些对新手来说比较棘手的权限问题

```shell
$ sudo adduser hadoop sudo
```

### 1.4 Hadoop集群的部署与使用

#### 1.4.1 集群的建立与自动化安装

为了缓解安装和维护每个节点上相同的软件的负担，可以使用一个自动化方法实现完全自动化安装，比如Red Hat Linux’ Kickstart、Debian或者Docker自动化安装部署工具，会通过记录在安装过程中对于各个选项的回答来完成自动化安装过程。

#### 1.4.2 Hadoop集群基准测试

如何判断一个Hadoop集群是否已经正确安装？

* Hadoop自带有一些基准测试程序，被打包在测试程序JAR文件中，可以运行基准测试，用TestDFSIO基准测试，来测试HDFS的IO性能

* 用排序测试 MapReduce：Hadoop自带一个部分排序的程序，这个测试过程的整个数据集都会通过洗牌 (Shuffle) 传输至Reducer，可以充分测试MapReduce的性能

#### 1.4.3 在云环境中使用Hadoop

* 可以在Amazon EC2中运行Hadoop。EC2是一个计算服务，允许客户租用计算机（实例），来运行自己的应用。客户可以按需运行或终止实例，并且按照实际使用情况来付费
* 在EC2上运行Hadoop尤其适用于一些工作流。例如，在Amazon S3中存储数据，在EC2上运行集群，在集群中运行MapReduce作业，读取存储在S3中的数据，最后，在关闭集群之前将输出写回S3中；如果长期使用集群，复制S3数据到运行在EC2上的HDFS中，则可以使得数据处理更加高效，因为，HDFS可以充分利用数据的位置，S3则做不到，因为，S3与EC2的存储不在同一个节点上



## 2. 分布式文件系统HDFS

### 2.1 分布式文件系统

#### 2.1.1 计算机集群结构

计算机集群由成千上万的计算机节点构成，分布式文件系统就是将文件分布存储到多个计算机节点上。

分布式文件系统是由普通硬件构成，大大降低了硬件上的开销。

#### 2.1.2 分布式文件系统的结构

节点主要分为主节点（Master Node），也叫**名称节点（NameNode）**和从节点（slave Node）也叫**数据节点（NameNode）**

### 2.2 HDFS简介

**实现的目标：**

* 兼容廉价的硬件设备
* 流数据读写
* 大数据集
* 简单的文件模型
* 强大的跨平台兼容性

**局限性：**

* 不适合低延迟数据访问

* 无法高效存储大量小文件
* 不支持多用户写入及任意修改文件

### 2.3 HDFS相关概念

#### 2.3.1 块

HDFS默认一个块大小64MB，一个文件被分成多个块，以块作为存储单元，可最小化寻址开销

<font color='red'>好处</font>：

* 支持大规模文件存储：单个文件的大小不会受到单个节点存储容量的限制
* 简化系统设计：文件块大小固定，很容易计算出单个节点可以存储的块数，其次，方便了元数据的管理，元数据不需要和文件块一起存储，可以由其他系统负责管理元数据
* 适合数据备份：每个文件块都可以冗余存储多个节点，大大提高了系统的容错性和可用性

**HDFS主要组件的功能**

| NameNode                                | DataNode                                   |
| --------------------------------------- | ------------------------------------------ |
| 存储元数据                              | 存储文件内容                               |
| 元数据保存在内存中                      | 文件内容保存在磁盘                         |
| 保存文件，block，datanode之间的映射关系 | 维护了block id到datanode本地文件的映射关系 |

#### 2.3.2 名称节点和数据节点

<font color='red'>名称节点的数据结构</font>

在HDFS中，名称节点负责管理分布式文件系统的命名空间，保存了两个核心的数据结构，即**FsImage**和**EditLog**

* FsImage：用于维护文件系统树以及文件树中所有的文件和文件夹的元数据

* EditLog：记录了所有针对文件的创建、删除、重命名等操作

名称节点记录了每个文件中各个块所在的数据节点的位置信息

<font color='red'>FsImage文件</font>

* 该文件包含文件系统中所有目录和文件inode的序列化形式
  * inode：是一个文件或目录的元数据的内部表示，并包含此类信息：文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。
  * 目录：存储修改时间、权限和配置元数据

该文件没有记录块存储在哪个数据节点。而是由名称节点把这些映射保留在内存中，当数据节点加入HDFS集群时，数据节点会把自己包含的块列表告知名称节点，此后会一直保持这种告知信息，以确保最新状态。

<font color='red'>名称节点的启动</font>

名称节点启动时，会将 FsImage 文件中的内容加载到内存中，之后再执行 EditLog 文件中的各项操作，使得内存中的元数据和实际的同步，存在内存中的元数据支持客户端的读操作。

一旦在内存中成功建立文件系统元数据的映射，则创建一个新的 FsImage 文件和一个空的 EditLog 文件。

名称节点起来之后，HDFS 中的更新操作会重新写到 EditLog 文件中，FsImage 文件一般很大，不可能将所有的更新操作都往 FsImage 文件中写，这样会导致系统运行十分缓慢，但 EditLog 文件小很多，每次执行写操作之后和向客户端发送成功代码之前，EditLog 文件都需要同步更新。

<font color='red'>名称节点运行期间EditLog不断变大问题的解决</font>

名称节点运行过程中，EditLogs 文件会不断增大，虽然对名称节点的运行不会产生影响，但若是重启，则名称节点加载 FsImage 后，会一条条执行 EditLog 中的记录，导致名称节点启动非常缓慢，由于此时处在安全模式，一直无法对外提供写操作，影响了用户使用。

**解决方式：提供SecondaryNameNode(第二名称节点)**

用来保存名称节点中对HDFS元数据信息的备份，减少名称节点重启的时间，一般单独运行在一台机器上

工作情况：

1. SecondaryNameNode会定期和NameNode通信，请求其停止使用EditLog文件，暂时将新的写操作写到一个新的文件edit.new上来，这个操作是瞬间完成，上层写日志的函数完全感觉不到差别；

2、SecondaryNameNode通过HTTP GET方式从NameNode上获取到FsImage和EditLog文件，并下载到本地的相应目录下；

3、SecondaryNameNode将下载下来的FsImage载入到内存，然后一条一条地执行EditLog文件中的各项更新操作，使得内存中的FsImage保持最新；这个过程就是EditLog和FsImage文件合并；

4、SecondaryNameNode执行完（3）操作之后，会通过post方式将新的FsImage文件发送到NameNode节点上

5、NameNode将从SecondaryNameNode接收到的新的FsImage替换旧的FsImage文件，同时将edit.new替换EditLog文件，通过这个过程EditLog就变小了

<font color='red'>数据节点</font>

数据节点是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者是名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块列表

每个数据节点中的数据会被保存在各自节点的本地Linux文件系统中

### 2.4 HDFS体系结构

#### 2.4.1 HDFS体系结构概述

采用主从结构模型，一个HDFS集群包含一个名称节点和若干个数据节点，名称节点作为中心服务器，负责管理文件系统的命名空间及客户端对文件的访问，集群中的数据节点一般是一个节点运行一个数据节点进程，负责处理文件系统客户端的读/写请求，在名称节点的统一调度下进行数据块的创建、删除和复制等操作。每个数据节点的数据实际上是保存在本地Linux文件系统中的。

#### 2.4.2 HDFS 命名空间管理

HDFS的命名空间包含目录、文件和块

在HDFS1.0体系结构中，整个集群有且只有一个命名空间和一个名称节点，该节点负责对这个命名空间进行管理

HDFS使用的是传统的分级文件体系，因此，用户可以像使用普通文件系统一样，创建、删除目录和文件，在目录间转移文件，重命名文件等

#### 2.4.3 通信协议

数据需要通过协议进行传输

所有的HDFS通信协议都是建立在TCP/IP协议基础上的

客户端通过一个可配置的端口向名称节点主动发送TCP连接，使用**客户端协议**与名称节点进行交互

名称节点和数据节点之间则使用**数据节点协议**进行交互

客户端与数据节点的交互是通过RPC来实现的，在设计上，名称节点不会主动发起RPC，而是响应来自客户端和数据节点的RPC请求

#### 2.4.4 客户端

客户端是用户操作HDFS最常用的方式，HDFS在部署时都提供了客户端

HDFS客户端是一个库，暴露了HDFS文件系统接口，这些接口隐藏了HDFS实现中的大部分复杂性

客户端可以支持打开、读取、写入等常见的操作，并且提供了类似Shell的命令行方式来访问HDFS中的数据

HDFS也提供了Java API，作为应用程序访问文件系统的客户端编程接口

#### 2.4.5 HDFS体系结构的局限性

**命名空间的限制：**名称节点是保存在内存中的，因此，名称节点能够容纳的对象（文件、块）的个数会受到内存空间大小的限制。

**性能的瓶颈：**整个分布式文件系统的吞吐量，受限于单个名称节点的吞吐量。

**隔离问题：**由于集群中只有一个名称节点和一个命名空间，因此，无法对不同应用程序进行隔离

**集群的可用性：**一旦这个唯一的名称节点发生故障，会导致整个集群变得不可用

### 2.5 HDFS存储原理

#### 3.5.1 冗余数据保存

为了保证系统的容错性和可用性，HDFS采用了多副本方式对数据进行冗余存储，通常一个数据块的多个副本会被分布到不同的数据节点上

**优点**

* 加快数据传输速度
* 容易检查数据错误
* 保证数据可靠性

#### 2.5.2 数据存取策略

**1. 数据存放**

* 第一个副本：放置在上传文件的数据节点；如果是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点
* 第二个副本：放置在与第一个副本不同的机架的节点上
* 第三个副本：与第一个副本相同机架的其他节点上
* 更多副本：随机节点

**2.数据读取**

HDFS提供了一个 API 可以确定一个数据节点所属的机架 ID，客户端也可以调用 API 获取自己所属的机架 ID

当客户端读取数据时，从名称节点获得数据块不同副本的存放位置列表，列表中包含了副本所在的数据节点，可以调用API来确定客户端和这些数据节点所属的机架ID，当发现某个数据块副本对应的机架ID和客户端对应的机架ID相同时，就优先选择该副本读取数据，如果没有发现，就随机选择一个副本读取数据

#### 3.5.3 数据错误与恢复

HDFS具有较高的容错性，可以兼容廉价的硬件，它把硬件出错看作一种常态，而不是异常

**1. 名称节点出错**

HDFS设置了备份机制，把FsImage和Editlog这些核心文件同步复制到备份服务器SecondaryNameNode上

**2. 数据节点出错**

1、每个数据节点会定期向名称节点发送“心跳”信息，向名称节点报告自己的状态

2、当数据节点发生故障，或网络发生断网时，名称节点就无法收到来自一些数据节点的心跳信息，这时，这些数据节点就会被标记为“宕机”，节点上面的所有数据都会被标记为“不可读”，名称节点不会再给它们发送任何I/O请求

3、这时，有可能出现一种情形，即由于一些数据节点的不可用，会导致一些数据块的副本数量小于冗余因子

4、名称节点会定期检查这种情况，一旦发现某个数据块的副本数量小于冗余因子，就会启动数据冗余复制，为它生成新的副本

**3. 数据出错**

网络传输和磁盘错误等因素，都会造成数据错误

1、客户端在读取到数据后，会采用md5和sha1对数据块进行校验，以确定读取到正确的数据

2、在文件被创建时，客户端就会对每一个文件块进行信息摘录，并把这些信息写入到同一个路径的隐藏文件里面

3、当客户端读取文件的时候，会先读取该信息文件，然后，利用该信息文件对每个读取的数据块进行校验，如果校验出错，客户端就会请求到另外一个数据节点读取该文件块，并且向名称节点报告这个文件块有错误，名称节点会定期检查并且重新复制这个块

### 2.6 HDFS数据读写过程

#### 3.6.1 读数据的过程

![1569983898597](imgs/1569983898597.png)

```java
import java.io.BufferedReader;
import java.io.InputStreamReader ;
import org.apache.hadoop.conf.Configuration ;
import org.apache.hadoop.fs.FileSystem ;
import org.apache.hadoop.fs.Path ;
import org.apache.hadoop.fs.FSDataInputStream ;
public class Chapter3 {
    public static void main(String[] args) {
        try {
            Configuration conf = new Configuration();
            FileSystem fs = FileSystem.get(conf);
            Path filename = new Path(“hdfs://localhost:9000/user/hadoop/test.txt");
            FSDataInputStream is = fs.open(filename);
            BufferedReader d = new BufferedReader(new InputStreamReader(is));
            String content = d.readLine(); //读取文件一行
            System.out.println(content);
            d.close(); //关闭文件
            fs.close(); //关闭hdfs
            } catch (Exception e) {
                e.printStackTrace();
            }
      }
}
```

#### 2.6.2 写数据的过程

![1569984014198](imgs/1569984014198.png)

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.Path;
public class Chapter3 {
    public static void main(String[] args) {
        try {
            Configuration conf = new Configuration();
            FileSystem fs = FileSystem.get(conf);
            byte[] buff = "Hello world".getBytes(); // 要写入的内容
            String filename = " hdfs://localhost:9000/user/hadoop/test.txt "; //要写入的文件名
            FSDataOutputStream os = fs.create(new Path(filename))
            os.write(buff,0,buff.length);
            System.out.println("Create:"+ filename);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```

**类说明**

`FileSystem`：是一个通用文件系统的抽象基类，可以被分布式文件系统继承，所有可能使用Hadoop文件系统的代码，都要使用这个类
Hadoop为 `FileSystem` 这个抽象类提供了多种具体实现
`DistributedFileSystem`：是 `FileSystem` 在HDFS文件系统中的具体实现
`FileSystem` 的 `open()` 方法返回的是一个输入流 `FSDataInputStream` 对象，在HDFS文件系统中，具体的输入流就是 `DFSInputStream`；`FileSystem ` 中的 `create()` 方法返回的是一个输出流 `FSDataOutputStream` 对象,在 HDFS文件系统中，具体的输出流就是 `DFSOutputStream`

> 创建一个 `Configuration`对象时，其构造方法会默认加载工程项目下两个配置文件，分别是hdfs-site.xml以及core-site.xml，这两个文件中会有访问HDFS所需的参数值，主要是fs.defaultFS，指定了HDFS的地址（比如hdfs://localhost:9000），有了这个地址客户端就可以通过这个地址访问HDFS了

### 2.7 HDFS编程实践

在学习HDFS编程实践前，我们需要启动Hadoop。执行如下命令：

```shell
cd /usr/local/hadoop
./bin/hdfs namenode -format # 格式化Hadoop的hdfs文件系统
./sbin/start-dfs.sh # 启动Hadoop
```

#### 3.7.1 HDFS常用命令

HDFS有很多shell命令，其中，fs命令可以说是HDFS最常用的命令。利用该命令可以查看HDFS文件系统的目录结构、上传和下载数据、创建文件等。该命令的用法为：
hadoop fs [genericOptions] [commandOptions]

备注：Hadoop中有三种Shell命令方式：

* hadoop fs：适用于任何不同的文件系统，比如本地文件系统和HDFS文件系统
* hadoop dfs：只能适用于HDFS文件系统
* hdfs dfs：只能适用于HDFS文件系统

实例：

`hadoop fs -ls <path>`：显示\<path>指定的文件的详细信息
`hadoop fs -mkdir <path>` :创建\<path>指定的文件夹
`hadoop fs -cat <path>` ：将\<path>指定的文件的内容输出到标准输出（stdout）
`hadoop fs -copyFromLocal <localsrc> <dst>`  ：将本地源文件\<localsrc>复制到路径\<dst>指定的文件或文件夹中

#### 2.7.2 HDFS与Java API及应用实例

具体请参见：

[《大数据技术原理与应用 第三章 Hadoop分布式文件系统 学习指南》](http://dblab.xmu.edu.cn/blog/290-2/)

## 3. 分布式数据库HBase

### 3.1 概述

#### 3.1.1 从BigTable说起

BigTable是一个起初用于解决典型互联网搜索问题的分布式存储系统，利用谷歌提出的MapReduce分布式并行计算模型来处理海量数据，使用谷歌分布式文件系统GFS作为底层数据存储，采用Chubby提供协同服务管理

#### 3.1.2 HBase简介

HBase是一个高可靠、高性能、面向列、可伸缩的分布式数据库，是谷歌BigTable的开源实现，主要用来存储非结构化和半结构化的松散数据。HBase的目标是处理非常庞大的表，可以通过水平扩展的方式。

 <img src="imgs/1570628030186.png" alt="1570628030186" style="zoom:50%;" />          <img src="imgs/1570628062199.png" alt="1570628062199" style="zoom:50%;" />

**为什么需要HBase?**

* Hadoop可以很好地解决大规模数据的离线批量处理问题，但是，受限于Hadoop MapReduce编程框架的高延迟数据处理机制，使得Hadoop无法满足大规模数据实时处理应用的需求
* HDFS面向批量访问模式，不是随机访问模式
* 传统的通用关系型数据库无法应对在数据规模剧增时导致的系统扩展性和性能问题(分库分表也不能很好解决)
* 传统关系数据库在数据结构变化时一般需要停机维护；空列浪费存储空间

#### 3.1.3 HBase与传统关系型数据库的对比分析

* 数据类型：关系数据库采用关系模型，具有丰富的数据类型和存储方式，HBase则采用了更加简单的数据模型，它把数据存储为未经解释的字符串
* 数据操作：关系数据库中包含了丰富的操作，其中会涉及复杂的多表连接。HBase操作则不存在复杂的表与表之间的关系，只有简单的插入、查询、删除、清空等，因为HBase在设计上就避免了复杂的表和表之间的关系
* 存储模式：关系数据库是基于行模式存储的。HBase是基于列存储的，每个列族都由几个文件保存，不同列族的文件是分离的
* 数据索引：关系数据库通常可以针对不同列构建复杂的多个索引，以提高数据访问性能。HBase只有一个索引——行键，通过巧妙的设计，HBase中的所有访问方法，或者通过行键访问，或者通过行键扫描，从而使得整个系统不会慢下来
* 数据维护：在关系数据库中，更新操作会用最新的当前值去替换记录中原来的旧值，旧值被覆盖后就不会存在。而在HBase中执行更新操作时，并不会删除数据旧的版本，而是生成一个新的版本，旧有的版本仍然保留
* 可伸缩性：关系数据库很难实现横向扩展，纵向扩展的空间也比较有限。相反，HBase和BigTable这些分布式数据库就是为了实现灵活的水平扩展而开发的，能够轻易地通过在集群中增加或者减少硬件数量来实现性能的伸缩

### 3.2 HBase访问接口

| 类别            | 特点                                                 | 场合                                          |
| --------------- | ---------------------------------------------------- | --------------------------------------------- |
| Native Java API | 最常规和高效的访问方式                               | 适合Hadoop MapReduce作业并行批处理HBase表数据 |
| HBase Shell     | HBase的命令行工具，最简单的接口                      | 适合HBase管理使用                             |
| Thrift Gateway  | 利用Thrift序列化技术，支持C++、PHP、Python等多种语言 | 适合其他异构系统在线访问HBase表数据           |
| REST Gateway    | 解除了语言限制                                       | 支持REST风格的Http API访问HBase               |
| Pig             | 使用Pig Latin流式编程语言来处理HBase中的数据         | 适合做数据统计                                |
| Hive            | 简单                                                 | 当需要以类似SQL语言方式来访问HBase的时候      |

### 3.3 HBase数据模型

#### 3.3.1 数据模型概述

HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、列族、列限定符和时间戳，每个值是一个未经解释的字符串，没有数据类型，用户在表中存储数据，每一行都有一个可排序的行键和任意多的列，

表在水平方向由一个或者多个列族组成，一个列族可以包含任意多个列，同一个列族里面的数据存储在一起列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行数据类型转换

#### 3.3.2 数据模型相关概念

* 表：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族
* 行：每个HBase表都由若干行组成，每个行由行键（row key）来标识
* 列族：一个HBase表被分组成许多“列族”(Column Family)的集合，它是基本的访问控制单元
* 列限定符：列族里的数据通过列限定符(或列)来定位
* 单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”(cell)，单元格中存储的数据没有数据类型，总被视为字节数组byte[]
* 时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引

#### 3.3.3 数据坐标

HBase中需要根据行键、列族、列限定符和时间戳来确定一个单元格，因此，可以视为一个“四维坐标”，即[行键，列族，列限定符，时间戳]

#### 3.3.4 概念视图

 <img src="imgs/1570631828076.png" alt="1570631828076" style="zoom:50%;" />

#### 3.3.5 物理视图

 <img src="imgs/1570631850926.png" alt="1570631850926" style="zoom:50%;" />   <img src="imgs/1570631878495.png" alt="1570631878495" style="zoom:50%;" />    

### 3.4 HBase的实现原理

#### 3.4.1 HBase功能组件

HBase的实现包括三个主要的功能组件

* 库函数：链接到每个客户端
* 一个Master主服务器
* 许多个Region服务器

主服务器Master负责管理和维护HBase表的分区信息，维护Region服务器列表，分配Region，负载均衡

Region服务器负责存储和维护分配给自己的Region，处理来自客户端的读写请求

客户端并不是直接从Master主服务器上读取数据，而是在获得Region的存储位置信息后，直接从Region服务器上读取数据

客户端并不依赖Master，而是通过Zookeeper来获得Region位置信息，大多数客户端甚至从来不和Master通信，这种设计方式使得Master负载很小

#### 3.4.2 表和Region

 <img src="imgs/1570632194704.png" alt="1570632194704" style="zoom:50%;" />           <img src="imgs/1570632264570.png" alt="1570632264570" style="zoom:50%;" />

* 开始只有一个Region，后来不断分裂
* Region拆分操作非常快，接近瞬间，因为拆分之后的Region读取的仍然是原存储文件，直到“合并”过程把存储文件异步地写到独立的文件之后，才会读取新文件
* 每个Region默认大小是100MB到200MB（2006年以前的硬件配置）
  * 每个Region的最佳大小取决于单台服务器的有效处理能力
  * 目前每个Region最佳大小建议1GB-2GB（2013年以后的硬件配置）
* 同一个Region不会被分拆到多个Region服务器
* 每个Region服务器存储10-1000个Region

 <img src="imgs/1570632436508.png" alt="1570632436508" style="zoom:50%;" />

#### 3.4.3 Region的定位

元数据表，又名.META.表，存储了Region和Region服务器的映射关系

当HBase表很大时， .META.表也会被分裂成多个Region

根数据表，又名-ROOT-表，记录所有元数据的具体位置

-ROOT-表只有唯一一个Region，名字是在程序中被写死的

Zookeeper文件记录了-ROOT-表的位置

 <img src="imgs/1570632551328.png" alt="1570632551328" style="zoom:50%;" />    <img src="imgs/1570632576483.png" alt="1570632576483" style="zoom:48%;" />

为了加快访问速度，.META.表的全部Region都会保存在内存中

假设.META.表的每行(一个映射条目)在内存中大约占用1KB，并且每个Region限制为128MB，那么，上面的三层结构可以保存的用户数据表的Region数目的计算方法是：
$$
(-ROOT-表能够寻址的.META.表的Region个数)×(每个.META.表的Region可以寻址的用户数据表的Region个数)
$$
一个-ROOT-表最多只能有一个Region，也就是最多只能有128MB，按照每行(一个映射条目)占用1KB内存计算，128MB空间可以容纳 $128MB/1KB=2^{17}$ 行，也就是说一个ROOT表可以寻址$2^{17}$个.META.表的Region

同理，每个.META.表的Region可以寻址的用户数据表的Region个数是$128MB/1KB=2^{17}$。

最终，三层结构可以保存的Region数目是$(128MB/1KB) × (128MB/1KB) = 2^{34}$个Region

客户端访问数据时的“三级寻址”

为了加速寻址，客户端会缓存位置信息，同时，需要解决缓存失效问题

寻址过程客户端只需要询问Zookeeper服务器，不需要连接Master服务器

### 3.5 HBase运行机制

#### 3.5.1 HBase系统架构

**1. 客户端**

客户端包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程

**2. Zookeeper服务器**

Zookeeper可以帮助选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题

**3. Master**

主服务器Master主要负责表和Region的管理工作：

* 管理用户对表的增加、删除、修改、查询等操作
* 实现不同Region服务器之间的负载均衡
* 在Region分裂或合并后，负责重新调整Region的分布
* 对发生故障失效的Region服务器上的Region进行迁移

**4. Region服务器**

Region服务器是HBase中最核心的模块，负责维护分配给自己的Region，并响应用户的读写请求

#### 3.5.2 Region服务器工作原理

 <img src="imgs/1570664125918.png" alt="1570664125918" style="zoom:80%;" />

**1. 用户读写数据过程**

* 用户写入数据时，被分配到相应Region服务器去执行

* 用户数据首先被写入到MemStore和Hlog中

* 只有当操作写入Hlog之后，commit()调用才会将其返回给客户端

* 当用户读取数据时，Region服务器会首先访问MemStore缓存，如果找不到，再去磁盘上面的StoreFile中寻找

**2. 缓存的刷新**

* 系统会周期性地把 MemStore 缓存里的内容刷写到磁盘的StoreFile文件中，清空缓存，并在Hlog里面写入一个标记
* 每次刷写都生成一个新的StoreFile文件，因此，每个Store包含多个StoreFile文件
* 每个Region服务器都有一个自己的HLog 文件，每次启动都检查该文件，确认最近一次执行缓存刷新操作之后是否发生新的写入操作；如果发现更新，则先写入MemStore，再刷写到StoreFile，最后删除旧的Hlog文件，开始为用户提供服务

**3. StoreFile的合并**

* 每次刷写都生成一个新的StoreFile，数量太多，影响查找速度
* 调用Store.compact()把多个合并成一个
* 合并操作比较耗费资源，只有数量达到一个阈值才启动合并

#### 3.5.3 Store工作原理

* Store是Region服务器的核心

* 多个StoreFile合并成一个

* 单个StoreFile过大时，又触发分裂操作，1个父Region被分裂成两个子Region

 <img src="imgs/1570664528027.png" alt="1570664528027" style="zoom:80%;" />

#### 3.5.4 HLog工作原理

分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复

HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log）

用户更新数据必须首先写入日志后，才能写入MemStore缓存，并且，直到MemStore缓存内容对应的日志已经写入磁盘，该缓存内容才能被刷写到磁盘

Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master

Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录

系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region服务器中，并把与该Region对象相关的HLog日志记录也发送给相应的Region服务器

Region服务器领取到分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复

共用日志优点：提高对表的写操作性能；缺点：恢复时需要分拆日志

### 3.6 HBase应用方案

#### 3.6.1 HBase实际应用中的性能优化方法

**行键**

行键是按照字典序存储，因此，设计行键时，要充分利用这个排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。

举个例子：如果最近写入HBase表中的数据是最可能被访问的，可以考虑将时间戳作为行键的一部分，由于是字典序排序，所以可以使用 `Long.MAX_VALUE -timestamp` 作为行键，这样能保证新写入的数据在读取时可以被快速命中

**InMemory**

创建表的时候，可以通过HColumnDescriptor.setInMemory(true)将表放到Region服务器的缓存中，保证在读取的时候被cache命中。

**Max Version**

创建表的时候，可以通过 `HColumnDescriptor.setMaxVersions(int maxVersions)` 设置表中数据的最大版本，如果只需要保存最新版本的数据，那么可以设置 `setMaxVersions(1)`。

**Time To Live**

创建表的时候，可以通过HColumnDescriptor.setTimeToLive(int timeToLive)设置表中数据的存储生命期，过期数据将自动被删除，例如如果只需要存储最近两天的数据，那么可以设置setTimeToLive(2 * 24 * 60 * 60)。

#### 3.6.2 HBase性能监视

**Master-status(自带)**

HBase Master默认基于Web的UI服务端口为60010，HBase region服务器默认基于Web的UI服务端口为60030，如果master运行在名为master.foo.com的主机中，mater的主页地址就是http://master.foo.com:60010，用户可以通过Web浏览器输入这个地址查看HBase集群的当前状态

**Ganglia**

Ganglia是UC Berkeley发起的一个开源集群监视项目，用于监控系统性能

**OpenTSDB**

OpenTSDB可以从大规模的集群（包括集群中的网络设备、操作系统、应用程序）中获取相应的metrics并进行存储、索引以及服务，从而使得这些数据更容易让人理解，如web化，图形化等

**Ambari**

Ambari 的作用就是创建、管理、监视 Hadoop 的集群

#### 3.6.3 在HBase之上构建SQL引擎

NoSQL区别于关系型数据库的一点就是NoSQL不使用SQL作为查询语言，至于为何在NoSQL数据存储HBase上提供SQL接口，有如下原因：

1. 易使用。使用诸如SQL这样易于理解的语言，使人们能够更加轻松地使用HBase。

2. 减少编码。使用诸如SQL这样更高层次的语言来编写，减少了编写的代码量。 

**方案**

**1. Hive整合HBase**

Hive与HBase的整合功能从Hive0.6.0版本已经开始出现，利用两者对外的API接口互相通信，通信主要依靠` hive_hbase-handler.jar`工具包(Hive Storage Handlers)。由于HBase有一次比较大的版本变动，所以并不是每个版本的Hive都能和现有的HBase版本进行整合，所以在使用过程中特别注意的就是两者版本的一致性

**2. Phoenix**

Phoenix由Salesforce.com开源，是构建在Apache HBase之上的一个SQL中间层，可以让开发者在HBase上执行SQL查询。

#### 3.6.4 构建HBase二级索引 

HBase只有一个针对行健的索引访问HBase表中的行，只有三种方式:

1. 通过单个行健访问
2. 通过一个行健的区间来访问
3. 全表扫描

原理：采用HBase0.92版本之后引入的Coprocessor特性

Coprocessor构建二级索引，提供了两个实现：endpoint和observer，endpoint相当于关系型数据库的存储过程；observer则相当于触发器，允许我们在记录put前后做一些处理，因此，而我们可以在插入数据时同步写入索引表

**优点**：非侵入性，引擎构建在HBase之上，既没有对HBase进行任何改动，也不需要上层应用做任何妥协

**缺点**：每插入一条数据需要向索引表插入数据，即耗时是双倍的，对HBase的集群的压力也是双倍的

 <img src="imgs/1570717173460.png" alt="1570717173460" style="zoom:50%;" />

使用其他产品为HBase行健提供索引功能:

**1. Hindex二级索引**

Hindex 是华为公司开发的纯 Java 编写的HBase二级索引，兼容 Apache HBase 0.94.8。当前的特性如下：

* 多个表索引
* 多个列索引
* 基于部分列值的索引

**2. HBase+Redis**

Coprocessor构建二级索引，Redis做客户端缓存，将索引实时更新到Redis等KV系统中，定时从KV更新索引到HBase的索引表中

 <img src="imgs/1570717351711.png" alt="1570717351711" style="zoom:50%;" />

**3. HBase+solr**

Solr是一个高性能，采用Java5开发，基于Lucene的全文搜索服务器。同时对其进行了扩展，提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展并对查询性能进行了优化，并且提供了一个完善的功能管理界面，是一款非常优秀的全文搜索引擎。

 <img src="imgs/1570717452447.png" alt="1570717452447" style="zoom:50%;" />

### 3.7 HBase编程实践

HBase上机实践，请参考：[《大数据原理与应用 第四章 分布式数据库HBase 学习指南》](
http://dblab.xmu.edu.cn/blog/588-2/)

#### 3.7.1 HBase的安装与配置

**1. HBase安装**

* 下载安装包hbase-1.1.2-bin.tar.gz
* 解压安装包hbase-1.1.2-bin.tar.gz至路径 /usr/local
* 配置系统环境,将hbase下的bin目录添加到系统的path中

**2. HBase配置**

HBase有三种运行模式，单机模式、伪分布式模式、分布式模式。以下先决条件很重要

* JDK
* Hadoop（单机模式不需要，伪分布式模式和分布式模式需要）
* SSH

启动关闭Hadoop和HBase的顺序一定是：启动Hadoop—>启动HBase—>关闭HBase—>关闭Hadoop

`HBASE_MANAGES_ZK=true`，则由HBase自己管理Zookeeper否则，启动独立的Zookeeper

> 建议：单机版HBase，使用自带Zookeeper；集群安装HBase则采用单独Zookeeper集群

#### 3.7.2 HBase常用Shell命令

**1. 创建表**

```shell
 # 创建表
 create 'student','Sname','Ssex','Sage','Sdept','course'
 # 查看表
 describe 'student'
```

即创建了一个“student”表，属性有：Sname、Sex等，行键默认为put命令操作中表名后第一个数据

**2. 添加及更新数据**

HBase中用put命令添加及更新数据，注意：一次只能为一个表的一行数据的一个列，也就是一个单元格添加一个数据，所以直接用shell命令插入数据效率很低，在实际应用中，一般都是利用编程操作数据

```shell
put 'student','95001','Sname','LiYing'
put 'student','95001','course:math','80'
```

**3. 删除数据**

在HBase中用delete以及deleteall命令进行删除数据操作

```shell
# 删除一个数据
delete 'student','95001','Ssex'
# 删除一行数据
deleteall 'student','95001'
# 查看一行数据
get 'student','95001'
# 查看表的所有数据
scan 'student'
```

**4. 删除表**

```shell
# 使表不可用
disable 'student'  
# 删除表
drop 'student'
```

**5. 查询历史数据**

```shell
# 指定保存的版本数（假设指定为5）
create 'teacher',{NAME=>'username',VERSIONS=>5}
# 插入数据然后更新数据，使其产生历史版本数据
put 'teacher','91001','username','Mary'
put 'teacher','91001','username','Mary1'
put 'teacher','91001','username','Mary2'
put 'teacher','91001','username','Mary3'
put 'teacher','91001','username','Mary4'  
put 'teacher','91001','username','Mary5'
# 查询时，指定查询的历史版本数。默认会查询出最新的数据。（有效取值为1到5）
 get 'teacher','91001',{COLUMN=>'username',VERSIONS=>3}
```

退出HBase数据库操作：`exit`

#### 3.7.3 HBase常用Java API及应用实例

这里只需要导入hbase安装目录中的lib文件中的所有jar包

**建立连接**

```java
//建立连接
public static void init(){
    configuration  = HBaseConfiguration.create();
    configuration.set("hbase.rootdir","hdfs://localhost:9000/hbase");
    try{
        connection = ConnectionFactory.createConnection(configuration);
        admin = connection.getAdmin();
    }catch (IOException e){
        e.printStackTrace();
    }
}
```

**关闭连接**

```java
//关闭连接
public static void close(){
    try{
        if(admin != null){
            admin.close();
        }
        if(null != connection){
            connection.close();
        }
    }catch (IOException e){
        e.printStackTrace();
    }
}
```

**1. 建表**

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.*;
import org.apache.hadoop.hbase.client.*;
import java.io.IOException;
/**
     * @param myTableName 表名
     * @param colFamily 列族名
     * @throws IOException
     */
    public static void createTable(String myTableName,String[] colFamily) throws IOException {
 
        init();
        TableName tableName = TableName.valueOf(myTableName);
 
        if(admin.tableExists(tableName)){
            System.out.println("talbe is exists!");
        }else {
            HTableDescriptor hTableDescriptor = new HTableDescriptor(tableName);
            for(String str:colFamily){
                HColumnDescriptor hColumnDescriptor = new HColumnDescriptor(str);
                hTableDescriptor.addFamily(hColumnDescriptor);
            }
            admin.createTable(hTableDescriptor);
            System.out.println("create table success");
        }
        close();
    }
```

**2.  删表**

```java
/**
     * 删除指定表
     * @param tableName 表名
     * @throws IOException
     */
    public static void deleteTable(String tableName) throws IOException {
        init();
        TableName tn = TableName.valueOf(tableName);
        if (admin.tableExists(tn)) {
            admin.disableTable(tn);
            admin.deleteTable(tn);
        }
        close();
    }
```

**3.  查表**

```java
 /**
     * 查看已有表
     * @throws IOException
     */
    public static void listTables() throws IOException {
        init();
        HTableDescriptor hTableDescriptors[] = admin.listTables();
        for(HTableDescriptor hTableDescriptor :hTableDescriptors){
            System.out.println(hTableDescriptor.getNameAsString());
        }
        close();
    }
```

**4.  插数据**

```java
/**
     * 向某一行的某一列插入数据
     * @param tableName 表名
     * @param rowKey 行键
     * @param colFamily 列族名
     * @param col 列名（如果其列族下没有子列，此参数可为空）
     * @param val 值
     * @throws IOException
     */
    public static void insertRow(String tableName,String rowKey,String colFamily,String col,String val) throws IOException {
        init();
        Table table = connection.getTable(TableName.valueOf(tableName));
        Put put = new Put(rowKey.getBytes());
        put.addColumn(colFamily.getBytes(), col.getBytes(), val.getBytes());
        table.put(put);
        table.close();
        close();
    }
```

**5.  删数据**

```java
 /**
     * 删除数据
     * @param tableName 表名
     * @param rowKey 行键
     * @param colFamily 列族名
     * @param col 列名
     * @throws IOException
     */
    public static void deleteRow(String tableName,String rowKey,String colFamily,String col) throws IOException {
        init();
        Table table = connection.getTable(TableName.valueOf(tableName));
        Delete delete = new Delete(rowKey.getBytes());
        //删除指定列族的所有数据
        //delete.addFamily(colFamily.getBytes());
        //删除指定列的数据
        //delete.addColumn(colFamily.getBytes(), col.getBytes());
 
        table.delete(delete);
        table.close();
        close();
    }
```

**6.  查数据**

```java
 /**
     * 根据行键rowkey查找数据
     * @param tableName 表名
     * @param rowKey 行键
     * @param colFamily 列族名
     * @param col 列名
     * @throws IOException
     */
    public static void getData(String tableName,String rowKey,String colFamily,String col)throws  IOException{
        init();
        Table table = connection.getTable(TableName.valueOf(tableName));
        Get get = new Get(rowKey.getBytes());
        get.addColumn(colFamily.getBytes(),col.getBytes());
        Result result = table.get(get);
        showCell(result);
        table.close();
        close();
    }
```

**7. 格式化输出**

```java
/**
     * 格式化输出
     * @param result
     */
    public static void showCell(Result result){
        Cell[] cells = result.rawCells();
        for(Cell cell:cells){
            System.out.println("RowName:"+new String(CellUtil.cloneRow(cell))+" ");
            System.out.println("Timetamp:"+cell.getTimestamp()+" ");
            System.out.println("column Family:"+new String(CellUtil.cloneFamily(cell))+" ");
            System.out.println("row Name:"+new String(CellUtil.cloneQualifier(cell))+" ");
            System.out.println("value:"+new String(CellUtil.cloneValue(cell))+" ");
        }
    }
```

**主函数**

```java
public class ExampleForHbase{
    public static Configuration configuration;
    public static Connection connection;
    public static Admin admin;
 
    public static void main(String[] args)throws IOException{
        //创建一个表，表名为Score，列族为sname,course
        createTable("Score",new String[]{"sname","course"});
 
        //在Score表中插入一条数据，其行键为95001,sname为Mary（因为sname列族下没有子列所以第四个参数为空）
        //等价命令：put 'Score','95001','sname','Mary'
        insertRow("Score", "95001", "sname", "", "Mary");
        //等价命令：put 'Score','95001','score:Math','88'
        insertRow("Score", "95001", "course", "Math", "88");
        //等价命令：put 'Score','95001','score:English','85'
        insertRow("Score", "95001", "course", "English", "85");
 
        //等价命令：delete 'Score','95001','score:Math'
        deleteRow("Score", "95001", "course", "Math");
        //等价命令：delete 'Score','95001','score'
        deleteRow("Score", "95001", "course", "");
        //等价命令：deleteall 'Score','95001'
        deleteRow("Score", "95001", "", "");
 
        //查询Score表中，行键为95001，列族为course，列为Math的值
        getData("Score", "95001", "course", "Math");
        //查询Score表中，行键为95001，列族为sname的值（因为sname列族下没有子列所以第四个参数为空）
        getData("Score", "95001", "sname", "");
 
        //删除Score表
        deleteTable("Score");
    }
}
```



## 4. NoSQL数据库

### 5.1 NoSQL简介

通常，NoSQL数据库具有以下几个特点：

（1）灵活的可扩展性

（2）灵活的数据模型

（3）与云计算紧密融合

### 5.2 NoSQ兴起的原因

**1. 关系数据库已经无法满足Web2.0的需求。主要表现在以下几个方面： **

（1）无法满足海量数据的管理需求 

（2）无法满足数据高并发的需求 

（3）无法满足高可扩展性和高可用性的需求 

<font color='red' >MySQL集群是否可以完全解决问题？</font>

**复杂性**： 部署、管理、配置很复杂  

**数据库复制**：MySQL主备之间采用复制方 式，只能是异步复制，当主库压力较大时可 能产生较大延迟，主备切换可能会丢失最后 一部分更新事务，这时往往需要人工介入， 备份和恢复不方便  

**扩容问题**：如果系统压力过大需要增加新的机器，这个过程涉及数据重新划分，整个过 程比较复杂，且容易出错 

**动态数据迁移问题**：如果某个数据库组压力 过大，需要将其中部分数据迁移出去，迁移 过程需要总控节点整体协调，以及数据库节点的配合。这个过程很难做到自动化  

**2. “One size fits all”模式很难适用于截然不同的业务场景 **

关系模型作为统一的数据模型既被用于数据分析，也被用于在线业务。 但这两者一个强调高吞吐，一个强调低延时，已经演化出完全不同的架构。用同一套模型来抽象显然是不合适的 

Hadoop就是针对数据分析 , MongoDB、Redis等是针对在线业务，两者都抛弃了关系模型 

**3. 关系数据库的关键特性包括完善的事务机制和高效的查询机制。但是，关系数据库引以为傲的两个关键特性，到了Web2.0时代却成了鸡肋，主要表现在以下几个方面：  **

（1）Web2.0网站系统通常不要求严格的数据库事务 

（2）Web2.0并不要求严格的读写实时性 

（3）Web2.0通常不包含大量复杂的SQL查询（去结构化，存储空间换取更好 的查询性能） 

### 5.3 NoSQL与关系数据库的比较

| 比较标准   | RDBMS    | NoSQL                                                        | 备注                                                         |
| ---------- | -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 数据库原理 | 完全支持 | 部分支持                                                     | RDBMS有关系代数理论作为基础 NoSQL没有统一的理论基础          |
| 数据规模   | 大       | 超大                                                         | RDBMS很难实现横向扩展，纵向扩展的空间也比较有限，性能会随着数据规模的增大而降低 NoSQL可以很容易通过添加更多设备来支持更大规模的数据 |
| 数据库模式 | 固定     | 灵活                                                         | RDBMS需要定义数据库模式，严格遵守数据定义和相关约束条件NoSQL不存在数据库模式，可以自由灵活定义并存储各种不同 类型的数据 |
| 查询效率   | 快       | 可以实现高效的简单查询，但是不具备高度结构化查询等特性， 复杂查询的性能不尽人意 | RDBMS借助于索引机制可以实现快速查询（包括记录查询和范 围查询） 很多NoSQL数据库没有面向复杂查询的索引，虽然NoSQL可以使用MapReduce来加速查询，但是，在复杂查询方面的性能仍然不如RDBMS |
| 一致性     | 强一致性 | 弱一致性                                                     | RDBMS严格遵守事务ACID模型，可以保证事务强一致性 很多NoSQL数据库放松了对事务ACID四性的要求，而是遵守 BASE模型，只能保证最终一致性 |
| 数据完整性 | 容易实现 | 很难实现                                                     | 任何一个RDBMS都可以很容易实现数据完整性，比如通过主键 或者非空约束来实现实体完整性，通过主键、外键来实现参照 完整性，通过约束或者触发器来实现用户自定义完整性 但是，在NoSQL数据库却无法实现 |
| 扩展性     | 一般     | 好                                                           | RDBMS很难实现横向扩展，纵向扩展的空间也比较有限，NoSQL在设计之初就充分考虑了横向扩展的需求，可以很容易 通过添加廉价设备实现扩展 |
| 可用性     | 好       | 很好                                                         | RDBMS在任何时候都以保证数据一致性为优先目标，其次才是 优化系统性能，随着数据规模的增大，RDBMS为了保证严格的 一致性，只能提供相对较弱的可用性，大多数NoSQL都能提供较高的可用性 |
| 标准化     | 是       | 否                                                           | RDBMS已经标准化（SQL） NoSQL还没有行业标准，不同的NoSQL数据库都有自己的查询 语言，很难规范应用程序接口 StoneBraker认为：NoSQL缺乏统一查询语言，将会拖慢NoSQL 发展 |
| 技术支持   | 高       | 低                                                           | RDBMS经过几十年的发展，已经非常成熟，Oracle等大型厂商 都可以提供很好的技术支持 NoSQL在技术支持方面仍然处于起步阶段，还不成熟，缺乏有 力的技术支持 |
| 可维护性   | 复杂     | 复杂                                                         | RDBMS需要专门的数据库管理员(DBA)维护 <br />NoSQL数据库虽然没有DBMS复杂，也难以维护 |

**总结**

 关系数据库

**优势**：以完善的关系代数理论作为基础，有严格的标准，支持事务ACID四性 ，借助索引机制可以实现高效的查询，技术成熟，有专业公司的技术支持 

**劣势**：可扩展性较差，无法较好支持海量数据存储，数据模型过于死板、无 法较好支持Web2.0应用，事务机制影响了系统的整体性能等 

NoSQL数据库 

**优势**：可以支持超大规模数据存储，灵活的数据模型可以很好地支持Web2.0 应用，具有强大的横向扩展能力等 **劣势**：缺乏数学理论基础，复杂查询性能不高，大都不能实现事务强一致性 ，很难实现数据完整性，技术尚不成熟，缺乏专业团队的技术支持，维护较 困难等 

### 5.4 NoSQL的四大类型

 典型的NoSQL数 据库通常包括键值数据库、列族数据库、文档数据库和图形数据库 

 <img src="imgs/1571268751488.png" alt="1571268751488" style="zoom:45%;" />      <img src="imgs/1571268776649.png" alt="1571268776649" style="zoom:45%;" />   

 <img src="imgs/1571268918949.png" alt="1571268918949" style="zoom:40%;" />     <img src="imgs/1571268940387.png" alt="1571268940387" style="zoom:40%;" />

#### 5.4.1 键值数据库

| 相关产品   | Redis、Riak、SimpleDB、Chordless、Scalaris、Memcached        |
| ---------- | ------------------------------------------------------------ |
| 数据模型   | 键/值对<br />键是一个字符串对象 <br />值可以是任意类型的数据，比如整型、字符型、数组、列表、集合等 |
| 典型应用   | 涉及频繁读写、拥有简单数据模型的应用 <br />内容缓存，比如会话、配置文件、参数、购物车等 <br />存储配置和用户数据信息的移动应用 |
| 优点       | 扩展性好，灵活性好，大量写操作时效能高                       |
| 缺点       | 无法存储结构化信息，条件查询效率较低                         |
| 不适用情形 | 不是通过键而是通过值来查：键值数据库根本没有通过值查询的途径 <br />需要存储数据之间的关系：在键值数据库中，不能通过两个或两个以上的键来关联数据<br />需要事务的支持：在一些键值数据库中，产生故障时，不可以回滚 |
| 使用者     | 百度云数据库（Redis）、GitHub（Riak）、BestBuy（Riak）、Twitter（Redis和Memcached）等 |

> Redis有时候会被人们称为“强化版的Memcached” 支持持久化、数据恢复、更多数据类型 

#### 5.4.2 列族数据库

| 相关产品   | BigTable、HBase、Cassandra、HadoopDB、GreenPlum、PNUTS       |
| ---------- | ------------------------------------------------------------ |
| 数据模型   | 列族                                                         |
| 典型应用   | 分布式数据存储与管理 <br />数据在地理上分布于多个数据中心的应用程序 <br />可以容忍副本中存在短期不一致情况的应用程序 <br />拥有动态字段的应用程序 拥有潜在大量数据的应用程序，大到几百TB的数据 |
| 优点       | 查找速度快，可扩展性强，容易进行分布式扩展，复杂性低         |
| 缺点       | 功能较少，大都不支持强事务一致性                             |
| 不适用情形 | 需要ACID事务支持的情形，Cassandra等产品就不适用              |
| 使用者     | Cassandra：Ebay、Instagram、NASA<br />HBase：Twitter 、Facebook、Yahoo! |

#### 5.4.3 文档数据库

 “文档”其实是一个数据记录，这个记录能够对包含的数据类型和内容进行“自 我描述”。XML文档、HTML文档和JSON 文档就属于这一类。SequoiaDB就是使用JSON格式的文档数据库，它的存储的数据是这样的： 

```json
{
    "ID":1,
    "NAME":"SequoiaDB",
    "Tel":{
        "Office":"123123","Mobile":"18292837271"
    }
    "Addr":"China,HF"
}
```

一个XML文档

```xml
<configuration>
    <property>
        <name>hbase.rootdir</name>
        <value>hdfs://localhost:9000/hbase</value>
    </property>
</configuration>
```

特性：

* 数据是不规则的，每一条记录包含了所有的有关“SequoiaDB”的信息而没有任何外部的引用，这条记录是“自包含”的 

* 这使得记录很容易完全移动到其他服务器，因为这条记录的所有信息都包含在里面了 ，不需要考虑还有信息在别的表没有一起迁移走 
* 同时，因为在移动过程中，只有被移动的那一条记录（文档）需要操作，而不像关系型中每个有关联的表都需要锁住来保证一致性，这样一来ACID的保证就会变得更快速 ，读写的速度也会有很大的提升 

| 相关产品   | MongoDB、CouchDB、Terrastore、ThruDB、RavenDB等              |
| ---------- | ------------------------------------------------------------ |
| 数据模型   | 键/值 <br />值（value）是版本化的文档                        |
| 典型应用   | 存储、索引并管理面向文档的数据或者类似的半结构化数据 <br />比如，用于后台具有大量读写操作的网站、使用JSON数据结构的应用、使用嵌套结 构等非规范化数据的应用程序 |
| 优点       | 性能好（高并发），灵活性高，复杂性低，数据结构灵活 <br />提供嵌入式文档功能，将经常查询的数据存储在同一个文档中 <br />既可以根据键来构建索引，也可以根据内容构建索引 |
| 缺点       | 缺乏统一的查询语法                                           |
| 不适用情形 | 在不同的文档上添加事务。文档数据库并不支持文档间的事务，如果对这方面有需求，则不应该选用这个解决方案 |
| 使用者     | MongoDB：百度云数据库、SAP、Codecademy、 Foursquare等        |

#### 5.4.4 图形数据库

| 相关产品 | Neo4J、OrientDB、InfoGrid、Infinite Graph、GraphDB           |
| -------- | ------------------------------------------------------------ |
| 数据模型 | 图结构                                                       |
| 典型应用 | 专门用于处理具有高度相互关联关系的数据，比较适合于社交网络、模式识别、依赖 分析、推荐系统以及路径寻找等问题 |
| 优点     | 灵活性高，支持复杂的图形算法，可用于构建复杂的关系图谱       |
| 缺点     | 复杂性高，只能支持一定的数据规模                             |
| 使用者   | Neo4J：Adobe、Cisco、T-Mobile                                |

#### 5.4.5 不同类型数据库比较分析

 ![1571273476182](imgs/1571273476182.png)

* MySQL产生年代较早，而且随着LAMP大潮得以成熟。尽管其没有什么大的改进，但是 新兴的互联网使用的最多的数据库 
* MongoDB是个新生事物，提供更灵活的数据模型、异步提交、地理位置索引等五花十色 的功能 
* HBase是个“仗势欺人”的大象兵。依仗着Hadoop的生态环境，可以有很好的扩展性。 但是就像象兵一样，使用者需要养一头大象(Hadoop)，才能驱使他
* Redis是键值存储的代表，功能最简单。提供随机数据存储。就像一根棒子一样，没有多 余的构造。但是也正是因此，它的伸缩性特别好。就像悟空手里的金箍棒，大可捅破天， 小能成缩成针 

### 5.5 NoSQL的三大基石

#### 5.5.1 CAP 

**C（Consistency）**： 一致性，是指任何一个读操作总是能够读到之前完成的 写操作的结果，也就是在分布式环境中，多点的数据是一致的，或者说，所 有节点在同一时间具有相同的数据 

**A:（Availability）**：可用性，是指快速获取数据，可以在确定的时间内返回操作结果，保证每个请求不管成功或者失败都有响应

**P（Tolerance of Network Partition）**：分区容忍性，是指当出现网络分区的情况时（即系统中的一部分节点无法和其他节点进行通信），分离的系统也能够正常运行，也就是说，系统中任意信息的丢失或失败不会影响系统的继续运作。

CAP理论告诉我们，一个分布式系统不可能同时满足一致性、可用性 和分区容忍性这三个需求，最多只能同时满足其中两个，正所谓“鱼和熊 掌不可兼得”  

一个牺牲一致性来换取可用性的实例 

 <img src="imgs/1571273778582.png" alt="1571273778582" style="zoom:59%;" />          <img src="imgs/1571273801846.png" alt="1571273801846" style="zoom:51%;" />

​    初始状态                                                                                正常执行过程

 <img src="imgs/1571274125538.png" alt="1571274125538" style="zoom:50%;" />

​                                 更新传播失败时的执行过程

当处理CAP的问题时，可以有几个明显的选择： 

**CA**：也就是强调一致性（C）和可用性（A），放弃分区容忍性（P），最简单的做法是把所有与事务相关的内容都放到同一台机器上。很显然，这种做法会严重影响系统的可扩展性。传统的关系数据库（MySQL、SQL Server和PostgreSQL），都采用了这 种设计原则，因此，扩展性都比较差 

**CP**：也就是强调一致性（C）和分区容忍性（P），放弃可用性（A），当出现网络分区的情况时，受影响的服务需要等待数据一致，因此在等待期间就无法对外提供服务 

**AP**：也就是强调可用性（A）和分区容忍性（P），放弃一致性（C），允许系统返回不一致的数据

 <img src="imgs/1571274322449.png" alt="1571274322449" style="zoom:50%;" />

#### 5.5.2 BASE

说起BASE（Basically Availble, Soft-state, Eventual consistency）， 不得不谈到ACID ，一个数据库事务具有ACID四性：

* A(Atomicity)：原子性，是指事务必须是原子工作单元，对于其数据修改，要么全都执行，要么全都不执行
* C(Consistency)：一致性：是指事务在完成时，必须使所有的数据都保持一致状态
* I(Isolation)：隔离性：是指由并发事务所做的修改必须与任何其它并发事务所做的修改隔离
* D(Durability)：持久性，是指事务完成之后，它对于系统的影响是永久性的，该修改即使出现致命的系统故障也将一直保持

BASE的基本含义是基本可用(Basically Availble)、软状态(Soft-state)和 最终一致性(Eventual consistency)：

**1. 基本可用**

基本可用，是指一个分布式系统的一部分发生问题变得不可用时，其他部分仍然可以正常使用，也就是允许分区失败的情形出现

**2. 软状态**

 “软状态（soft-state）”是与“硬状态（hard-state）”相对应的一种提法。数据库保存的数据是“硬状态”时，可以保证数据一致性，即保证数据一直是正确的。“软状态 ”是指状态可以有一段时间不同步，具有一定的滞后性

**3. 最终一致性**

 一致性的类型包括强一致性和弱一致性，二者的主要区别在于高并发的数据访问操作下 ，后续操作是否能够获取最新的数据。 

*  对于强一致性而言，当执行完一次更新操作后，后续的其他读操作就可以保证读到更新后的最新数据；反之，如果不能保证后续访问读到的都是 更新后的最新数据，那么就是弱一致性。 
*  而最终一致性只不过是弱一致性的一种特例，允许后续的访问操作可以暂时读不到更新后的数据，但是经过一段时间之后，必须最终读到更新 后的数据。  

>  最常见实现最终一致性的系统是DNS（域名系统）。一个域名更新操作根据配置的形式被分发出去，并结合有过期机制的缓存；最终所有的客户端可以看到最新的值。 

 最终一致性根据更新数据后各进程访问到数据的时间和方式的不同，又可以区分为： 

* <font color='red'>因果一致性</font>：如果进程A通知进程B它已更新了一个数据项，那么进程B的后续访问将获得A写入的最新值。而与进程A无因果关系的进程C的访问，仍然遵守一般的最终一致性规则 
* <font color='red'>“读己之所写”一致性</font>：可以视为因果一致性的一个特例。当进程A自己执行一 个更新操作之后，它自己总是可以访问到更新过的值，绝不会看到旧值 
* <font color='red'>单调读一致性</font>：如果进程已经看到过数据对象的某个值，那么任何后续访问都不 会返回在那个值之前的值
* <font color='red'>会话一致性</font>：它把访问存储系统的进程放到会话（session）的上下文中，只要会话还存在，系统就保证“读己之所写”一致性。如果由于某些失败情形令会话终止，就要建立新的会话，而且系统保证不会延续到新的会话 
* <font color='red'>单调写一致性</font>：系统保证来自同一个进程的写操作顺序执行。系统必须保证这种程度的一致性，否则就非常难以编程了

<font color='red'>**如何实现各种类型的一致性？**</font>

对于分布式数据系统： 

N — 数据复制的份数 
W — 更新数据是需要保证写完成的节点数 
R — 读取数据的时候需要读取的节点数 

如果W+R>N，写的节点和读的节点重叠，则是强一致性。例如对于典型的一主一备同步 复制的关系型数据库，N=2, W=2, R=1，则不管读的是主库还是备库的数据，都是一致的 。一般设定是 R＋W = N+1，这是保证强一致性的最小设定 

如果W+R<=N，则是弱一致性。例如对于一主一备异步复制的关系型数据库， N=2,W=1,R=1，则如果读的是备库，就可能无法读取主库已经更新过的数据，所以是弱 一致性。 

对于分布式系统，为了保证高可用性，一般设置N>=3。不同的N,W,R组合，是在可用性 和一致性之间取一个平衡，以适应不同的应用场景。 

如果N=W,R=1，任何一个写节点失效，都会导致写失败，因此可用性会降低，但是由于 数据分布的N个节点是同步写入的，因此可以保证强一致性。 

**实例**：HBase是借助其底层的HDFS来实现其数据冗余备份的。HDFS采用的就是强一致 性保证。在数据没有完全同步到N个节点前，写操作是不会返回成功的。也就是说它的W ＝N，而读操作只需要读到一个值即可，也就是说它R＝1。 

> 像Voldemort，Cassandra和Riak这些类Dynamo的系统，通常都允许用户按需要设置N ，R，W三个值，即使是设置成W＋R<= N也是可以的。也就是说他允许用户在强一致性 和最终一致性之间自由选择。而在用户选择了最终一致性，或者是 W <N的强一致性时，则总会出现 一段“各个节点数据不同步导致系统处理不一致的时间”。为了提供最终一 致性的支持，这些系统会提供一些工具来使数据更新被最终同步到所有相关节点。 

### 5.6 从NoSQL到NewSQL数据库

 <img src="imgs/1571323668033.png" alt="1571323668033" style="zoom:50%;" />

 <img src="imgs/1571323708954.png" alt="1571323708954" style="zoom:50%;" />

### 5.7 文档数据库MongoDB

**5.7.1 MongoDB简介**

* MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库 系统。 
* 在高负载的情况下，添加更多的节点，可以保证服务器性能。 
* MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。 
* MongoDB 将数据存储为一个文档，数据结构由键值(key=>value)对组成。 MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档 数组。 

**主要特点**

* 提供了一个面向文档存储，操作起来比较简单和容易 
* 可以设置任何属性的索引来实现更快的排序 
* 具有较好的水平可扩展性 
* 支持丰富的查询表达式，可轻易查询文档中内嵌的对象及数组 
* 可以实现替换完成的文档（数据）或者一些指定的数据字段
* MongoDB中的Map/Reduce主要是用来对数据进行批量处理和聚合操作 
* 支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等语言  

**5.7.2 MongoDB概念解析**

在mongodb中基本的概念是文档、集合、数据库 

| SQL术语/概念 | MongoDB术语/概念 | 解释/说明                             |
| ------------ | ---------------- | ------------------------------------- |
| database     | database         | 数据库                                |
| table        | collection       | 数据库表/集合                         |
| row          | document         | 数据记录行/文档                       |
| column       | field            | 数据字段/域                           |
| index        | index            | 索引                                  |
| table joins  |                  | 表连接，MongoDB不支持                 |
| primary key  | primary key      | 主键，MongoDB自动将\_id字段设置为主键 |

**数据库**

一个mongodb中可以建立多个数据库。 

MongoDB的默认数据库为"db"，该数据库存储在data目录中。 

MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集 合和权限，不同的数据库也放置在不同的文件中。  

**文档**

文档是一个键值(key-value)对(即BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。

  下表列出了 RDBMS 与 MongoDB 对应的术语： 

| RDBMS   | 数据库 | 表格 | 行   | 列   | 表联合   | 主键            |
| ------- | ------ | ---- | ---- | ---- | -------- | --------------- |
| MongoDB | 数据库 | 集合 | 文档 | 字段 | 嵌入文档 | 主键(key为\_id) |

**集合**

* 集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统： Relational Database Management System)中的表格。 

* 集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插 入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定 的关联性。

```json
{"site":"www.baidu.com"}
{“site”:“dblab.xmu.edu.cn”,“name”:“厦门大学数据库实验室"}
{"site":"www.runoob.com","name":"菜鸟教程","num":5}
```

**数据类型**

![1571324745774](imgs/1571324745774.png)

**5.7.3 安装MongoDB**

启动 MongoDB服务：只需要在MongoDB安装目录的bin目录下执行'mongod'即可 

#### 5.7.4 访问MongoDB

##### 使用shell命令访问MongoDB

使用 MongoDB shell 来连接 MongoDB 服务器 ：mongodb://localhost 

使用用户名和密码连接登陆到指定数据库： mongodb://admin:123456@localhost/test

**1. 创建数据库**

MongoDB 创建数据库的语法格式如下： use DATABASE_NAME 

> 如果数据库不存在，则创建数据库，否则切换到指定数据库 

**2. 创建集合**

MongoDB没有单独创建集合名的shell命令，在插入数据的时候， MongoDB会自动创建对应的集合。

**3. 插入文档**

文档的数据结构和JSON基本一样。 所有存储在集合中的数据都是BSON格式。 BSON是一种类JSON的一种二进制形式的存储格式,简称Binary JSON。 MongoDB 使用 insert() 或 save() 方法向集合中插入文档，语法如下： 

```json
db.col.insert({title: 'MongoDB 教程',
description: 'MongoDB 是一个 Nosql 数据库',
by:‘厦门大学数据库实验室',
url: 'http://dblab.xmu.edu.cn',
tags: ['mongodb', 'database', 'NoSQL'],
likes: 100
})
```

##### 使用Java程序访问MongoDB

**MongoDB Java **

环境配置：在Java程序中如果要使用MongoDB，需要确保已经安装了Java环境及 MongoDB JDBC 驱动 

需要将mongo.jar包含在你的 classpath 中 

**1. 连接数据库**

```java
import com.mongodb.MongoClient;
……//这里省略其他需要导入的包
public class MongoDBJDBC{
    public static void main( String args[] ){
        try{
            // 连接到 mongodb 服务
            MongoClient mongoClient = new MongoClient("localhost" , 27017);
            // 连接到数据库
            DB db = mongoClient.getDB("test");
            System.out.println("Connect to database successfully");
            boolean auth = db.authenticate(myUserName, myPassword);
            System.out.println("Authentication: "+auth);
        }catch(Exception e){
            System.err.println( e.getClass().getName() + ": " + e.getMessage());
        }
    }
}
```

**2. 创建集合**

可以使用 `com.mongodb.DB` 类中的 `CreteCollection()`来创建集合

```java
public class MongoDBJDBC{
    public static void main( String args[] ){
        try{
            // 连接到 mongodb 服务
            MongoClient mongoClient = new MongoClient( "localhost" , 27017 );
            // 连接到数据库
            DB db = mongoClient.getDB( "test" );
            System.out.println("Connect to database successfully");
            boolean auth = db.authenticate(myUserName, myPassword);
            System.out.println("Authentication: "+auth);
            DBCollection coll = db.createCollection("mycol");
            System.out.println("Collection created successfully");
        }catch(Exception e){
            System.err.println(e.getClass().getName() + ": " + e.getMessage());
        }
    }
}
```

**3. 插入文档**

可以使用 `com.mongodb.DBCollection` 类的 `insert()` 方法来插入一个文档 

```java
public class MongoDBJDBC{
    public static void main( String args[] ){
        try{ // 连接到 mongodb 服务
            MongoClient mongoClient = new MongoClient( "localhost" , 27017 );
            DB db = mongoClient.getDB( "test" ); // 连接到数据库
            System.out.println("Connect to database successfully");
            boolean auth = db.authenticate(myUserName, myPassword);
            System.out.println("Authentication: "+auth);
            DBCollection coll = db.getCollection("mycol");
            System.out.println("Collection mycol selected successfully");
            BasicDBObject doc = new BasicDBObject("title", "MongoDB").
                append("description", "database").
                append("likes", 100).
                append("url", "http://www.w3cschool.cc/mongodb/").
                append("by", "w3cschool.cc");
            coll.insert(doc);
            System.out.println("Document inserted successfully");
        }catch(Exception e){
            System.err.println(e.getClass().getName() + ": " + e.getMessage());
        }
    }
}
```

## 5. 云数据库

### 5.1 云数据库概述

#### 5.1.1 云计算是云数据库兴起的基础

通过整合、管理、调配分布在网络各处的计算资源，通过互联网以统一界面，同时向大量的用户提供服务

#### 5.1.2 云数据库概念

云数据库是部署和虚拟化在云计算环境中的数据库。云数据库是在云计算的大背景 下发展起来的一种新兴的共享基础架构的方法，它极大地增强了数据库的存储能力 ，消除了人员、硬件、软件的重复配置，让软、硬件升级变得更加容易。云数据库具有高可扩展性、高可用性、采用多租形式和支持资源有效分发等特点。 

#### 5.1.3 云数据库的特性

云数据库具有以下特性

（1）动态可扩展（2）高可用性（3）较低的使用代价（4）易用性（5）高性能（6）免维护（7）安全

#### 5.1.4 云数据库是个性化数据存储需求的理想选择

企业类型不同，对于存储的需求也千差万别，而云数据库可以很好地满 足不同企业的个性化存储需求： 

（1）大企业的海量数据存储（2）中小企业的低成本数据存储（3）企业动态变化的数据存储

**选择标准**：大型企业可以自建数据库，财力有限的中小企业，可以选择云数据库，低成本，免维护

#### 5.1.5 云数据库与其它数据库的关系

从数据模型的角度分析，云数据库并非一种全新的数据库技术，只是以服务的方式提供数据功能，它并没有专有的数据模型，所采用的数据模型既可以是关系型也可以是非关系型模型，而且，同一家公司也可以提供采用不同数据模型的多种云数据库服务。

### 5.2 云数据库产品

#### 5.2.1 云数据库厂商概述

| 企业        | 产品                                 |
| ----------- | ------------------------------------ |
| Amzaon      | Dynamo、SimpleDB、RDS                |
| Google      | Google Cloud SQL                     |
| Microsoft   | Microsoft SQL Azure                  |
| Oracle      | Oracle Cloud                         |
| Yahoo！     | PNUTS                                |
| Vertica     | Analytic Database v3.0 for the Cloud |
| EnerpriseDB | Postgre Plus in the Cloud            |
| 阿里        | 阿里云RDS                            |

#### 5.2.2 Amazon的云数据产品

Amazon的云数据产品有：S3存储服务、EC2计算服务、数据库服务（关系型数据库RDS、键值数据库SimpleDB、NoSQL数据库DynamoDB、数据仓库Redshift、分布式内存缓存ElastiCache）

#### 5.2.3 Google的云数据库产品

Google Cloud SQL是谷歌公司推出的基于MySQL的云数据库，使用Cloud SQL，所有的事务都在云中，并由谷歌管理，用户不需要配置或者排查错误，谷歌还提供导入或导出服务，方便用户将数据库带进或带出云，谷歌使用用户非常熟悉的MySQL，带有JDBC支持和DB-API支持的传统MySQL数据库环境，因此，多数应用程序不需过多调试即可运行，数据格式对于大多数开发者和管理员来说也是非常熟悉的。

#### 5.2.4 Microsoft的云数据库产品

SQL Azure具有以下特性：

属于关系型数据库：支持使用TSQL（Transact Structured Query Language）来管理、创建和操作云数据库

支持存储过程：它的数据类型、存储过程和传统的SQL Server具有很大的相似性，因此，应用可以在本地进行开发，然后部署到云平台上

支持大量数据类型：包含了几乎所有典型的SQL Server 2009的数据类型

支持云中的事务：支持局部事务，但是不支持分布式事务

### 5.3 云数据库系统架构

#### 5.3.1 UMP 系统概述

UMP系统是低成本和高性能的MySQL云数据库方案，总的来说，UMP系统架构设计遵循了以下原则：

保持单一的系统对外入口，并且为系统内部维护单一的资源池

消除单点故障，保证服务的高可用性

保证系统具有良好的可伸缩，能够动态地增加、删减计算与存储节点

保证分配给用户的资源也是弹性可伸缩的，资源之间相互隔离，确保应用和数据安全

#### 5.3.2 UMP系统架构

UMP系统中的角色包括：

Controller服务器，Proxy服务器，Agent服务器，Web控制台，日志分析服务器，信息统计服务器，愚公系统

依赖的开源组件包括：Mnesia、LVS、RabbitMQ、ZooKeeper

**1. Mnesia**

它是一个分布式数据库管理系统，支持事务，支持透明的数据分片，利用两阶段锁实现分布式事务，可以线性扩展到至少50个节点，它的数据库模式可在运行时动态重配置，表能被迁移或复制到多个节点来改进容错性，这些特性使其在开发云数据库时被用来提供分布式数据库服务

**2. RabbitMQ**

它是一个工业级的消息队列产品（功能类似于IBM公司的消息队列产品IBM Websphere MQ），作为消息传输中间件来使用，可以实现可靠的消息传送，UMP集群中各个节点之间的通信，不需要建立专门的连接，都是通过读写队列消息实现的

**3. Zookeeper**

它是高效和可靠的协同工作系统，提供分布式锁之类的基本服务（比如统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等），用于构建分布式应用，减轻分布式应用程序所承担的协调任务

在UML系统中，Zookeeper主要发挥三个作用：

* 作为全局的配置服务器

* 提供分布式锁（选出一个集群的“总管”）

* 监控所有MySQL实例

**4. LVS**

LVS即LInux虚拟服务器，是一个虚拟的服务器集群系统，UMP系统借助于LVS来实现集群内部的负载均衡，LVS集群采用IP负载均衡技术和基于内容请求分发技术，调度器是LVS集群系统的唯一入口点，调度器具有很好的吞吐率，将请求均衡地转移到不同的服务器上执行，且调度器自动屏蔽掉服务器的故障，从而将一组服务器构成一个高性能的、高可用的虚拟服务器，整个服务器集群的结构对客户是透明的，而且无需修改客户端和服务器端的程序。

**5. Controller服务器**

Controller服务器向UMP集群提供各种管理服务，实现集群成员管理、元数据存储、MySQL实例管理、故障恢复、备份、迁移、扩容等功能，Controller服务器上运行了一组Mnesia分布式数据库服务，其中存储了各种系统元数据，主要包括集群成员、用户的配置和状态信息，以及用户名到后端MySQL实例地址的映射关系（或称为“路由表”）等，当其它服务器组件需要获取用户数据时，可以向Controller服务器发送请求获取数据

为了避免单点故障，保证系统的高可用性，UMP系统中部署了多台Controller服务器，然后，由Zookeeper的分布式锁功能来帮助选择一个“总管”，负责各种系统任务的调度和监控。

**6. Web控制台**

Web控制台向用户提供系统管理界面

**7. Proxy服务器**

Proxy服务器向用户提供访问MySQL数据库的服务，它完全实现了 MySQL协议，用户可以使用已有的MySQL客户端连接到Proxy服务器， Proxy服务器通过用户名获取到用户的认证信息、资源配额的限制(例如 QPS、IOPS（I/O Per Second）、最大连接数等) ，以及后台MySQL实例的地址，然后，用户的SQL查询请求会被转发到相应的MySQL实例上。 除了数据路由的基本功能外，Proxy服务器中还实现了很多重要的功能， 主要包括屏蔽MySQL实例故障、读写分离、分库分表、资源隔离、记录 用户访问日志等 

**8. Agent服务器**

Agent服务器部署在运行MySQL进程的机器上，用来管理每台物理机上的 MySQL实例，执行主从切换、创建、删除、备份、迁移等操作，同时，还负责收集和分析MySQL进程的统计信息、慢查询日志（Slow Query Log）和bin-log 

**9. 日志分析服务器**

日志分析服务器存储和分析Proxy服务器传入的用户访问日志，并支持实时查询一段时间内的慢日志和统计报表

**10. 信息统计服务器**

信息统计服务器定期将采集到的用户的连接数、QPS数值以及MySQL实例的进程状态用RRDtool进行统计，可以在 Web界面上可视化展示统计结果，也可以把统计结果作为今后实现弹性的资源分配和自动化的MySQL实例迁移的依据 

**11. 愚公系统**

愚公系统是一个全量复制结合bin-log分析进行增量复制的工具，可以实现在不停机的情况下动态扩容、缩容和迁移 

  ![1571959387679](imgs/1571959387679.png)



#### 5.3.3 UMP系统功能

UMP系统是构建在一个大的集群之上的，通过多个组件的协同作业，整个系统实现了对用户透明的各种功能：

（1）容灾（2）读写分离（3）分库分表（4）资源管理（5）资源调度（6）资源隔离（7）数据安全

**1. 容灾**

为了实现容灾，UMP系统会为每个用户创建两个MySQL实例，一个是主库，一个是从库，主库和从库的状态是由Zookeeper负责维护的

主从切换过程如下：

* Zookeeper探测到主库故障，通知Controller服务器

* Controller服务器启动主从切换时，会修改"路由表"，即用户名到后端MySQL实例地址的映射关系

* 把主库标记为不可用

* 借助于消息中间件RabbitMQ通知所有Proxy服务器修改用户名后到后端MySQL实例地址的映射关系

* 全部过程对用户透明

宕机后的主库在进行恢复处理后需要再次上线，过程如下：

* 在主库恢复时，会把从库的更新复制给自己

* 当主库的数据库状态快要达到和从库一致的状态时，Controller服务器就会命令从库停止更新，进入不可写状态，禁止用户写入数据

* 等到主库更新到和从库完全一致的状态时，Controller服务器就会发起主从切换操作，并在路由表中吧主库标记为可用状态

* 通知Proxy服务器把写操作切回主库上，用户写操作可以继续执行，之后再把从库修改为可写状态

**2. 读写分离**

* 充分利用主从库实现用户读写操作的分离，实现负载均衡 
* UMP系统实现了对于用户透明的读写分离功能，当整个功能被开启时， 负责向用户提供访问MySQL数据库服务的Proxy服务器，就会对用户发起 的SQL语句进行解析，如果属于写操作，就直接发送到主库，如果是读操 作，就会被均衡地发送到主库和从库上执行

**3. 分库分表**

UMP支持对用户透明的分库分表，当采用分库分表时，系统处理用户查询的过程如下：

* 首先，Proxy服务器解析用户SQL语句，提取出重写和分发SQL语句所需要的信息
* 其次，对SQL语句进行重写，得到多个针对相应MySQL实例的子语句，然后把子语句分发到对应的MySQL实例上执行
* 最后，接收来自各个MySQL实例的SQL语句执行结果，合并得到最终结果

**4. 资源管理**

UMP系统采用资源池机制来管理数据库服务器上的CPU、内存、磁 盘等计算资源，所有的计算资源都放在资源池内进行统一分配，资源 池是为MySQL实例分配资源的基本单位  

整个集群中的所有服务器会根据其机型、所在机房等因素被划分多个 资源池，每台服务器会被加入到相应的资源池中 

对于每个具体MySQL实例，管理员会根据应用部署在哪些机房、需要哪些计算资源等因素，为该MySQL实例具体指定主库和从库所在的 资源池，然后，系统的实例管理服务会本着负载均衡的原则，从资源 池中选择负载较轻的服务器来创建MySQL实例 

**5. 资源调度**

UMP系统中有三种规格的用户，分别是数据量和流量比较小的用户、 中等规模用户以及需要分库分表的用户

*  多个小规模用户可以共享同一个MySQL实例
* 对于中等规模的用户，每个用户独占一个MySQL实例
* 对于分库分表的用户，会占有多个独立的MySQL实例

**6. 资源隔离**

UMP采用的两种资源隔离方式

| 方法                      | 应用场合                                  | 实现方式                                                     |
| ------------------------- | ----------------------------------------- | ------------------------------------------------------------ |
| 用Cgroup限制MySQL进程资源 | 适用于多个MySQL实例共享同一台物理机的情况 | 可以对用户的MySQL进程最大可以使用的CPU使用率、内存和IOPS等进行限制 |
| 在Proxy服务器端限制QPS    | 适用于多个用户共享同一个MySQL实例的情况   | Controller服务器监测用户的MySQL实例的资源消耗情况，如果明显超过配额，就通知Proxy服务器通过增加延迟的方法去限制用户的QPS，以减少用户对系统资源的消耗 |

**7. 数据安全**

 UMP系统设计了多种机制来保证数据安全： 

* SSL数据库连接：SSL(Secure Sockets Layer)是为网络通信提供安全及数据完整性的一种安全协议，它在传输层对网络连接进行加密。 

* Proxy服务器实现了完整的MySQL客户端/服务器协议，可以与客户端之间建立SSL数据库连接 

* 数据访问IP白名单：可以把允许访问云数据库的IP地址放入“白名单”，只有白名单内的IP地址才能访问，其他IP地址的访问都会被拒绝， 从而进一步保证账户安全 

* 记录用户操作日志：用户的所有操作记录都会被记录到日志分析服务器，通过检查用户操作记录，可以发现隐藏的安全漏洞 

* SQL拦截：Proxy服务器可以根据要求拦截多种类型的SQL语句，比如 全表扫描语句“select *” 

### 5.4 Amazon AWS和云数据库

#### 5.4.1 Amazon和云计算的渊源

 <img src="imgs/1572051390403.png" alt="1572051390403" style="zoom:50%;" />                <img src="imgs/1572051407719.png" alt="1572051407719" style="zoom:50%;" />

**AWS Global Infrastructure(AWS 全局基础设施)**

在全局基础设施中有3个很重要的概念，第一个是Region，每个Region是相互独立的，自成一套云服务体系，分布在全球各地。目前全球有10个Region，第二个是Availability Zone(可用区)，每个Region又由数个可用区组成，每个可用区可以看做一个数据中心，相互之间通过光纤连接，第三个是Edge Location(边缘节点)。是一各内容分发网络，可以降低内容分发的延迟，保证终端用户获取资源的速度。

#### 5.4.2 Amzaon AWS

**Networking(网络)**

AWS提供的网络服务主要有：

Direct Connect：支持企业自身的数据中心与AWS的数据中心直连，充分利用企业现有的资源

VPN Connection：通过VPN连接AWS，保证数据的安全性 

Virtual Private Cloud： 私有云，从AWS云资源中分一块给 你使用，进一步提高安全性 

Route 53：亚马逊提供的高可用的可伸缩的云域名解析系统，高效地将用户请求连接到AWS中的运行的基础设施

**Storage(存储)**

S3： Simple Storage Service，简单存储服务，是亚马逊对 外提供的对象存储服务。不限容量，单个对象大小可达5TB， 可实现高达99.999999999%的可用性 

EBS： Elastic Block Storage，专门为Amazon EC2 虚拟机 设计的弹性块存储服务，Amazon EBS可以为Amazon EC2的 虚拟机创建卷volumns。 EBS相当于一个分布式块设备，可 以直接挂载在EC2实例上，用于替代EC2实例本地存储，从 而增强EC2可靠性 

Glacier：主要用于较少使用的存储存档文件和备份文件，价 格便宜量又足，安全性高

**Database(数据库)**

亚马逊提供关系型数据库和NOSQL数据库，以及一些cache等数据库服务

SimpleDB：基于云的键/值数据存储服务

DynamoDB：DynamoDB是亚马逊自主研发的No SQL数据库，性能高，容错性强，支持分布式

RDS：Relational Database Service，关系型数据库服务，支持SQL Server和Oracle等数据库

Amazon ElastiCache：数据库缓存服务

**Application Service(应用程序服务)**

Cloud Search：一个弹性的搜索引擎，可用于企业级搜索

Amazon SQS：队列服务，存储和分发消息

CloudFront：世界范围的内容分发网络（CDN） 

EMR： Elastic MapReduce，一个Hadoop框架的实例，可用于大数据处理

**Deployment & Admin(部署和管理)**

Elastic BeanStalk: 一键式创建各种开发环境和运行时

CloudFormation：采用JSON格式的模板文件来创建和管理一系列亚马逊云资源

OpsWorks： OpsWorks允许用户将应用程序的部署模块化，可以实现对数据库、 运行时、服务器软件等自动化设置和安装 

IAM： Identity & Access Management，认证和访问管理服务。用户使用云服务 最担心的事情之一就是安全问题。亚马逊通过IAM提供了立体化的安全策略，保证用户在云上的资源绝对的安全

**Amazon EC2架构**

相比传统的虚拟机托管，EC2的最大特点是允许用户根据需求动态调整运行的实例类型和数量，实现按需付费

Amazon EC2平台主要包含如下部分：EC2实例，弹性块存储(AMI)，弹性负载均衡(自动缩放)

**EC2存储**

EC2本地存储是实例自带的磁盘空间，但它并不是持久的，也就是说这个实 例所在的节点出现故障时，相应的磁盘空间也会随之清空

为了解决本地存储不可靠问题，EC2推出了EBS， EBS通过卷来组织数据，每个EBS卷只能挂载到一个EC2实例， EBS卷并不与实例绑定，而是与用户帐号绑定

Amazon S3和EBS的区别

![1572053259132](imgs/1572053259132.png)

在EC2中创建虚拟机实例时，会提示选择镜像（Images）的类型： 

* S3-Hosted images：镜像需从S3存储中拷贝到EC2实例的本地存储。完成虚拟机镜像拷贝后启动EC2实例

* EBS-backed images:虚拟机启动要快得多，当关闭虚拟机后，虚拟机的数据还在EBS上 

**AWS云管理平台**

云平台负责根据客户的需求（并发数、吞吐量、数据存储空间等）来弹性地分配资源，然后将不用的资源收回 

任何一个SaaS在提供服务的时候，云平台都会通过4个阶段对服务进行资源 的分配及调整：  

1. 首先启动服务，当有客户进行服务操作时，云平台会启动服务 
2. 启动后监控服务的需求情况 
3. 当无人访问时，停止服务 
4. 回收不被使用的资源

**Amazon AWS**

一个典型的Hadoop作业执行时，AWS具体的操作流程：

消息平台首先发送服务启动的命令给启动 控制器，由启动控制器首先将启动信息放在SimpleDB的缓冲区里

分配EC2的计算资源，启动Hadoop等操 作，将计算数据从S3中导入EC2, 开始进行计算和分析

监控控制器接收到监控信息后，对应用中所有的资源和错误进行监控，更新 SimpleDB的缓冲区中的状态，并且根据用 户的需要随时增减资源（计算节点和存储 节点）

关闭控制器在收到关闭消息后，会停止 EC2、Hadoop等资源，将运算结果放入S3 或者客户指定的存储目标，并发消息给结 算控制器

#### 5.4.3 Amazon AWS平台上的云数据库

**SimpleDB**

SimpleDB是AWS上的第一个NoSQL数据库服务（键值数据库）
记录由主键和多个属性组成
可以把数据进行多副本存储，支持高并发读取
更新操作只能针对主副本进行，但可以快速传播到其他副本，提供最终一致性
SimpleDB更适合存储小型、碎片化的零散数据

缺陷如下：

* SimpleDB有单表限制。SimpleDB 数据模型由域、项目、属性和值组成，每 个域最多只能保存10GB的数据，所以得自己分区以免超过此限制
* 性能不稳定。SimpleDB以简单为设计目标，SimpleDB并不需要用户指定主 键，也不需要用户创建索引，会默认对所有属性创建索引。然而这种简洁性 也带来了一些副作用
* 一致性问题。SimpleDB设计时采用的是最终一致性模型

**Amazon DynamoDB**

采纳了SimpleDB中成功的托管服务形式及灵活的数据模型

记录由主键和多个属性组成，这一点类似于SimpleDB与BigTable，这比 简单的KV模型更易用

提供了一致性读功能

限制了系统的功能，只能通过主键去操作记录，不能进行批量更新，这 使得系统可以保证可伸缩性及任何时候的高性能

全面使用SSD来提升系统性能

**Amazon RDS**

Amazon RDS 有超过10万活跃客户和 多个数据库引擎可供选择，已成为云 中运行关系数据库的新常态

MySQL、Oracle、SQL Server、PostgreSQL、MariaDB、Aurora 

> 借助 AWS 数据库迁移服务及其附带模式转换工具，客户可选择从本地部署向 AWS 迁移相同数据库引擎 RDS可以建立3TB和3万的DB实例 

### 5.5 微软云数据库SQL Azure

SQL Azure是微软的云关系型数据库，后端存储又称为“云SQL Server” ,它是构建在SQL Server之上，通过分布式技术提升传统关系数据库的可扩展性和容错能力  

**1. 逻辑模型**

 一个逻辑数据库称为一个表格组，表格组中所有划分主键相同的行集合 称为行组（row group） ，它只支持同一个行组内的事务，同一个 行组的数据逻辑上会分布到一台服务器，以此规避分布式事务。通过主备复制将数据复制到多个副本， 保证高可用性

**2. 物理模型**

在物理层面，每个有主键的表格组根据划分主键列有序地分成多个数据分区。每个行组属于唯一分区

分区是SQL Azure复制、迁移、负载均衡的基本单位。每个分区包含多个副 本（默认为3），每个副本存储在一台物理的SQL Server上

SQL Azure保证每个分区的多个副本分布到不同的故障域。每个分区有一个副本为主副本（Primary）,其他副本为从副本（Secondary）。主副本处理 所有的查询、更新事务，并以操作日志的形式，将事务同步到从副本，从副 本接收主副本发送的事务日志并应用到本地数据库

每台物理SQL Server数据库混合存放了来自不同逻辑分区的主副本和从副本 

 <img src="imgs/1572054495164.png" alt="1572054495164" style="zoom:60%;" /><img src="imgs/1572054520216.png" alt="1572054520216" style="zoom:70%;" />

**3. 体系架构**

SQL Azure分为四个主要部分： SQL Server实例、全局分区管理器、协议网关、 分布式基础部件 

每个SQL Server实例是一个运行着SQL Server的物理进程。每个物理数据库包含 多个子数据库，它们之间相互隔离。子数据库是一个分区，包含用户的数据以及 schema信息

全局分区管理器维护分区映射表信息 

协议网关负责将用户的数据库连接请求转 发到相应的主分区上 

分布式基础部件（Fabric）用于维护机器 上下线状态，检测服务器故障并为集群中 的各种角色执行选取主节点操作

> SQL Azure的体系架构中包含了一个虚拟机簇，可以根据工作负载的变化， 动态增加或减少虚拟机的数量，每台虚拟机SQL Server VM(virtual machine)安装了SQL Server 数据库管 理系统，以关系模型存储数据。通常，一个数据库会被散存储到3~5 台SQL Server VM中 

### 5.6 云数据库实践

#### 5.6.1 阿里云RDS简介

RDS是阿里云提供的关系型数据库服务，它将直接运行于物理服务 器上的数据库实例租给用户，是专业管理的、高可靠的云端数据库服务 

#### 5.6.2 RDS中的概念

**RDS实例**，是用户购买RDS服务的基本单位。在实例中： 

可以创建多个数据库
可以使用常见的数据库客户端连接、管理及使用数据 
可以通过RDS管理控制台或OPEN API来创建、修改和删除数据库 

**RDS数据库**，是用户在一个实例下创建的逻辑单元：

一个实例可以创建多个数据库，在实例内数据库命名唯一，所有数据库都会共享该实例下的资源，如CPU、内存、磁盘容量等 
RDS不支持使用标准的SQL语句或客户端工具创建数据库，必须使用 OPEN API或RDS管理控制台进行操作 

**地域**指的是用户所购买的RDS实例的服务器所处的地理位置

**RDS可用区**是指在同一地域下，电力、网络隔离的物理区域，可用区之间 内网互通，可用区内网络延时更小，不同可用区之间故障隔离，RDS可用区又分为单可用区和多可用区

* **单可用区**是指RDS实例的主备节点位于相同的可用区，它可以有效控制云产品间的网络延迟
* **多可用区**是指RDS实例的主备节点位于不同的可用区，当主节点所在可用 区出现故障（如机房断电等），RDS进行主备切换后，会切换到备节点所 在的可用区继续提供服务。多可用区的RDS轻松实现了同城容灾 

**磁盘容量**是用户购买RDS实例时，所选择购买的磁盘大小 

> 实例所占用的磁盘容量，除了存储表格数据外，还有实例正常运行所需 要的空间，如系统数据库、数据库回滚日志、重做日志、索引等

**RDS连接数**，是应用程序可以同时连接到RDS实例的连接数量

> 任意连接到RDS实例的连接均计算在内，与应用程序或者网站能够支持 的最大用户数无关， 用户在购买RDS实例时所选择的内存大小决定了该实例的最大连接数 

## 6. MapReduce

### 6.1 概述

#### 6.1.1 分布式并行编程

谷歌公司最先提出分布式并行编程模型MapReduce，Hadoop MapReduce是它的开源实现，后面使用门槛更低

问题：在MapReduce出现之前，已经有像MPI这样非常成熟的并行计算框架了，为什么还需要MapReduce？优势在哪？

|                  | 传统并行计算框架                          | MapReduce                  |
| ---------------- | ----------------------------------------- | -------------------------- |
| 集群架构/容错性  | 共享式（共享内存/共享存储），容错性差     | 非共享式，容错性好         |
| 硬件/价格/扩展性 | 刀片服务器、高速网、SAN，价格贵，扩展性差 | 普通PC机，便宜，扩展性好   |
| 编程/学习难度    | what-how，难                              | what，简单                 |
| 使用场景         | 实时、细粒度计算、计算密集型              | 批处理、非实时、数据密集型 |

#### 6.1.2 MapReduce模型简介

MapReduce将复杂并行计算过程高度抽象为俩个函数：**Map**和**Reduce**，编程更加容易，不需要掌握分布式并行编程细节，也可以很容易把自己的程序运行在分布式系统上，完成海量计算。

MapReduce采用**分而治之**策略，一个存储在分布式文件系统中的大规模数据集，会被切分成许多独立的分片，这些分片可以被多个Map任务并行处理，它的设计理念就是**计算向数据靠拢**，因此，移动数据需要大量的开销；采用的是Master/Slave架构，包括一个Master和若干个Slave，Master上运行JobTracker，Slave上运行TaskTracker

Hadoop框架使用Java实现的，但是，MapReduce应用程序则不一定要用Java来写

#### 6.1.3 Map和Reduce函数

​                                                  表1 Map和Reduce函数
<img src="imgs/1572656404650.png" alt="1572656404650" style="zoom:60%;" />

### 6.2 MapReduce体系结构

 <img src="imgs/1572658329456.png" alt="1572658329456" style="zoom:80%;" />

MapReduce主要有由以下四部分组成：

**1. Client**

用户编写的MapReduce程序通过Client提交到JobTracker端，用户可以通过Client提供的一些接口查看作业的运行状态

**2. JobTracker**

JobTracker负责资源调监控和作业调度，它监控所有TaskTracker与Job的健康状况，一旦发生失败，就将相应的任务转移到其他节点；跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源

**3. TaskTracker**

TaskTracker会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给JobTracker，同时接收JobTracker发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）
TaskTracker使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个Task获取到一个slot后才有机会运行，而Hadoop调度器的作用就是将各个TaskTracker上的空闲slot分配给Task使用，slot分为Map slot和Reduce slot两种，分别供Map Task和Reduce task使用

**4. Task**

Task分为Map Task 和Reduce Task两种，均由TaskTracker启动

### 6.3 MapReduce工作流程

#### 6.3.1 工作流程概述

 <img src="imgs/1572658615393.png" alt="1572658615393" style="zoom:80%;" />

不同的Map任务之间不会进行通信
不同的Reduce任务之间也不会发生任何信息交换
用户不能显式地从一台机器向另一台机器发送消息
所有的数据交换都是通过MapReduce框架自身去实现的

#### 6.3.2 MapReduce各个执行阶段

 <img src="imgs/1572658864338.png" alt="1572658864338" style="zoom:80%;" />

**关于Split（分片）**

HDFS以固定大小的block为基本单位存储数据，而对于MapReduce而言，其处理单元是split，split是一个逻辑概念，它只包含一些元数据信息，，比如数据起始位置、数据长度、数据所在节点等，它的划分方法完全由用户自己决定。

**Map任务的数量**

Hadoop为每个split创建一个Map任务，split的多少决定了Map任务的数目，大多数情况下，理想的分片大小是一个HDFS快

**Reduce任务的数量**
最优的Reduce任务个数取决于集群中可用的reduce任务槽(slot)的数目，通常设置比reduce任务槽数目稍微小一些的Reduce任务个数（这样就可以预留一些系统资源处理可能发生的错误）

#### 6.3.3 Shuffle过程详解

**1. shuffle过程简介**

 <img src="imgs/1572659426905.png" alt="1572659426905" style="zoom:80%;" />

**2. Map端的Shuffle过程**

<img src="imgs/1572659488109.png" alt="1572659488109" style="zoom:80%;" />

每个Map任务分配一个缓存，MapReduce默认100MB缓存，设置溢出比例0.8，分区默认采用哈希函数，排序是默认的操作，排序后可以合并，合并不能改变最终结果，在Map任务全部结束之前进行归并，归并得到一个大的文件，放在本地磁盘，文件归并时，如果溢出文件数量大于预定值（默认3）则可以再次启动Combiner，少于3则不需要
JobTracker会一直监测Map任务的执行，并通知Reduce任务来认领数据

> 合并和归并的区别：两个键值对<"a",1>和<"a",1>，如果合并，会得到<"a",2>,如果归并，会得到<"a",<1,1>>

**3. Reduce端的Shuffle过程**

Reduce任务通过RPC向JobTracker询问Map任务是否已经完成，若完成，则领取数据；
Reduce领取数据先放入缓存，来自不同Map机器，先归并，再合并，写入磁盘；
多个溢写文件归并成一个或多个大文件，文件中的键值对是排序的；
当数据很少时，不需要溢写到磁盘，直接在缓存中归并，然后输出给Reduce；

<img src="imgs/1572660121614.png" alt="1572660121614" style="zoom:80%;" />

#### 7.3.4 MapReduce应用程序执行过程

<img src="imgs/1572660188640.png" alt="1572660188640" style="zoom:80%;" />

### 6.4 实例分析：WordCount

#### 6.4.1 WordCount程序任务

| 程序          |                                                    | 输出                                                         |
| ------------- | -------------------------------------------------- | ------------------------------------------------------------ |
| wordCount     | 一个包含大量单词的文本文件                         | 文件中每个单词及其出现次数（频数），并按照单词字母顺序排序，每个单词和其频数占一行，单词和频数之间有间隔 |
| wordCount实例 | Hello World<br />hello Hadoop<br />hello MapReduce | Hadoop 1<br />Hello 3<br />MapReduce 1<br />World 1          |

#### 6.4.2 WordCount设计思路

* 首先需要检查WordCount程序任务是否可以采用MapReduce来实现
* 其次，确定MapReduce程序的设计思路
* 最后，确定MapReduce程序的执行过程

#### 6.4.3 一个WordCount执行过程的实例

 <img src="imgs/1572660576556.png" alt="1572660576556" style="zoom:60%;" /><img src="imgs/1572660606910.png" alt="1572660606910" style="zoom: 60%;" />

​            Map过程示意图                                                         用户没有定义Combiner时的Reduce过程示意图

 <img src="imgs/1572661869225.png" alt="1572661869225" style="zoom:70%;" />

用户有定义Combiner时的Reduce过程示意图

### 6.5 MapReduce的具体应用

MapReduce可以很好地应用于各种计算问题

* 关系代数运算（选择、投影、并、交、差、连接）
* 分组与聚合运算
* 矩阵-向量乘法
* 矩阵乘法

**用MapReduce实现关系的自然连接**

![1572662527620](imgs/1572662527620.png)

* 假设有关系R(A， B)和S(B,C)，对二者进行自然连接操作 
* 使用Map过程，把来自R的每个元祖<a,b>转换成一个键值对<b, <R,a>>，其中的键就是属性B的值，把关系R包含到值中，这样做使得我们可以在Reduce阶段，只是把哪些来自R的元祖和来自S的元祖进行匹配。类似地，使用Map过程，把来自S的每个元祖<b,c>，转换成一个键值对<b，<S,c>>
* 所有具有相同B值的元祖被发送到同一个Reduce进程中，Reduce进程的任务是，把来自关系R和S的、具有相同属性B值的元祖进行合并
* Reduce进程的输出则是连接后的元祖<a,b,c>，输出被写到一个单独的输出文件中

 <img src="imgs/1572662580410.png" alt="1572662580410" style="zoom:67%;" />

### 6.6 MapReduce编程实践

#### 6.6.1 任务要求

文件A的内容如下：                                                                        文件B的内容如下：

 <img src="imgs/1572662777369.png" alt="1572662777369" style="zoom:80%;" />

期望结果如右侧所示：

 <img src="imgs/1572662811938.png" alt="1572662811938" style="zoom:60%;" />

#### 6.6.2 编写Map处理逻辑

Map输入类型为<key,value>

期望的Map输出类型为<单词，出现次数>

Map输入类型最终确定为<Object,Text>

Map输出类型最终确定为<Text,IntWritable>

```java
public static class MyMapper extends Mapper<Object,Text,Text,IntWritable>
{
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();
    public void map(Object key, Text value, Context context) throwsIOException,InterruptedException
    {
        StringTokenizer itr = new StringTokenizer(value.toString());
        while (itr.hasMoreTokens())
        {
        word.set(itr.nextToken());
        context.write(word,one);
        }
    }
}
```

#### 6.6.3 编写Reduce处理逻辑

在Reduce处理数据之前，Map的结果首先通过Shuffle阶段进行整理

Reduce阶段的任务：对输入数字序列进行求和

Reduce的输入数据为<key，Iterable容器>

Reduce任务的输入数据：

<”I”,<1,1>>
<”is”,1>
……
<”from”,1>
<”China”,<1,1,1>>

```java
public static class MyReducer extends Reducer<Text,IntWritable,Text,IntWritable>
{
    private IntWritable result = new IntWritable();
    public void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException,
    InterruptedException
    {
        int sum = 0;
        for (IntWritable val : values)
        {
            sum += val.get();
        }
        result.set(sum);
        context.write(key,result);
    }
}
```

#### 6.6.4 编写main方法

```java
public static void main(String[] args) throws Exception
{
    Configuration conf = new Configuration(); //程序运行时参数
    String[] otherArgs = new GenericOptionsParser(conf,args).getRemainingArgs();
    if (otherArgs.length != 2)
    { 
        System.err.println("Usage: wordcount <in> <out>");
        System.exit(2);
    }
    Job job = new Job(conf,"word count"); //设置环境参数
    job.setJarByClass(WordCount.class); //设置整个程序的类名
    job.setMapperClass(MyMapper.class); //添加MyMapper类
    job.setReducerClass(MyReducer.class); //添加MyReducer类
    job.setOutputKeyClass(Text.class); //设置输出类型
    job.setOutputValueClass(IntWritable.class); //设置输出类型
    FileInputFormat.addInputPath(job,new Path(otherArgs[0])); //设置输入文件
    FileOutputFormat.setOutputPath(job,new Path(otherArgs[1])); //设置输出文件
    System.exit(job.waitForCompletion(true)?0:1);
}
```

WordCount类的具体代码如下：

```java
public class WordCount
{
    public static class MyMapper extends Mapper<Object,Text,Text,IntWritable>
    {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();
        public void map(Object key, Text value, Context context) throws IOException,InterruptedException
        {
            StringTokenizer itr = new StringTokenizer(value.toString());
            while (itr.hasMoreTokens())
            {
                word.set(itr.nextToken());
                context.write(word,one);
            }
        }
    }
    # 加上MyReducer代码
    # 加上main方法
}
```

#### 6.6.5 编译打包代码以及运行程序

使用java编译程序，生成.class文件

将.class文件打包为jar包

运行jar包（需要启动Hadoop）

查看结果

**hadoop依赖的jar包**

通过命令hadoop classpath可以得到运行Hadoop程序所需的全部classpath信息

将Hadoop的classpath信息添加到CLASSPATH变量中，在~/.bashrc中增加如下几行：

```shell
export HADOOP_HOME=/usr/local/hadoop
export CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$CLASSPATH
```

执行source ~/.bashrc使变量生效，接着就可以通过javac命令编译WordCount.java

```java
javac WordCount.java
```

接着把.class文件打包成jar，才能在Hadoop中运行：

```shell
jar -cvf WordCount.jar ./WordCount*.class
```

运行程序：

```java
/usr/local/hadoop/bin/hadoop jar WordCount.jar WordCount input output
```

#### 6.6.6 Hadoop中执行MapReduce任务的几种方式

* Hadoop jar
* Pig
* Hive
* Python
* Shell脚本

## 7. 数据仓库Hive

### 7.1 概述

#### 7.1.1 数据仓库概念

数据仓库是一个面向主题的、集成的、相对稳定的、反映历史变化的数据集合，用于支持管理决策。

#### 7.1.2 传统数据仓库面临的挑战

（1）、无法满足快速增长的海量数据存储需求

（2）、无法有效处理不同类型的数据

（3）、计算和处理能力不足

#### 7.1.3 Hive简介

Hive是一个构建于Hadoop顶层的数据仓库工具，支持大规模数据存储、分析，具有良好的可扩展性，某种程度上可以看作是用户编程接口，本身不存储和处理数据，依赖分布式文件系统HDFS存储数据，依赖分布式并行计算模型MapReduce处理数据，定义了简单的类似SQL的查询语言——HiveQL，用户可以通过编写的HiveQL语句运行MapReduce任务，可以很容易把原来构建在关系数据库上的数据仓库应用程序移植到Hadoop平台上，它是一个可以提供有效、合理、直观组织和使用数据的分析工具Hive具有的特点非常适用于数据仓库。

* 采用批处理方式处理海量数据
  * Hive需要把HiveQL语句转换成MapReduce任务进行运行
  * 数据仓库存储的是静态数据，对静态数据的分析适合采用批处理方式，不需要快速响应给出结果，而且数据本身也不会频繁变化
* 提供适合数据仓库操作的工具
  * Hvie本身提供了一系列对数据进行提取、转换、加载（ETL）的工具，可以存储、查询和分析存储在Hadoop中的大规模数据
  * 这些工具能够很好地满足数据仓库各种应用场景

#### 7.1.4 Hive与Hadoop生态系统中其他组件的关系

Hive依赖于HDFS存储数据

Hvie依赖于MapReduce处理数据

在某些场景下Pig可以作为Hive的替代工具

HBase提供数据的实时访问

#### 7.1.5 Hive与传统数据库的对比分析

Hive在许多方面和传统的关系数据库类似，但是它的底层依赖的是HDFS和MapReduce，所以在很多方面又有别于传统数据库

| 对比项目 | Hive         | 传统数据库         |
| -------- | ------------ | ------------------ |
| 数据插入 | 支持批量导入 | 支持单条和批量导入 |
| 数据更新 | 不支持       | 支持               |
| 索引     | 支持         | 支持               |
| 分区     | 支持         | 支持               |
| 执行延迟 | 高           | 低                 |
| 扩展性   | 好           | 有限               |

#### 7.1.6 Hive在企业中的部署和应用

 <img src="imgs/image-20191117214502994.png" alt="image-20191117214502994" style="zoom:67%;" />

图 企业中一种常见的大数据分析平台部署框架

### 7.2 Hive系统架构

用户接口模块：CLI、HWI、JDBC、ODBC、Thrift Server

驱动模块(Driver)：编译器、优化器、执行器等，负责把HiveSQL语句转换成一系列MapReduce作业

元数据存储模块(Metastore)：是一个独立的关系型数据库(自带derby数据库，或MySQL数据库)

 <img src="imgs/image-20191117214821769.png" alt="image-20191117214821769" style="zoom:67%;" />

### 7.3 Hive工作原理

#### 7.3.1 SQL语句转换成MapReduce作业的基本原理

<font color='red'>**1. join的实现原理**</font>

 <img src="imgs/image-20191117215026691.png" alt="image-20191117215026691" style="zoom:67%;" />

<font color='red'>**2. group by 的实现原理**</font>

存在一个分组（Group By）操作，其功能是把表Score的不同片段按照rank和level的组合值进行合并，计算不同rank和level的组合值分别有几条记录：`select rank,level,count(*) as value from score group by rank,level`

![image-20191117215358261](imgs/image-20191117215358261.png)

#### 7.3.2 Hive中SQL查询转换成MapReduce作业的过程

当用户向Hive输入一段命令或查询时，Hive需要与Hadoop交互工作来完成该操作：

* 驱动模块接收该命令或查询编译器
* 对该命令或查询进行解析编译
* 由优化器对该命令或查询进行优化计算
* 该命令或查询通过执行器进行执行

第1步：由Hive驱动模块中的编译器对用户输入的SQL语句进行词法和语法解析，将SQL语句转化成抽象语法树的形式

第2步：抽象语法树的结构仍和复杂，不方便直接翻译为MapReduce算法程序，因此，把抽象语法树转化为查询块

第3步：把查询块转换成逻辑查询计划，里面包含了许多逻辑操作符

第4步：重写逻辑查询计划，进行优化，合并多余操作，减少MapReduce任务数量

第5步：将逻辑操作符转换成需要执行的具体MapReduce任务

第6步：对生成的MapReduce任务进行优化，生成最终的MapReduce任务执行计划

第7步：由Hive驱动模块中的执行器，对最终的MapReduce任务进行执行输出
> 几点说明：
>
> * 当启动MapReduce程序时，Hive本身是不会生成MapReduce算法程序的，需要通过一个表示"Job 执行计划"的XML文件驱动执行内置的、原生的Mapper和Reducer模块
> * Hive通过和JobTracker通信来初始化MapReduce任务，不必直接部署在JobTracker所在的管理节点上执行
> * 通常在大型集群上，会有专门的网关机来部署Hive工具。网关机的作用主要是远程操作和管理节点上的JobTracker通信来执行任务
> * 数据文件通常存储在HDFS上，HDFS由名称节点管理

### 7.4 Hive HA基本原理

问题：在实际应用中，Hive也暴露出不稳定的问题

解决方案：Hive HA（High Availability)

由多个Hive实例进行管理的，这些Hive实例被纳入到一个资源池中，并由HAProxy提供一个统一的对外接口

对于程序开发人员来说，可以把它认为是一台超强“Hive”

 <img src="imgs/image-20191117221832213.png" alt="image-20191117221832213" style="zoom:80%;" />

​                                                                        图 Hive HA基本原理

### 7.5 Impala

#### 7.5.1 Impala简介

Impala是由Cloudera公司开发的新型查询系统，它提供SQL语义，能查询存储在Hadoop的HDFS和HBase上的PB级大数据，在性能上比Hive高出3~30倍，Impala的运行需要依赖于Hive的元数据，它是参照Dremel系统进行设计的，采用了与商用并行关系数据库类似的分布式查询引擎，可以直接与HDFS和HBase进行交互查询，Impala和Hive采用相同的SQL语法、ODBC驱动程序和用户接口

#### 7.5.2 Impala系统架构

Impala和Hive、HDFS、HBase等工具是统一部署在一个Hadoop平台上的，Impala主要由Impalad，State Store和CLI三部分组成

 <img src="imgs/image-20191117222437994.png" alt="image-20191117222437994" style="zoom:80%;" />

1、Impalad

* 负责协调客户端提交的查询的执行
* 包含Query Planner、Query Coordinator和Query Exec Engine三个模块
* 与HDFS的数据节点（HDFS DN）运行在同一节点上
* 给其他Impalad分配任务以及收集其他Impalad的执行结果进行汇总
* Impalad也会执行其他Impalad给其分配的任务，主要就是对本地HDFS和HBase里的部分数据进行操作

2、State Store

* 会创建一个statestored进程
* 负责收集分布在集群中各个Impalad进程的资源信息，用于查询调度

3、CLI

* 给用户提供查询使用的命令行工具
* 还提供了Hue、JDBC及ODBC的使用接口

> 说明：Impala中的元数据直接存储在Hive中，Impala采用与Hive相同的元数据、SQL语法、ODBC驱动程序和用户接口，从而使得在一个Hadoop平台上，可以统一部署Hive和Impala等分析工具，同时支持批处理和实时查询

#### 7.5.3 Impala查询执行过程

 <img src="imgs/image-20191117223238061.png" alt="image-20191117223238061" style="zoom:70%;" />

第0步：当用户提交查询前， Impala先创建一个负责协调客户端提交的查询的Impalad进程， 该进程会向Impala State Store提交注册订阅信息，State Store会创建一个statestored进程， statestored进程通过创建多个线程来处理Impalad的注册订阅信息 

第1步：用户通过CLI客户端提交一个查询到impalad进程， Impalad的Query Planner对SQL语句进行解析， 生成解析树；然后， Planner把这个查询的解析树变成若干PlanFragment， 发送到Query Coordinator 。

第2步：Coordinator通过从MySQL元数据库中获取元数据， 从HDFS的名称节点中获取数据地址， 以得到存储这个查询相关数据的所有数据节点 。

第3步：Coordinator初始化相应impalad上的任务执行， 即把查询任务分配给所有存储这个查询相关数据的数据节点  

第4步：Query Executor通过流式交换中间输出， 并由Query Coordinator汇聚来自各个impalad的结果  

第5步：Coordinator把汇总后的结果返回给CLI客户端 

#### 7.5.4 Impala与Hive的比较

Hive与Impala的相同点总结如下：

1. Hive与Impala使用相同的存储数据池，都支持把数据存储于HDFS和HBase中
2. Hive与Impala使用相同的元数据
3. Hive与Impala中对SQL的解释处理比较相似，都是通过词法分析生成执行计划

**总结：**

Impala的目的不在于替换现有的MapReduce工具，把Hive与Impala配合使用效果更佳，可以先使用Hive进行数据转换处理，之后再使用Impala在Hive处理后的结果数据集上进行快速的数据分析

### 7.6 Hive编程实践

Hive学习实践详细过程，请参考：[《Hive实践教程》](  http://dblab.xmu.edu.cn/blog/hive-in-practice  
  )

#### 7.6.1 Hive的数据类型

| 类型      | 描述                                                         | 示例                       |
| --------- | ------------------------------------------------------------ | -------------------------- |
| TINYINT   | 1个字节（8位）有符号整数                                     | 1                          |
| SMALLINT  | 2个字节（16位）有符号整数                                    | 1                          |
| INT       | 4个字节（32位）有符号整数                                    | 1                          |
| BIGINT    | 8个字节（64位）有符号整数                                    | 1                          |
| FLOAT     | 4个字节（32位）单精度浮点数                                  | 1.0                        |
| DOUBLE    | 8个字节（64位）双精度浮点数                                  | 1.0                        |
| BOOLEAN   | 布尔类型，true/false                                         | true                       |
| STRING    | 字符串，可以指定字符串                                       | “xmu"                      |
| TIMESTAMP | 整数、浮点数或者字符串                                       | 1327882394（Unix新纪元秒） |
| BINARY    | 字符数组                                                     | [0,1,0,1,0,1,0,1]          |
| ARRAY     | 一组有序字段，字段的类型必须相同                             | Array(1,2)                 |
| MAP       | 一组无序的键/值对，键的类型必须是原子的，值可以是任何数据类型，同一个映射的键和值的类型必须相同 | Map('a',1,'b',2)           |
| STRUCT    | 一组命名的字段，字段类型可以不同                             | Struct('a',1,1,0)          |

#### 7.6.2 Hive基本操作

**1.create：创建数据库、表、视图**

创建数据库Hive：`create database hive;`

> 若Hive存在，则会抛出异常，因此需要加上if not exists关键字，则不会抛出异常

`create database if not exists hive;`

创建表，例如usr，含三个属性id，name，age

`use hive;`

`create table if not exists usr(id bigint, name string, age int) `

存储路径为：

`location '/usr/local/hive/warehouse/hive/usr';`

创建视图little_usr，只包含usr表中id，age属性

`create view little_usr as select id ,age from usr;`

**2. 查看数据库、表、视图**

查看Hive中包含的所有数据库：`show databases;`

查看Hive中以h开头的所有数据库：`show databases like 'h.*'`

查看数据库hive中所有表和视图：`use hive;`,`show tables;`

查看数据库hive中以u开头的所有表和视图：`show tables in hive like 'u.*';`

**3. load: 向表中装载数据**

把目录`/usr/local/data`下的数据文件中的数据装载进usr表并覆盖原有数据

`load data local inpath '/usr/local/data' overwrite into table usr;`

把目录`/usr/local/data`下的数据文件中的数据装载进usr表不覆盖原有数据

`load data local inpath '/usr/local/data'into table usr;`

把分布式文件系统目录`hdfs://master_server/usr/local/data`下的数据文件数据装载进usr表并覆盖原有数据

`load data inpath'hdfs://master_server/usr/local/data' overwrite into table usr;`

**4.insert：向表中插入数据或从表中导出数据**

向表usr1中插入来自usr表的数据并覆盖原有数据

`insert overwrite table usr1 select * from usr where age=10;`

向表usr1中插入来自usr表的数据并追加在原有数据后

`insert into table usr1 select * from usr where age=10;`

#### 7.6.3 Hive应用实例：WordCount

































































