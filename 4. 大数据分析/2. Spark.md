---

layout:     post
title:      spark 2019
subtitle:   Spark编程基础（python版）
date:       2019-09-02
author:     BY
header-img: img/post-bg-cook.jpg
catalog: true
tags:

    - 大数据

---

# Spark编程基础（python版）

## 1. 大数据概念
### 1.1 大数据特征

* 数据量大

* 数据类型繁多，包括结构化数据与非结构化数据

* 处理速度快，时间窗口非常小

* 价值密度低
###  1.2 大数据的影响

* 全样而非抽样

* 效率而非精确

* 相关而非因果

### 1.3 大数据关键技术

* 分布式存储
  * GFS\HDFS
  * BigTable\HBase
  * NoSQL（键值、列族、图形、文档数据库）
  * NewSQL（如：SQL Azure）

* 分布式处理
  * MapReduce

### 1.4 大数据计算模式

| 大数据计算模式 |            解决问题            |          代表产品           |
| :------------: | :----------------------------: | :-------------------------: |
|     流计算     |      针对流数据的实时处理      | Storm、S4、Flume、Streams等 |
|     图计算     |   针对大规模图结构数据的处理   |   Pregel、GraphX、Giraph    |
|  查询分析计算  | 大规模数据的存储管理和查询分析 |   Dremel、Hive、Cassandra   |

### 1.5 代表性大数据技术
#### 1.5.1 Hadoop
![image.png-143.5kB][1]

1.  MapReduce并行计算过程抽象为2个函数：`Map `和 `Reduce`
2.  采用的是“分而治之”策略，存储在HDFS的大规模数据集被切分成许多独立的分片，这些分片被多个Map任务并行处理
3.  
![image.png-101.8kB][2]
<div></div>
#### 1.5.2 YARN

YARN的目标就是实现“一个集群多个框架”，即在一个集群上部署一个统一的资源调度管理框架YARN，在YARN上可以部署其他各种计算框架，为这些计算框架提供统一的资源调度管理服务。

![image.png-95.1kB][3]

#### 1.5.3 Spark

![image.png-152.9kB][4]

Hadoop的缺点：
* 表达能力有限
* 磁盘IO开销大
* 延迟高

Spark的优点：
* 提供了多种数据集操作类型，编程模型不局限于Map和Reduce操作，更加灵活
* 提供了内存计算，可将中间结果放到内存中，迭代效率更高
* 基于DAG（有向无环图）的任务调度执行机制，优于Hadoop MapReduce的迭代执行机制

#### 1.5.4 其它的计算技术
Flink

* 计算模型：一行一行处理，基于操作符的连续流模型
* 流式计算跟storm性能差不多，支持毫秒级计算

Beam

*  谷歌公司提出的想要统一大数据技术的框架



## 2. 设计与运行原理

### 2.1 Spark简介
Spark具有如下几个主要特点：
* 运行速度快
* 通用性
* 运行模式多样
### 2.2 Spark生态系统
在实际应用中，大数据处理主要包括以下三个类型：
* 复杂的批量数据处理：通常时间跨度在数十分钟到数小时之间
* 基于历史数据的交互式查询：通常时间跨度在数十秒到数分钟之间
* 基于实时数据流的数据处理：通常时间跨度在数百秒到数秒之间

![image.png-86.4kB][5]

![image.png-173.3kB][6]
### 2.3 Spark运行架构
#### 2.3.1 基本概念
* <font color='red'>RDD:</font> 是弹性分布式数据集的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型
* <font color='red'>DAG: </font>是有向无环图的简称，反映RDD之间的依赖关系
* <font color='red'>Executor:</font> 是运行在工作节点（WorkerNode）的一个进程，负责运行Task
* <font color='red'>应用（Application）:</font> 用户编写的Spark应用程序
* <font color='red'>任务（Task）</font>: 运行在Executor上的工作单元
* <font color='red'>作业（Job）</font>: 一个作业包含多个RDD及作用于相应的RDD上的各种操作
* <font color='red'>阶段（Stage）</font>: 是作业的基本调度单位，一个作业会分为多组任务，每组任务被称为阶段，或者也被称为任务集合，代表了一组关联的、相互之间没有Shuffle依赖关系的任务组成的任务集

#### 2.3.2 架构设计
![image.png-73kB][7]

![image.png-78.7kB][8]

#### 2.3.3 Spark运行基本流程
![image.png-99.8kB][9]

1. 首先为应用构建起基本的运行环境，即由  `Driver` 创建一个 `SparkContext`，进行资源的申请、任务的分配和监控
2. 资源管理器为 `Executor` 分配资源，并启动 `Executor` 进程
3.  `SparkContext`根据RDD的依赖关系构建DAG图，DAG图提交给 `DAGScheduler` 解析成Stage，然后把一个个 `TaskSet` 提交给底层调度器TaskScheduler处理；Executor向SparkContext申请Task，`TaskScheduler` 将Task发放给 `Executor` 运行，并提供应用程序代码
4. `Task` 在 `Executor` 上运行，把执行结果反馈给 `TaskScheduler`，然后反馈给 `DAGScheduler`，运行完毕后写入数据并释放所有资源

#### 2.3.4 RDD运行原理

**1. RDD设计背景**
* 不必担心底层数据的分布式特性，只需要将具体的应用逻辑表达为一系列转换处理，不同RDD之间的转换操作形成依赖关系，可以实现管道化，避免中间数据存储

**2. RDD概念**
* 一个RDD就是一个只读分布式对象集合，每个RDD可分为多个分区，每个分区就是一个数据集片段，不同的分区可保存在集群中不同的节点上
* 提供了一种高度受限的共享内存模型，只能执行创建或转换RDD操作
* 提供了一组丰富的操作以支持常见的数据运算，分为“动作”和“转换”两种类型
* RDD提供的转换接口都十分简单，类似map、filter、groupBy、join等粗粒度的数据转换操作
* Spark提供了RDD的API，程序员可以通过调用API实现对RDD的各种操作

RDD执行过程如下：

* RDD读入外部数据源进行创建
* RDD经过一系列的转换操作，每一次都会产生不同的RDD，供给下一个转换操作使用
*  最后一个RDD经过“动作”操作进行转换，并输出到外部数据源

这一系列处理称为**Lineage(血缘关系)**，即DAG拓扑排序的结果

![image.png-62.7kB][10]

**3. RDD特性**
* 高效的容错性

* 中间结果持久化到内存

* 存放的数据可以是Java对象

**4. RDD之间的依赖关系**

* 是否包含Shuffle操作是区分**窄依赖**和**宽依赖**的依据

* 窄依赖表现为一个或多个父RDD的分区对应一个子RDD的分区

* 宽依赖表现为存在一个父RDD的一个分区对应一个子RDD的多个分区

**5. 阶段的划分**

* Spark根据DAG图中的RDD依赖关系，把一个作业分成多个阶段，阶段划分的依据就是窄依赖和宽依赖
* 窄依赖对于作业优化很有利，宽依赖无法优化
* 每个RDD操作都是一个fork/join（一种用于并行执行任务的框架），把计算fork到每个RDD分区，完成计算后对各个分区得到的结果进行join操作，然后fork/join下一个RDD操作

 **6. RDD运行原理**

*  创建RDD对象；
* SparkContext负责计算RDD之间的依赖关系，构建DAG；
* DAGScheduler负责把DAG图分解成多个Stage，每个Stage中包含了多个Task；
* TaskScheduler将每个Task分发给各个WorkerNode上的Executor去执行

![image.png-102kB][11]

### 2.4 Spark的部署方式

* Standalone（类似于MapReduce1.0, slot为资源分配单位）
* Spark on Mesos（和Spark有血缘关系，更好支持Mesos）
* Spark on YARN



## 3. 环境搭建和使用方法

### 3.1 安装Spark
Spark安装地址：http://dblab.xmu.edu.cn/blog/1307-2/
Hadoop安装地址：http://dblab.xmu.edu.cn/blog/install-hadoop/

Spark部署模式包括：
* Local模式：单机模式
* Standalone模式：使用Spark自带的简单集群管理器
* YARN模式：使用YARN作为集群管理器
* Mesos模式：使用Mesos作为集群管理器
### 3.2 在pyspark中运行代码
pyspark命令及其常用参数如下：

```shell
pyspark --master [master -url]
```


Spark的运行模式取决于[master -url]，分为：

* <font color='red'>local：</font>使用一个Worker线程本地化运行SPARK（完全不并行）
* <font color='red'>loca[*]：</font>使用逻辑CPU的数量的线程本地化运行Spark
* <font color='red'>local[K]：</font>使用K个Worker线程本地化运行Spark
* <font color='red'>spark://HOST：</font>POST连接到指定的Spark standalone master。默认端口号是7077
* <font color='red'>yarn-client：</font>以客户端模式连接YARN集群，集群的位置可以在HADOOP_CONF_DIR环境变量中找到
* <font color='red'>yarn-cluster：</font>以集群模式连接YARN集群，集群的位置可以在HADOOP_CONF_DIR环境变量中找到
* <font color='red'>mesos://HOST：</font>POST连接到指定的Mesos集群。默认端口是5050

执行如下命令启动pyspark（默认是local模式）：
```shell
$ cd /usr/local/spark
$ cd ./bin/pyspark
```
可以使用 `exit()` 退出pyspark
```python
>>> exit()
```
### 3.3 开发Spark独立应用程序

#### 3.3.1 安装编译打包工具

WordCount.py

```python
1  from pyspark import SparkConf, SparkContext 
2  conf = SparkConf().setMaster("local").setAppName("My App") 
3  sc = SparkContext(conf = conf) 
4  logFile = "file:///usr/local/spark/README.md" 
5  logData = sc.textFile(logFile, 2).cache() 
6  numAs = logData.filter(lambda line: 'a' in line).count() 
7  numBs = logData.filter(lambda line: 'b' in line).count() 
8  print('Lines with a: %s, Lines with b: %s' % (numAs, numBs))
```

对于WordCount.py这个文件，可以采用如下命令执行：

```shell
$ cd /usr/local/spark/mycode/python
$ python3 WordCount.py
```
执行该命令后，可以得到如下结果：
```python
Lines with a: 62, Lines with b: 30
```
#### 3.3.2 通过spark-submit运行程序

可以通过spark-submit提交应用程序，该命令的格式如下:

```python
spark-submit
--master <master-url>--deploy-mode <deploy-mode> #部署模式
... #其他参数
<application-file> #Python代码文件
[application-arguments] #传递给主类的主方法的参数
```

也可以通过spark-submit提交到Spark中运行，命令如下：

```python
$ /usr/local/spark/bin/spark-submit /usr/local/spark/mycode/python/WordCount.py
```
为了避免多余信息的干扰，可以修改log4j的日志信息显示级别：
* log4j.rootCategory=INFO, console -> log4j.rootCategory=ERROR, console
### 3.4 Spark集群环境搭建
搭建Hadoop集群环境：http://dblab.xmu.edu.cn/blog/1177-2/

启动Spark集群：

1. 首先启动Hadoop集群

   ```shell
   $ cd /usr/local/hadoop/
   $ cd sbin/start-all.sh
   ```


2. 启动Master节点

   ```shell
   $ cd /usr/local/spark/
   $ sbin/start-master.sh
   ```


3. 启动所有Slave节点

   ```shell
   $ sbin/start-slaves.sh
   ```

4. 在Master主机上打开浏览器，访问主页查看：http://master:8080

> 关闭节点的话，只需要把上面所有的start改为stop就行。
### 3.5 在集群上运行Spark应用程序
#### 3.5.1 采用独立集群管理器

**1. 在集群中运行应用程序**

* 向独立集群管理器提交应用，需要把spark://master:7077作为主节点参数递给spark-submit

* 可以运行Spark安装好以后自带的样例程序SparkPi，它的功能是计算得到pi的值（3.1415926）

  ```python
  $ cd /usr/local/spark/
  $ bin/spark-submit --master spark://master:7077 /usr/local/spark/examples/src/python/pi.py 2>&1 | grep "Pi is roughly"
  ```

**2. 在集群中运行pyspark**

也可以用pyspark连接到独立集群管理器上

```python
$ cd /usr/local/spark/
$ bin/pyspark --master spark://master:7077
>>> textFile=sc.textFile("hdfs://master:9000/README.md")
>>> textFile.count()
105
>>> textFile.first()
'# Apache Spark'
```
#### 3.5.2 采用Hadoop YARN管理器

**1. 在集群中运行应用程序**

需要把yarn-client或yarn-cluser作为主节点参数递给spark-submit

```python
$ cd /usr/local/spark/
$ bin/spark-submit --master yarn-client /usr/local/spark/example/src/main/python/pi.py
```
**2. 在集群上运行pyspark**

也可用pyspark连接到采用YARN作为集群管理器的集群上

```python
$ bin/pyspark --master yarn
>>> textFile=sc.textFile("hdfs://master:9000/REASDME.md")
>>> textFile.count()
105
>>> textFile.first()
'# Apache Spark'
```
查看集群信息：http://master:8088/cluster

## 4. RDD编程
### 4.1 RDD编程
#### 4.1.1 RDD创建
**1.从文件系统中加载数据创建RDD**

```shell
>>> lines=sc.textFile("file://usr/local/spark/.../*.txt")
>>> lines.foreach(print)
Hadoop is good
Spark is fast
Spark is better
```
**2. 从分布式文件系统HDFS中加载数据**

```shell
>>> lines=sc.textFile("hdfs://localhost:9000/usr/hadoop/*.txt")
>>> lines=sc.textFile("/usr/hadoop/*.txt")
>>> lines=sc.textFile("word.txt")
```
> 三条语句完全等价

**3. 通过并行集合（列表）创建RDD**

可以调用 `SparkContext`的 `parallelize` 方法，在Driver中一个已经存在的集合（列表）上创建

```shell
>>> array=[1,2,3,4,5]
>>> rdd=sc.parallelize(array)
>>> rdd.foreach(print)
1
2
3
4
5
```
#### 4.1.2 RDD操作
**1. 转换操作**

对于RDD而言，每一个转换操作都会得到不同的RDD，供给下一个RDD使用，但转换操作得到的RDD是惰性的，只会记录发生转换的轨迹，并不会发生真的计算，直到遇到第一次行动操作。

常见的转换操作

|操作|含义|
|:---|
|filter(func)|筛选出满足函数func的元素，并返回一个新的数据|
|map(func)|将每个元素传递到函数func中，并将结果返回为一个新的数据集|
|flatMap(func)|与map()相似，但每个输入元素都可以映射到0或多个输出结果|
|groupByKey()|应用于（K,V）键值对的数据集时，返回一个新的（K,Iterable）形式的数据集|
|reduceByKey(func)|应用于（K,V）键值对的数据集时，返回一个新的（K,V）形式的数据集，其中每个值是将每个key传递到函数func中进行聚合后的结果|
例1：<font color='red'>filter(func)</font>

```shell
>>> lines=sc.textFile("file:///usr/local/spark/mycode/rdd/word.txt")
>>> linesWithSpark=lines.filter(lambda line: "Spark in line")
>>> linesWithSpark.foreach(print)
Spark is better
Spark is fast
```
例2：<font color='red'>map(func)</font>
```shell
>>> data=[1,2,3,4,5]
>>> rdd1=sc.parallelize(data)
>>> rdd2=rdd1.map(lambda x:x+10)
>>> rdd2.foreach(print)
11
12
13
14
15
```
另外一个实例：
```shell
>>> lines=sc.textFile("file:///usr/local/spark/mycode/rdd/word.txt")
>>> words=lines.map(lambda line: line.split(" "))
>>> words.foreach(print)
['Hadoop','is','good']
['Spark','is','fast']
['Spark','is','better']
```
例3：<font color='red'>flatMap(func)</font>
```shell
>>> lines=sc.textFile("file:///usr/loca/spark/mycode/rdd/word.txt")
>>> words=lines.flatMap(lambda line: line.split(" "))
```
例4：<font color='red'>groupByKey()</font>

`groupByKey()`应用于（K,V）键值对的数据集时，返回一个新的（K,Iterable）形式的数据集

```python
>>> words=sc.parallelize(["Hadoop",1),("is",1),("good",1),("Spark",1),("is",1),("fast",1),("Spark",1)])
>>> words1=words.groupByKey()
>>> words1.foreach(print)
('Hadoop',<pyspark.resultiterable.ResultIterable object at 0x7fb210552c88>)
('better',<pyspark.resultiterable.ResultIterable object at 0x7fb210552e80>)
('fast',<pyspark.resultiterable.ResultTterable object at 0x7fb210552c88>)
...
```
例5：<font color='red'> reduceByKey(func)</font>
`reduceByKey(func)`应用于（K,V）键值对的数据集时，返回一个新的（K,V）形式的数据集，其中的每个值是将每个 `key` 传递到函数`func`中进行聚合后得到的结果

```shell
>>> words=sc.parallelize([("Hadoop",1),("is",1),("good",1),("Spark",1),("is",1),("fast",1),("Spark",1),("is",1),("better",1)])
>>> words1=words.reduceByKey(lambda a,b:a+b)
>>> words1.foreach(print)
('good',1)
('Hadoop',1)
('better',1)
('Spark',2)
('fast',1)
('is',3)
```
**2. 行动操作**

行动操作是真正触发计算的地方

表 常用的RDD行动操作API

|操作|含义|
|:---|:--|
|count()|返回数据集中的元素个数|
|collect()|以数组的形式返回数据集中的所有元素|
|first()|返回数据集中的第一个元素|
|take(n)|以数组的形式返回数据集中的前n个元素|
|reduce(func)|通过函数func（输入两个参数并返回一个值）聚合数据集中的元素|
|foreach(func)|将数据集中的每个元素传递到函数func中运行|

```shell
>>> rdd=sc.parallelize([1,2,3,4,5])
>>> rdd.count()
5
>>> rdd.first()
1
>>> rdd.take(3)
[1,2,3]
>>> rdd.reduce(lambda a,b:a+b)
15
>>> rdd.collect()
[1,2,3,4,5]
>>> rdd.foreach(lambda elem:print(elem))
1
2
3
4
5
```
**3. 惰性机制**

下面给出一段简单的代码来反映“惰性机制”

```shell
>>> lines=sc.textFile("file:///usr/local/spark/mycode/rdd/word.txt")
>>> lineLengths=lines.map(lambda s:len(s))
>>> totaoLength=lineLengths.reduce(lambda a,b:a+b)
>>> print(totalLength)
```
#### 4.1.3 持久化
在Spark中，RDD采用惰性求值的机制，每次遇到行动操作，都会从头开始执行计算。每次调用行动操作，都会触发一次从头开始的计算，这对迭代计算而言，代价很大，举例：

```shell
>>> list=["hadoop","Spark","Hive"]
>>> rdd=sc.parallelize(list)
>>> print(rdd.count()) //行动操作，触发一次真正从头到尾的计算
3
>>> print(",".join(rdd.collect())) //行动操作，触发一次真正从头到尾的计算
Hadoop,Spark,Hive
```
可以通过持久化（缓存）机制避免这种重复计算的开销

可以使用 `persist()` 方法对一个RDD标记为持久化，持久化只针对遇到第一个行动操作时，才会把计算结果持久化
`persist()` 的圆括号中包含的是持久化级别参数：

* `persist(MEMORY_ONLY):`  表示将RDD作为反序列化的对象存储于JVM中，如果内存不足，就要按照LRU原则替换缓存中的内容
* `persist(MEMORY_AND_DISK)` 表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，超过的分区将会被存放在硬盘上
* 一般而言，使用 `cache()` 方法时，会调用 `persist(MEMORY_ONLY)`
* 可以使用 `unpersist()` 方法手动地把持久化的RDD从缓存中移除

针对上面的实例，增加持久化语句以后的执行过程如下：
```shell
>>> list=["Hadoop","Spark","Hive"]
# 会调用persist(MEMORY_ONLY)，但是，语句执行到这里并不会缓存rdd，因为这时rdd还没有被计算生成
>>> rdd=sc.parallelize(list)
>>> rdd.cache()
# 第一次行动操作，触发一次真正的从头到尾的计算，这时上面的rdd.cache()才会被执行，放入缓存中
>>> print(rdd.count())
# 第二次行动操作，不需要触发从头到尾的计算，只需要重复使用上面缓存中的rdd
>>> print(",".join(rdd.collect()))
Hadoop,Spark,Hive
```
#### 4.1.4 分区
RDD是弹性分布式数据集，通常RDD很大，会被分成很多个分区，分别保存在不同的节点上

**1. 分区的作用**

* 增加并行度
* 减少通信开销

**2. RDD分区原则**

RDD分区的一个原则为使得分区的个数尽量等于集群中的CPU核心（core）数据对于不同的Spark部署模式而言，均可通过设置 `spark.default.parallelism` 这个参数的值，来配置默认的分区数目，一般来说：

* 本地模式：默认为本地机器的CPU数目，若设置了`local[N]`则默认为N
* Apache Mesos：默认的分区数为8
* Standalone或YARN：在"集群中所有CPU核心数目总和"和"2"二者中取较大值作为默认值

**3.设置分区的个数**

（1）创建RDD时手动指定分区个数

在调用textFile()和parallelize()方法的时候手动指定分区个数即可，语法格式如下：

`sc.textFile(path,paratitionNum)`

其中，path参数用于指定要加载的文件的地址，partitionNum参数用于指定分区个数。

```shell
>>> list=[1,2,3,4,5]
>>> rdd=sc.parallelize(list,2) //设置两个分区
```
（2）使用reparitition方法重新设置分区个数

通过转换操作得到新RDD时，直接调用repartition方法即可，例如：

```shell
>>> data=sc.parallelize([1,2,3,4,5],2)
# 显示data这个RDD的分区数量
>>> len(data.glom().collect())
2
# 对data这个RDD进行重新分区
>>> rdd=data.repartition(1)
# 显示rdd这个RDD的分区数量
>>> len(rdd.glom().collect())
1
```
4 自定义分区方法

Spark提供了自带的HashPartitioner（哈希分区）与RangePartitoner（区域分区），能够满足大多数应用场景的

需求，但也支持自定义分区原则

#### 4.1.5 一个综合实例

实例：根据key值的最后一位数字，写到不同的文件
例如：
10写入到part-00000
11写入到part-00001
.
.
.
19写入到part-00009

```python
from pyspark import SparkConf, SparkContext

def MyPartitioner(key):
    print("MyPartitioner is running")
    print("The key is %d"%key)
    return key%10
    
def main():
    print("The main function is running")
    conf=SparkConf().setMaster("local").setAppName("MyApp")
    sc=SparkContext(conf=conf)
    data=sc.parallelize(range(10),5)
    data.map(lambda x:(x,1)).partitionBy(10,MyPartitioner)
    .map(lambda x:x[0]).
    saveAsTextFile("file:///user/local/spark/mycode/rdd/partitioner")
    
if __name__="__main__":
    main()
```
使用如下命令运行TestPartitioner.py:
```python
$ cd /usr/local/spark/mycode/rdd
$ python3 TestPartitioner.py
```
或者，使用如下命令运行TestPartitioner.py
```python
$ cd /usr/local/spark/mycode/rdd
$ /usr/local/spark/bin/spark-submit TestPartitioner.py
```
程序运行结果会返回如下信息：
```
The main function is running
MyPartitioner is running 
The key is 0
MyPartitioner is running 
The key is 1
......
Mypartitioner is running
The key is 9
```
例：假设有一个本地文件word.txt，里面包含了很多行文本，每行文本由多个单词构成，单词之间用空格分隔。可以使用如下语句进行词频统计（即统计每个单词出现的次数）：
```shell
>>> lines=sc.textFile("file:///usr/local/spark/mycode/rdd/word.txt")
>>> wordCount=lines.flatMap(lambda line:line.split("")).
    map(lambda word:(word,1)).
    reduceByKey(lamdbda a,b:a+b)
>>> print(wordCount.collect())
[('good',1),('Spark',2),('is',3),('better',1),('Hadoop',1),('fast',1)]
```
### 4.2 键值对RDD
#### 4.2.1 键值对RDD的创建
**1. 第一种创建方式：从文件中加载**

```shell
>>> lines=sc.textFile("file:///usr/local/spark/mycode/pairrdd/word.txt")
>>> pairRDD=lines.flatMap(lambda line:line.split(" ")).map(lambda word:(word,1))
>>> pairRDD.foreach(print)
('l',1)
('love',1)
('Hadoop',1)
...
```
**2. 第二种创建方式：通过并行集合（列表）创建RDD**

```shell
>>> list=['Hadoop','Spark','Hive','Spark']
>>> rdd=sc.parallelize(list)
>>> pairRDD=rdd.map(lambda word:(word,1))
>>> pairRDD.foreach(print)
(Hadoop,1)
(Spark,1)
(Hive,1)
(Spark,1)
```
#### 4.2.2 常用的键值对RDD转换操作

`reduceByKey(func)`: 使用func函数合并具有相同键的值

```shell
>>> pairRDD=sc.parallelize([("Hadoop",1),("Spark",1),("Hive",1),("Spark",1)])
>>> pairRDD.reduceByKey(lambda a,b:a+b).foreach(print)
('Spark',2)
('Hive',1)
('Hadoop',1)
```
`groupByKey()`: 对具有相同键的值进行分组

```shell
>>> list=[("spark",1),("spark",2),("hadoop",3),("hadoop",5)]
>>> pairRDD=sc.parallelize(list)
>>> pairRDD.groupByKey(()
PythonRDD[27] at RDD at PythonRDD.scala:48
>>> pairRDD.groupByKey().foreach(print)
('hadoop',<pyspark.resultiterable.ResultIterable object at 0x7f2c1093ecf8>)
('spark',<pyspark.resultiterable.ResultTterable object at 0x7f2c1093ecf8>)
```
reduceByKey和groupByKey的区别
* `reduceByKey` 用于对每个key对应的多个value进行merge操作，最重要的是它能够在本地先进行merge操作，并且merge操作可以通过函数自定义
* `groupByKey` 也是对每个key进行操作，但只生成一个sequence，groupByKey本身不能自定义函数，需要先用groupByKey生成RDD，然后才能对此RDD通过map进行自定义函数操作
例：
```shell
>>> words=["one","two","two","three","three"]
>>> wordPairsRDD=sc.parallelize(words).map(lambda word:(word,1))
>>> wordCountsWithReduce=wordPairsRDD.reduceByKey(lambda a,b:a+b)
>>> wordCountsWithReduce.foreach(print)
('one',1)
('two',2)
('three',3)
>>> wordCountsWithGroup=wordPairsRDD.groupByKey().map(lambda t:(t[0],sum(t[1])))
>>> wordCountsWithGroup.foreach(print)
('two',2)
('three',3)
('one',1)
```
`keys`: 只会把Pair RDD中的key返回形成一个新的RDD

```shell
>>> list=[("Hadoop",1),("Spark",1),("Hive",1),("Spark",1)]
>>> pairRDD=sc.parallelize(list)
>>> pairRDD.keys().foreach(print)
Hadoop
Spark
Hive
Spark
```
`values`：只会把Pair RDD中的value返回形成一个新的RDD。
```shell
>>> list=[("Hadoop",1),("Spark",1),("Hive",1),("Spark",1)]
>>> pairRDD=sc.parallelize(list)
>>> pairRDD.values().foreach(print)
1
1
1
1
```
`sortByKey()`：返回一个根据键排序的RDD

```shell
>>> list=[("Hadoop",1),("Spark",1),("Hive",1),("Spark",1)]
>>> pairRDD=sc.parallelize(list)
>>> pairRDD.foreach(print)
("Hadoop",1)
("Spark",1)
("Hive",1)
("Spark",1)
>>> pairRDD.sortByKey().foreach(print)
('Hadoop',1)
('Hive',1)
('Spark',1)
('Spark',1)
```
`sortByKey()`和`sortBy()`
```shell
d1=sc.parallelize(['c',8),('b',25),('c',17),('a',42),('b',4),('d',9),('e',17),('c',2),('f',29),('g',21),('b',9)])
>>> d1.reduceByKey(lambda a,b:a+b).sortByKey(False).collect()
[('g',21),('f',29),('e',17),('d',9),('c',27),('b',38'),('a',42)]
```
![image.png-209.7kB][12]

`mapValues(func)`：对键值对RDD中的每个value都应用一个函数，但是，key不会发生变化

![image.png-99.5kB][13]

`join`
表示内连接，对于内连接，对于给定的两个输入数据集（K,V1）和（K,V2），只有在两个数据集中都存在的key才会被输出，最终得到一个（K,(V1,V2))类型的数据集。

![image.png-71.9kB][14]

一个综合实例

题目：给定一组键值对("spark",2), ("hadoop",6), ("hadoop",4), ("spark",6), 键值对的key表示图书名称，value表示某天图书销量，请计算每个键对应的平均值，也就是计算每种图书的每天平均销量。

![image.png-60.3kB][15]

### 4.3 数据读写

#### 4.3.1 文件数据读写

**1. 本地文件系统的数据读写**

把RDD写入到文本文件中

![image.png-39.1kB][16]

如果想再次把数据加载在RDD中，只要使用writeback这个目录即可，如下：

![image.png-20.1kB][17]

2 分布式文件系统HDFS的数据读写
可以使用SaveAsTextFile()方法把RDD中的数据保存到HDFS文件中，命令如下：
```shell
>>> textFile=sc.textFile("word.txt")
>>> textFile.SaveAsTextFile("writeback")
```
#### 4.3.2 读写HBase数据

**0. HBase简介**

* HBase是Google Big Table的开源实现

![image.png-67.1kB][18]

HBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、列族、列限定符和时间戳

![image.png-99.4kB][19]

* HBase中需要根据行键、列族、列限制符和时间戳来确定一个单元格，因此，可以视为一个“四维坐标”，即[行

  键，列族，列限定符，时间戳]
|键|值|
|---|
|['201505003','Info','email',1174184619081|'xie@qq.com'|
|['201505003','Info','email',1174184620720|'you@163.com'|

首先，请参照厦大数据库实验室博客完成HBase的安装（伪分布式模式）：

http://dblab.xmu.edu.cn/blog/install-hbase/

因HBase是伪分布式模式，需要调用HDFS，因此，请首先输入下面命令启动Hadoop:

```shell
$ cd /usr/local/hadoop
$ ./sbin/start-all.sh
```
下面就可以启动HBase，命令如下：
```shell
$ cd /usr/loca/hbase
# 启动HBase
$ ./bin/start-hbase.sh 
# 启动hbase shell
$ ./bin/hbase shell
```
如果里面有一个名为student的表，请使用如下命令删除：
```shell
hbase> disable 'student'
hbase> drop 'student'
```
**1. 创建一个HBase表**

下面创建一个student表，要在这个表中录入如下数据：

|id|name|gender|age|
|:---|:---|:---|:---|
|1|Xueqian|F|23|
|2|Weiliang|M|24|

```shell
// 首先录入student表的第一个学生记录
hbase create 'student','info'
hbase> put 'student','1','info:name','Xueqian'
hbase> put 'student','1','info:gender','F'
hbase> put 'student','1','info:age','23'
// 然后录入student表的第二个学生记录
hbase> put 'student','2','info:name','Weiliang'
hbase> put 'student','2','info:gender','M'
hbase> put 'student','2','info:age','24'
```
**2. 配置Spark**

把HBase的lib目录下的一些jar文件拷贝到Spark中，这些都是编程时需要引入的jar包，需要拷贝的jar文件包括：所有hbase开头的jar文件、guava-12.0.1jar、htrace-core-3.1.0-incubating.jar和protobuf-java-2.5.0.jar

执行命令

```shell
$ cd /usr/local/spark/jars
$ mkdir hbase
$ cd hbase
$ cp /usr/loca/hbase/lib/hbase*.jar./
$ cp /usr/local/hbase/lib/guava-12.0.1.jara ./
$ cp /usr/loca/hbase/lib/htrace-core-3.1.0-incubating.jar./
$ cp /usr/local/hbase/lib/protobuf-java-2.5.0.jar./
```
此外，在Spark2.0以上版本中，缺少把HBase数据转换为Python可读写数据的jar包，需要另行下载，可以访问下面网址下载spark-examples_2.11-1.6.0-typesafe-001.jar:

http://mvnrepository.com/artifact/org.apache.spark/spark-examples_2.11/1.6.0-typesafe-001

下载以后保存到"/usr/local/spark/jars/hbase/"目录中
编辑并配置spark-env.sh文件，告知HBase的jar包所在地

```shell
$ cd /usr/local/spark/conf
$ vim spark-env.sh
```
打开spark-env.sh文件以后，可以在文件最前面增加下面一行内容：
```shell
export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop
classpath):$(/usr/local/hbase/bin/hbase classpath):/usr/loca/spark/jars/hbase/*
```
这样后面的编译和运行过程不会出错

**3. 编写程序读取HBase数据**

如果让Spark读取HBase，就需要使用SparkContext提供的newAPIHadoopRDD这个API将表的内容以RDD的形式加载到Spark中。
![image.png-99.5kB][20]

执行该代码文件，命令如下：

```shell
$ cd /usr/lcoa/spark/mycode/rdd
$ /usr/local/spark/bin/spark-submit SparkOperateHBase.py
```
执行后得到如下结果：
![image.png-98kB][21]

**4 编写程序向HBase写入数据**

下面编写应用程序把表中的两个学生信息插入到HBase的student表中

<table>
<tr>
  <th rowspan="2">id</th>
  <th colspan="3">info</th>
</tr>
<tr>
  <td>name</td>
  <td>gender</td>
  <td>age</td>
</tr>
<tr>
  <td>3</td>
  <td>Rongcheng</td>
  <td>M</td>
  <td>26</td>
</tr>
<tr>
  <td>4</td>
  <td>Guanhua</td>
  <td>M</td>
  <td>27</td>
</tr>
</table>

在SparkWriteHBase.py文件中输入下面代码：

![image.png-117.6kB][22]
```shell
$ cd /usr/local/spark/mycode/rdd
$ /usr/local/spark/bin/spark-submit SparkWriteHBase.py
```
切换到HBase Shell中，执行命令`hbase>scan'student'`查看student表

![image.png-91.2kB][23]

### 4.4 综合案例
#### 4.1 案例

**1. 求TOP值**

任务描述：

orderid,userid,payment,productid

![image.png-57.7kB][24]

求Top N个payment值

TopN.py

![image.png-94.6kB][25]

#### 4.2 案例2：文件排序

任务描述：

 有多个输入文件，每个文件的每一行内容均为一个整数。要求读取所有文件中的整数，进行排序后，输出到一个新的文件中，输出的内容个数为每行两个整数，第一个整数为第二个整数的排序位次，第二个整数的排序位次，第二个整数为原待排序的整数
 ![image.png-16kB][26]![image.png-14.4kB][27]
 FileSort.py
 ![image.png-74.4kB][28]

#### 4.3 案例3：二次排序

任务要求：

对于一个给定的文件（数据如file1.txt所示），请对数据进行排序，首先根据第1列数据降序排序，如果第一列数据相等，则根据第2列数据降序排序

![image.png-19.5kB][29]

![image.png-73.2kB][30]

SecondarySortKey.py代码如下：
```shell
#!/usr/bin/env/ python3
from opreator import gt
from pyspark import SparkContext, SparkConf

class SecondarySortKey():
    def __init__(self,k):
    self.column1=k[0]
    self.column2=k[1]
    
    def __gt__(self,other):
        if other.column1==self.column1:
            return gt(self.column2,other.column2)
        else:
        return gt(self.column1,otehr.column1)
        
def main():
    conf =SparkConf().setAppName('spark_sort').setMaster('local[1]')
    sc=SparkContext(conf=conf)
    file="file:///usr/local/spark/mycode/rdd/secondarysort/file4.txt"
    rdd1=sc.textFile(file)
    rdd2=rdd1.filter(lambda x:(len(x.strip())>0))
    rdd3=rdd2.map(lambda x:(int(x.split(" ")[0]),int(x,split(" ")[1])),x))
    rdd4=rdd3.map(lambda x: (SecondarySortKey(x[0],x[1]))
    rdd5=rdd4.sortByKey(False)
    rdd6=rdd5.map(lambda x:x[1])
    rdd6.foreach(print)
    
if __name__='__main__':
    main()
```
## 5 Spark SQL

### 5.1 Spark SQL简介

#### 5.1.1 从Shark说起
![绘图1.png-21.4kB][31]

Spark即Hive on Spark,为了实现与Hive兼容，Shark在HiveQL方面重用了Hive中HiveQL的解析、逻辑执行计划翻译、执行计划优化等逻辑，可以近似认为仅将物理执行计划从MapReduce作业替换成了Spark上的RDD操作
Shark的设计导致了两个问题：

* 执行计划完全依赖于Hive，不方便添加新的优化策略
* Spark是线程级并行，MapReduce是进程级并行，兼容存在问题
#### 5.1.2 Spark SQL设计

![绘图1.png-25.4kB][32]
* Spark SQL增加了DataFrame(即带有Schema信息的RDD),使用户可以在Spark SQL中执行SQL语句，数据既可以来自RDD，也可以是Hive、HDFS、Cassandra等外部数据源，还可以是JSON格式的数据
* Spark SQL目前支持Scala、Java、Python三种语言，支持SQL-92规范

#### 5.1.2 为什么推出Spark SQL

* 关系数据库在大数据时代已经不能满足要求
* 在实际应用中，经常需要融合关系查询和复杂分析算法

Spark SQL填补了这一鸿沟：
* 首先，可以提供DataFrame API,可以对内部和外部各种数据源执行各种关系型操作
* 其次，可以支持大数据中的大量数据源和数据分析算法
### 5.2 DataFrame概述
* DataFrame的推出，使Spark具备了处理大规模结构化数据的能力，不仅比原有的RDD转化方式更加简单易用，且获得了更高的计算性能
* Spark能够轻松实现从MySQL到DataFrame的转化，并且支持SQL查询
* RDD是分布式的Java对象的集合，但是，对象内部结构对于RDD而言却是不可知的
* DataFrame是一种以RDD为基础的分布式数据集，提供了详细的结构信息
### 5.3 DataFrame的创建
* SparkSession支持从不同的数据源加载数据，并把数据转换成DataFrame，并且支持把DataFrame转换成SQLContext自身中的表，然后使用SQL语句来操作数据。

可以通过如下语句创建一个SparkSession对象：
```shell
from pyspark import SparkContext,SparkConf
from pyspark.sql import SparkSession
spark=SparkSession.builder.config(conf=SparkConf()).getOrCreate()
```
实际上，在启动进入pyspark以后，pyspark就默认提供了一个SparkContext对象（名称为sc）和一个SparkSession对象（名称为spark）

在创建DataFrame时，可以使用spark.read操作，从不同类型的文件中加载数据创建DataFrame，例如：

* spark.read.txt("people.txt"):  读取文本文件people.txt创建DataFrame
* spark.read.json("people.json"): 读取people.json文件创建DataFrame;在读取本地文件或HDFS文件时，要注意给出正确的文件路径
* spark.read.parquet("people.parquet"): 读取people.parquet文件创建DataFrame

也可使用如下格式的语句：
* spark.read.format("text").load("people.txt"): 读取文本文件people.json创建DataFrame;
* spark.read.format("json").load("people.json"): 读取JSON文件people.json创建DataFrame;
* spark.read.format("parquet").load("people.parquet"): 读取Parquet文件people.parquet创建DataFrame。

一个实例：
* 在"/usr/local/spark/examples/src/main/resources/"这个目录下，这个目录下有两个样例数据people.json和people.txt

people.json文件的内容如下：
```shell
{"name":"Michael"}
{"name":"Andy","age":30}
{"name":"Justin","age":19}
```
people.txt文件的内容如下：
```shell
Michael,29
Andy,30
Justin,19
```
![image.png-32.7kB][33]

### 5.4 DataFrame的保存

可以使用spark.write保存成不同格式的文件，如下：

* df.write.text("people.txt")

* df.write.json("people.json")

* df.write.parquet("people.parquet")

或者也可以使用如下格式的语句：

* df.write.format("text").save("people.txt")

* df.write.format("json").save("people.json")

* df.write.format("parquet").save("people.parquet")

下面从示例文件people.json中创建一个DataFrame，名称为peopleDF,把peopleDF保存到另外一个JSON文件中，然后，再从peopleDF中选取一个列(即name列),把该列数据保存到一个文本文件中

![image.png-62.4kB][34]

会生成一个名为newpeople.json的目录（不是文件）和一个名为newpeople.txt的目录（不是文件）
### 5.5 DataFrame的常用操作
可以执行一些常用的DataFrame操作

```>>> df=spark.read.json("people.json")```

printSchema()

 ![image.png-26.8kB][35]

select()
![image.png-29.3kB][36]

filter()
![image.png-25kB][37]

groupBy()
![image.png-27.5kB][38]

sort()
![image.png-63.1kB][39]

### 5.6 从RDD转换得到DataFrame

#### 5.6.1 利用反射机制推断RDD模式

在"/usr/local/spark/examples/src/main/resources/"目录下，有个Spark安装时自带的样例数据people.txt，其内容如下：

```
Michael,29
Andy,30
Justin,19
```
现在要把people.txt加载到内存中生成一个DataFrame，并查询其中的数据

![image.png-140.9kB][40]

#### 5.6.2 使用编程方式定义RDD模式

当无法提前获知数据结构时，就需要采取编程方式定义RDD模式。

比如，现在需要通过编程方式把people.txt加载进来生成DataFrame，并完成SQL查询。
![image.png-35.6kB][41]
![image.png-129kB][42]
![image.png-57.8kB][43]
### 5.7 使用Spark SQL读写数据库
Spark SQL可以支持Parquet、JSON、Hive等数据源，并且可以通过JDBC连接外部数据源
#### 5.7.1 准备工作
MySQL数据库配置地址：http://dblab.xmu.edu.cn/blog/install-mysql/
在Linux中启动MySQL数据库

```shell
$ service mysql start
$ mysql -u root -p
# 屏幕会显示你输入密码
```
输入下面SQL语句完成数据库和表的创建：
```shell
mysql> create database spark;
mysql> use spark;
mysql> create table student(id int(4), name char(20),gender char(4),age int(4));
mysql> insert into student values(1,'Xueqian','F',23);
mysql> insert into student values(2,'Weiliang','M',24);
mysql> select * from student;
```
* 下载MySQL的JDBC驱动程序，比如mysql-connector-java-5.1.40.tar.gz
* 把该驱动程序拷贝到spark的安装目录"/usr/local/spark/jars"下
* 启动pyspark
```shell
$ cd /usr/local/spark
$ cd ./bin/pyspark
```
#### 5.7.2 读取MySQL数据库中的数据

执行下面语句，连接数据库，读取数据，显示：

![image.png-82.8kB][44]

#### 5.7.3 向MySQL数据库写入数据

在MySQL数据库中创建了一个名称为spark的数据库，并创建了一个名称为student的表创建后，查看一下数据库内容：
![image.png-80.5kB][45]

现在开始编程，往spark.student表中插入两条记录

![image.png-92.1kB][46]![image.png-138.5kB][47]

可以看一下效果，看看MySQL数据库中的spark.student表发生了什么变化

 ![image.png-48.7kB][48]

## 6. Spark Streaming

### 6.1 流计算概述
#### 6.1.1 静态数据和流数据
* 很多企业为了支持决策分析而构建的数据仓库系统，其中存放的大量历史数据就是静态数据。
* 近年来，在Web应用、网络监控、传感监测等领域，兴起了一种新的数据密集型应用——流数据，即数据以大量、快速、时变的流形式持续到达。
#### 6.1.2 批量计算和实时计算
* 批量计算：充裕时间处理静态数据，如Hadoop
* 流计算：流数据必须采用流计算，响应时间为秒级
#### 6.1.3 流计算概念
* 流计算：实时获取来自不同数据源的海量数据，经过实时分析处理，获得有价值的信息
* 流计算系统需满足以下条件：
 * 高性能：处理大数据的基本要求，如每秒处理几十万条数据
 * 海量式：支持TB级甚至是PB级的数据规模
 * 实时性：保证较低的延迟时间，达到秒级别，甚至是毫秒级别
 * 分布式：支持大数据的基本架构，必须能够平滑扩展
 * 可靠性：能可靠地处理流数据
#### 6.1.4 流计算框架
* 商业级：IBM InfoSphere Streams和IBM StreamBase
* 开源级：
 * Twitter Storm，免费、开源，可简单高效、可靠地处理大量的流数据
 * Yahoo! S4，通用的，分布式的、可扩展的、
* 自主开发的流计算框架：Facebook Dstream和银河流数据处理平台（淘宝）
#### 6.1.5 流计算处理流程
**1. 概述**
传统的数据处理流程，需要先采集数据并存储在关系型数据库等数据管理系统中，之后由用户通过查询操作和数据管理系统进行交互，一是存储的数据是旧的，二是需要用户主动发出查询来获取结果。流计算的处理流程一般包括三个阶段：数据实时采集、数据实时计算、实时查询服务。

**2. 数据实时采集**

数据实时采集阶段通常采集多个数据源的海量数据，需要保证实时性、低延迟与稳定可靠。
——Facebook的Scribe
——LinkeIn的Kafka
——淘宝的Time Tunnel
——基于Hadoop的Chukwa和Flume

**3. 数据实时计算**

数据实时计算阶段对采集的数据进行实时的分析和计算，并反馈实时结果。

经流处理系统处理后的数据，可视情况进行存储，以便之后再进行分析计算。在时效性要求较高的场景中，处理之后的数据也可以直接丢弃。

* 实时查询
  实时查询服务：经由流计算框架得出的结果可供用户进行实时查询、展开或存储。

* 因此流计算系统与传统的数据处理系统有如下不同
> * 流计算系统处理的是实时的数据，而传统的数据处理是预先存储好的静态数据
> * 用户通过流处理系统获取的是实时结果
> * 流处理系统无需用户主动发出查询
### 6.2 Spark Streaming
#### 6.2.1 Spark Streaming 设计
* Spark Streaming可整合多种输入数据源，如Kafka、Flume、HDFS，甚至是普通的TCP套接字。经处理后的数据可存储至文件系统、数据库，或显示在仪表盘里。
* 基本原理：是将实时输入数据流以时间片（秒级）为单位进行拆分，然后经Spark引擎以类似批处理的方式处理每个时间片数据。
* Spark Streaming最主要的抽象是DStream（Discretized Stream，离散化数据流），表示连续不断的数据流。在内部实现上，Spark Streaming的输入数据按照时间片（如1秒）分成一段一段，每一段数据转换为Spark中的RDD，这些分段就是Dstream，并且对DStream的操作都最终转变为对相应的RDD的操作。
#### 6.2.2 Spark Streaming 与 Storm的对比
* Spark Streaming和Storm最大的区别在于，Spark Streaming无法实现毫秒级的流计算，而Storm可以实现毫秒级响应
* Spark Streaming构建在Spark上，一方面是因为Spark的低延迟执行引擎（100ms+）可以用于实时计算，另一方面，相比于Storm，RDD数据集更容易作高效的容错处理
* Spark Streaming采用的小批量处理的方式使得它可以同时兼容批量和实时数据处理的逻辑和算法，因此，方便了一些需要历史数据和实时数据联合分析的特定应用场景。
#### 6.2.3 从"Hadoop+Storm"架构转向Spark架构
* 采用Spark架构具有如下优点：
* 实现一键式安装和配置、线程级别的任务监控和告警；
* 降低硬件集群构建、软件维护、任务监控和应用开发的难度；
* 便于做成统一的硬件、计算平台资源池。
### 6.3 DStream操作概述
#### 6.3.1 Spark Streaming工作机制
* 在Spark Streaming中，会有一个组件Reciver，作为一个长期运行的task跑在一个Executor上
* 每个Receiver都会负责一个input DStream（比如从文件中读取数据的文件流，比如套接字流，或者从Kafka中读取的一个输入流等等）
* Spark Streaming通过input DStream与外部数据源进行连接，读取相关数据
#### 6.3.2 Spark Streaming程序的基本步骤
1. 通过创建输入DStream来定义输入源
2. 通过对DStream应用转换操作和输出操作来定义流计算
3. 用`streamingContext.start()`来开始接收数据和处理流程
4. 通过`streamingContext.awaitTermination()`方法来等待处理结束（手动结束或因为错误而结束）
5. 可以通过`streamingContext.stop()`来手动结束流计算进程
#### 6.3.3 创建StreamingContext对象
* 如果要运行一个Spark Streming程序，就需要首先生成一个StreamingContext对象，它是Spark Streaming程序的主入口
* 可以从一个SparkConf对象创建一个StreamingContext对象。
* 在pyspark中的创建方法：进入pyspark以后，就已经获得了一个默认的SparkContext对象，也就是sc。因此，可以采用如下方式来创建StreamingContext对象：
```shell
>>> from pyspark.streaming import StreamingContext
>>> ssc=StreamingContext(sc,1)
```
如果编写的是一个独立的Spark Streaming程序，而不是在pyspark中运行，则需要通过如下方式创建StreamingContext对象：
```shell
from pyspark import SparkContext,SparkConf
from pyspark.streaming import StreamingContext
conf=SparkConf()
conf.setAppName('TestDStream')
conf.setMaster('local[2]')
sc=SparkContext(conf=conf)
ssc=StreamingContext(sc,1)
```
### 6.4 基本输入源
#### 6.4.1 文件流

**1. 在pyspark中创建文件流**

```shell
$ cd /usr/local/spark/mycode
$ mkdir streaming
$ cd streaming
$ mkdir logfile
$ cd logfile
```
进入pyspark创建文件流。请另外打开一个终端窗口，启动进入pyspark
```shell
>>> from pyspark import SparkContext
>>> from pyspark.streaming import StreamingContext
>>> ssc=StreamingContext(sc,10)
>>> lines=ssc.textFileStream('file:///usr/local/spark/mycode/streaming/logfile')
>>> words=lines.flatMap(lambda line:line.split(''))
>>> wordCounts=words.map(lambda x:(x,1)).reduceByKey(lambda a,b:a+b)
>>> wordCounts.pprint()
>>> ssc.start()
>>> ssc.awitTermination()
```
上面在pyspark中执行的程序，一旦你输入ssc.start()以后，程序就开始自动进入循环监听状态，屏幕上会显示一

堆的信息，如下：

```shell
--------------------------------------
Time: 2018-12-30 15:35:30
--------------------------------------

--------------------------------------
Time: 2018-12-30 15:35:40
--------------------------------------

--------------------------------------
Time: 2018-12-30 15:35:50
--------------------------------------
```
在'usr/local/spark/mycode/streaming/logfile'目录下新建一个log.txt文件，就可以在监听窗口中显示词频统计结果

**2. <font color='red'>采用独立应用程序方式创建文件流</font>**

```shell
$ cd /usr/local/spark/mycode
$ cd streaming
$ cd logfile
$ vim FileStreaming.py
```
用vim编辑器新建一个FileStreaming.py代码文件，请在里面输入以下代码：
```shell
#!/usr/bin/env/python3
from pyspark import SparkContext,SparkConf
from pyspark.streaming import StreamingContext

conf=SparkConf()
conf.setAppName('TestDStream')
conf.setMaster('local[2]')
sc=SparkContext(conf=conf)
ssc=StreamingContext(sc,10)
lines=ssc.textFileStream('file:///usr/local/spark/mycode/streaming/logfile')
words=lines.flatMap(lambda line: line.split(''))
wordCounts=words.map(lambda x:(x,1).reduceByKey(lambda a,b:a+b)
wordCounts.pprint()
ssc.start()
ssc.awaitTermination()
```
```shell
$ cd /usr/local/spark/mycode/streaming/logfile/
$ /usr/local/spark/bin/spark-submit FileStreaming.py
```
#### 6.4.2 套接字流
* Spark Streaming可以通过Socket端口监听并接收数据，然后进行相应处理

<font color='red'>1. Socket工作原理</font>

![绘图1.png-37.1kB][49]

<font color='red'>2. 使用套接字流作为数据源</font>
```shell
$ cd /usr/local/spark/mycode
# 如果已经存在该目录，则不用创建
$ mkdir streaming
$ cd streaming
$ mkdir socket
$ cd socket
$ vim NetworkWordCount.py
```
请在NetworkWordCount.py文件中输入如下内容：
```python
#!/usr/bin/env python3
from __future__ import print_function
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

if __name__=="__main__":
    if len(sys.argv)!=3:
        print("Usage: NetworkWordCount.py <hostname> <port>",file=sys.stderr)
        exit(-1)
    sc=SparkContext(appName="PythonStreamingNetworkWordCount")
    ssc=StreamingContext(sc,1)
    lines=ssc.socketTextStream(sys.argv[1],int(sys.argv[2]))
    counts=lines.flatMap(lambda line: line.split("")).map(lambda word: (word,1)).reduceByKey(lambda a,b :a+b)
    counts.pprint()
    ssc.start()
    ssc.awaitTermination()
    
```
新打开一个窗口，启动nc程序：
```shell
$ nc -lk 9999
```
再新建一个终端（记作“流计算终端”），执行如下代码启动流计算：
```shell
$ cd /usr/local/spark/mycode/streaming/socket
$ /usr/loca/spark/bin/spark-submit NetWordCount.py localhost 9999
```
可以在nc窗口中随意输入一些单词，监听窗口就会自动获得单词数据流信息，在监听窗口每隔1秒就会打印出词频统计信息，大概会在屏幕上出现类似如下的结果：
```shell
-------------------------------------------------
Time: 2018-12-24 11:30:26
-------------------------------------------------
('Spark',1)
('love',1)
('l',1)
(spark,1)
```
<font color='red'>3. 使用Socket编程实现自定义数据源</font>

下面我们再前进一步，把数据源头的产生方式修改一下，不要使用nc程序，而是采用自己编写的程序产生Socket数据源

```shell
$ cd /usr/local/spark/mycode/streaming/socket
$ vim DataSourceSocket.py
```
```python
#!/usr/bin/env python3
import socket
# 生成socket对象
server=socket.socket()
# 绑定ip和端口
server.bind(('localhost',9999))
# 监听绑定的端口
server.listen(1)
while 1:
    # 为了方便识别，打印一个“我在等待”
    print("I'm waiting the connect...")
    # 这里用两个值接受，因为连接上之后使用的是客户端发来请求的这个实例，所以下面的传输要使用conn实例操作
    conn,addr=server.accpet()
    # 打印连接成功
    print("Connect success! Connection is from %s"% addr[0])
    # 打印正在发送数据
    print("Sending data...')
    conn.send('I love hadoop I love spark hadoop is good spark is fast'.encode())
    conn.close()
    print("Connection is broken.")
```
执行如下命令启动Socket服务端
```shell
$ cd /usr/local/spark/mycode/streaming/socket
$ /usr/local/spark/bin/spark-submit DataSourceSocket.py
```
启动客户端，即NetworkWordCount程序。新建一个终端（记作“流计算终端”），输入以下命令启动NetworkWordCount程序：
#### 6.4.3 RDD队列流
* 在调试Spark Streaming应用程序时，我们可以使用StreamingContext.queueStream(queueOfRDD)创建基于RDD队列的DStream
* 新建一个RDDQueueStream.py代码文件，功能是：每隔1秒创建一个RDD，Streaming每隔2秒就对数据进行处理
```python
#!/usr/bin/env python3

import time
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

if __name__="__main__":
    sc=SparkContext(appName="PythonStreamingQueueStream")
    ssc=StreamingContext(sc,2)
    #创建一个队列，通过该队列可以把RDD推给一个RDD队列流
    rddQueue=[]
    for i in range(5):
        rddQueue+=[ssc.sparkContext.parallelize([j for j in range(1,1001)],10)]
        time.sleep(1)
    #创建一个RDD队列流
    inputStream=ssc.queueStream(rddQueue)
    mappedStream=inputStream.map(lambda x:(x % 10,1))
    reduceStream=mappedStream.reduceByKey(lambda a,b: a+b)
    reducedStream.pprint()
    ssc.start()
    ssc.stop(stopSparkContext=True,stopGraceFully=True)
```
下面执行如下命令运行该程序：
```shell
$ cd /usr/local/spark/mycode/streaming/rddqueue
$ /usr/local/spark/bin/spark-submit RDDQueueStream.py
```
```python
-----------------------------------------------------
Time: 2018-12-31 15:42:15
----------------------------------------------------
(0,100)
(8,100)
(2,100)
(4,100)
(6,100)
(1,100)
(3,100)
(9,100)
(5,100)
(7,100)
```
### 6.5 高级数据源
#### 6.5.1 Kafka简介
* Kafka是一种高吞吐量的分布式发布订阅消息系统，用户可以通过Kafka系统发布大量的消息，同时也能实时订阅消费消息
* Kafka可以同时满足在线实时处理和批量离线处理
* 在公司的大数据生态系统中，可以把Kafka作为数据交换枢纽，不同类型的分布式系统（关系数据库、NoSQL数据库、流处理系统、批处理系统等），可以统一接入到Kafka，实现和Hadoop各个组件之间的不同类型数据的实时高效交换
* Broker
Kafka集群包含一个或多个服务器，这种服务器被称作broker
* Topic
每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic.(物理上不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上，但用户只需指定消息的Topic即可生产或消费数据而不必关心数据存于何处）
* Partition
Partiton是物理上的概念，每个Topic包含一个或多个Partition.
* Producer
负责发布消息到Kafka broker
* Consumer
消息消费者，向Kafka broker读取消息的客户端。
* Consumer Group
每个Consumer属于一个特定的Consumer Group（可为每个Consumer指定group name，若不指定group name则属于默认的group）
#### 6.5.2 Kafka准备工作
1. 安装Kakfa
请参考http://dblab.xmu.edu.cn/blog/1096-2/
* 这里假定已经成功安装Kafka到"/usr/local/kafka"目录下
2. 启动Kakfa
说明：本课程下载的安装文件为Kafka-2.11-0.8.2.2.tgz，前面的2.11就是该Kafka所支持的Scala版本号，后面的0.8.2.2是Kafka自身的版本号
打开一个终端，输入下面命令启动Zookeeper服务：
```
$ cd /usr/local/kafka
$ ./bin/zookeeper-server-start.sh config/zookeeper.properties
```
千万不要关闭这个终端窗口，一旦关闭，Zookeeper服务就停止了
打开第二个终端，然后输入下面命令启动Kafka服务：
```
$ cd /usr/local/kafka
$ bin/kafka-server-start.sh config/server.properties
```
千万不要关系这个终端窗口，一旦关闭，Kafka服务就停止了
3. 测试Kafka是否正常工作
再打开第三个终端，然后输入下面命令创建一个自定义名称为"wordsendertest"的Topic:
```shell
$ cd /usr/local/kafka
$ ./bin/kafka-topics.sh --create --zookeeper localhost:2181 >--replication-factor 1 --partitions 1 --topic wordsendertest
#可以用list列出所有创建的Topic，验证是否创建成功
$ ./bin/kafka-topics.sh --list --zookeeper localhost:2181
```
下面用生产者（Producer）来产生一些数据，请在当前终端内继续输入下面命令：
```shell
$ ./bin/kafka-console-producer.sh --broker -list localhost:9092 > --topic wordsendertest
```
上面命令执行后，就可以在当前终端内用键盘输入一些英文单词，比如可以输入：
```
hello hadoop
hello spark
```
现在可以启动一个消费者，来查看刚才生产者产生的数据。请另外打开第四个终端，输入下面命令：
```
$ cd /usr/local/kafka
$ ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 >--topic wordsendertest --from-beginning
```
可以看到，屏幕上会显示如下结果，也就是刚才在另外一个终端里面输入的内容：
```
hello hadoop
hello spark
```
#### 6.5.3 Spark准备工作
**1. 添加相关jar包**

Kafka和flume等高级输入源，需要依赖独立的库（jar文件）

对于Spark2.4.0版本，如果要使用Kafka，则需要下载
spark-streaming-kafka-0-8_2.11相关jar包
spark-streaming-kafka-0-8_2.11-2.4.0.jar
下载地址：
http://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8_2.11/2.4.0
把jar文件复制到Spark目录的jars目录下

```shell
$ cd /usr/local/spark/jars
$ mkdir kafka
$ cd ~
$ cd 下载
$ cp ./spark-streaming-kafka-0-8_2.11-2.4.0.jar
/usr/local/spark/jars/kafka
```

继续把Kafka安装目录的libs目录下的所有jar文件复制到"/usr/local/spark/jars/kafka"目录下，请在终端中执行下面命令：

```shell
$ cd /usr/local/spark/conf
$ vim spark-env.sh
```

然后，修改Spark配置文件，命令如下：
```shell
$ cd /usr/local/spark/conf
$ vim spark-env.sh
```
把Kafka相关jar包的路径信息增加到spark-env.sh，修改后的spark-env.sh类似如下：
```shell
export SPARK_DIST_CLASSPATH=$(/usr/lcoal/hadoop/bin/hadoop
classpath):$(/usr/local/hbase/bin/hbase
classpath):/usr/local/spark/jars/hbase/*:/usr/local/spark/examples/jars/*:/usr/local/spark/jars/kafka/*:/usr/local/kafka/libs/*
```
#### 6.5.4 编写Spark Streaming程序使用Kafka数据源
KafkaWordCount.py
```python
#!/usr/bin/env pyhton3

from __future__ import print_function
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils

if __name__="__main__":
if len(sys.argv)!=3:
    print("Usage: KafkaWordCount.py <zk> <topic>",file=sys.stderr)
    exit(-1)
sc=SparkContext(appName="PythonStreamingKafkaWordCount")
ssc=StreamingContext(sc,1)
zkQuorum,topic=sys.argv[1:]
kvs=KafkaUtils.createStream(ssc,zkQuorum,"spark-streaming-consumer",{topic:1})
lines=kvs.map(lambda x:x[1])
counts=lines.flatMap(lambda line: line.split("")).map(lambda word:(word,1)).reduceByKey(lambda a,b:a+b)
counts.pprint()
ssc.start()
ssc.awaitTermination()
```
然后，新建一个终端（记作"流计算终端"），执行KafkaWordCount.py，命令如下：
```shell
$ cd /usr/local/spark/mycode/streaming/kafka/
$ /usr/local/spark/bin/spark-submit >./KafkaWordCount.py localhost:2181 wordsendertest
```
这时再切换到之前已经打开的“数据源终端”，用键盘手动敲入一些英文单词在流计算终端内就可以看到类似如下的词频统计动态结果
```shell
-----------------------------------------------
Time:2018-12-31 10:40:42
-----------------------------------------------
('hadoop',1)
-----------------------------------------------
Time:2018-12-31 10:40:43
-----------------------------------------------
('spark',1)
```
### 6.6 转换操作
#### 6.6.1 DStream无状态转换操作
* `map(func)`: 对源DStream的每个元素，采用func函数进行转换，得到一个新的DStream
* `flatMap(func)`: 与map相似，但是每个输入项可用被映射为0个或者多个输出项
* `filter(func)`: 返回一个新的DStream，仅包含源DStream中满足函数func的项
* `repartition(numPartitions)`: 通过创建更多或更少的分区改变DStream的并行程度
* `reduce(func)`: 利用函数func聚集源DStream中每个RDD的元素，返回一个包含单元素RDDs的新DStream
* `count()`: 统计源DStream中每个RDD的元素数量
* `union(otherStream)`: 返回一个新的DStream，包含源DStream和其它DStream的元素
* `countByValue()`: 应用于元素类型为K的DStream上，返回一个（K,V）键值对类型的新DStream，每个键的值是在原DStream的每个RDD中的出现次数
* `reduceByKey（func，[numTasks]）`:当在一个由（K,V）键值对组成的DStream上执行该操作时，返回一个新的由（K,V）键值对组成的DStrem，每一个key的值均由给定的reduce函数（func）聚集起来
* `join(otherStream,[num Tasks])`: 当应用于两个DStream（一个包含（K,V）键值对，一个包含（K,W）键值对），返回一个包含（K,（V,W））键值对的新Dstream
* `cogroup(otherStream,[numTasks])`: 当应用于两个DStream（一个包含（K,V））键值对，一个包含（K,W）键值对），返回一个包含（K,Seq[V],Seq[W]）的元组
* `transform(func)`：通过对源DStream的每个RDD应用RDD-to-RDD函数，创建一个新的DStream。支持在新的DStream中作任何RDD操作

**无状态转换操作实例：**

之前“套接字流”部分介绍的词频统计，就是采用无状态转换，每次统计，都是只统计当前批次到达的单词的词频，和之前批次无关，不会进行累计。

#### 6.6.2 DStream有状态转换操作
**1. 滑动窗口转换操作**

* 事先设定一个滑动窗口的长度（也就是窗口的持续时间）

* 设定滑动窗口的时间间隔（每隔多长时间执行一次计算），让窗口按照指定时间间隔在源DStream上滑动

* 每次窗口停放的位置上，都会有一部分Dstream（或者一部分RDD）被框入窗口内，形成一个小段的Dstream

* 可以启动对这个小段DStream的计算

* `window(windowLength,slideInterval)`：基于源DStream产生的窗口化的批数据，计算得到一个新的DStream

**一些窗口转换操作的含义：**

* `countByWindow(windowLength,slideInterval)`：返回流中元素的一个滑动窗口数

* `reduceByWindow(func,windowLength,slideInterval)`：返回一个单元素流。利用函数func聚集滑动时间间隔的流的元素创建这个单元素流。函数func必须满足结合律，从而可以支持并行计算

* `countByValueAndWindow(windowLength,slideInterval,[numTasks])`：当应用到一个（K,V）键值对组成的DStream上，返回一个由（K,V）键值对组成的新的DStream。每个key的值都是它们在滑动窗口中出现的频率。

* `reduceByKeyAndWindow(func,windowLength,slideInterval,[numTasks])`：应用到一个（K,V）键值对组成的DStream上时，会返回一个由（K,V）键值对组成的新的DStream。每一个key的值均由给定的reduce函数（func函数）进行聚合计算。注意：在默认情况下，这个算子利用了Spark默认的并发任务数去分组。可以通过numTasks参数的设置来指定不同的任务数

* `reduceByKeyAndWindow(func,invFunc,windowLength,slideInterval,[numTasks])`：更加高效的`reduceByKeyAndWindow`, 每个窗口的reduce值，是基于先前窗口的`reduce` 值进行增量计算得到的；它会对进入滑动窗口的新数据进行`reduce`操作，并对离开窗口的老数据进行“逆向reduce”操作。但是，只能用于“可逆reduce函数”，即那些reduce函数都有一个对应的“逆向reduce函数”（以InvFunc参数传入）

* WindowedNetworkWordCount.py

  ```shell
  #!/usr/bin/env python3
  from __future__ import print_function
  import sys
  from pyspark import SparkContext
  form pyspark.streaming import StreamingContext
  if __name__=="__main__":
      if len(sys.argv)!=3:
      print("Usage: WindowedNetworkWordCount.py <hostname> <port>",file=sys.stderr)
      exit(-1)
      sc=SparkContext(appName="PythonStreamingWindowedNetworkWordCount")
      ssc=StreamingContext(sc,10)
      ssc.checkpoint("file:///usr/local/spark/mycode/streaming/socket/checkpoint")
      lines=ssc.socketTextStream(sys.argv[1],int(sys.argv[2]))
      counts=lines.flatMap(lambda line:line.split("")).map(lambda word: (word,1)).reduceByKeyAndWindow(lambda x,y:x+y,lambda x,y:x-y,30,10)
      ssc.pprint()
      ssc.start()
      ssc.awaitTermination()
      reduceByKeyAndWindow(lambda x,y:x+y,lambda x,y:x-y,30,10)
  ```

为了测试程序的运行效果，首先新建一个终端（记作“数据源终端”），执行如下命令运行nc程序：

```shell
$ cd /usr/local/spark/mycode/streaming/socket/
$ nc -lk 9999
```

再新建一个终端（记作“流计算终端”），运行客户端程序WindowedNetworkWordCount.py，命令如下：
```
$ cd /usr/local/spark/mycode/streaming/socket/
$ /usr/local/spark/bin/spark-submit >WindowedNetworkWordCount.py localhost 9999
```
在数据源终端内，用键盘连续敲入10个“hadoop”，每个hadoop单独占一行（即输入一个hadoop就回车），再用键盘连续敲入10个“spark”，每个spark单独占一行。这时，可以查看流计算终端内显示的词频动态统计结果，可以看到，随着时间的流逝，词频统计结果会发生动态变化。

**2. updateStateByKey操作**

需要在跨批次之间维护状态时，就必须使用updateStateByKey操作

词频统计实例：

对于有状态转换操作而言，本批次的词频统计，会在之前批次的词频统计结果的基础上进行不断累加，所以，最终统计得到的词频，是所有批次的单词的总的词频统计结果

NetworkWordCountStateful.py

```python
#!/usr/bin/env python3
from __future__ import print_function
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
if __name__=="__main__":
    if len(sys.argv)!=3:
    print("Usage: NetworkCountStateful.py <hostname><port>",file=sys.stderr)
    exit(-1)
    sc=SparkContext(appName="PythonStreamingStatefulNetworkWordCount")
    ssc=StreamingContext(sc,1)
    ssc.checkpoint("file:///usr/local/spark/mycode/streaming/stateful/")
    # RDD with initial state (key,value) pairs
    initialStateRDD=sc.parallelize(u"hello",1),(u'world',1)])
    def updateFunc(new_values,last_sum):
        return sum(new_values)+(last_sum or 0)
        
    lines=ssc.socketTextStream(sys.argv[1],int(sys.argv[2]))
    running_counts=lines.flatMap(lambda line:line.split("")).map(lambda word: (word,1)).updateStateByKey(updateFunc,initialRDD=initialStateRDD)
    running_counts.pprint()
    ssc.start()
    ssc.awaitTermination()
```
新建一个终端（记作“数据源终端”），执行如下命令启动nc程序：
```shell
$ nc -lk 9999
```
新建一个Linux终端（记作“流计算终端”），执行如下命令提交运行程序：
```shell
$ cd /usr/local/spark/mycode/streaming/stateful
$ /usr/local/spark/bin/spark-submit >NetworkWordCountStateful.py localhost 9999
```
在数据源终端内手动输入一些单词并回车，再切换到流计算终端，可以看到已经输入了类似入下的词频统计信息：
```
--------------------------------------------------
Time: 2018-12-30 20:53:02
--------------------------------------------------
('hadoop',1)
('world',1)
('hello',1)
('spark',1)
--------------------------------------------------
```
### 6.7 输出操作
在Spark应用中，外部系统经常需要使用到Spark DStream处理后的数据，因此，需要采用输出操作把DStream的数据输出到数据库或者文件系统中
#### 6.7.1 把DStream输出到文本文件中
```python
#!/usr/bin/env python3
from __future__ import print_function
import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
if __name__=="__main__":
    if len(sys.argv)!=3:
    print("Usage: NetworkCountStateful.py <hostname><port>",file=sys.stderr)
    exit(-1)
    sc=SparkContext(appName="PythonStreamingStatefulNetworkWordCount")
    ssc=StreamingContext(sc,1)
    ssc.checkpoint("file:///usr/local/spark/mycode/streaming/stateful/")
    # RDD with initial state (key,value) pairs
    initialStateRDD=sc.parallelize(u"hello",1),(u'world',1)])
    def updateFunc(new_values,last_sum):
        return sum(new_values)+(last_sum or 0)
    lines=ssc.socketTextStream(sys.argv[1],int(sys.argv[2]))
    running_counts=lines.flatMap(lambda line:line.split("")).map(lambda word: (word,1)).updateStateByKey(updateFunc,initialRDD=initialStateRDD)
    running_counts.saveAsTextFiles("file:///usr/local/spark/mycode/streaming/stateful/output")
    running_counts.pprint()
    ssc.start()
    ssc.awaitTermination()
```
#### 6.7.2 把DStream写入到MySQL数据库中

启动MySQL数据库，并完成数据库和表的创建：

```shell
$ server mysql start
$ mysql -u root -p\
$ #屏幕会提示你输入密码
```
在此前已经创建好的“spark”数据库中创建一个名称为“worldcount”的表：
```shell
mysql> use spark
mysql> create table wordcount (word char(20),count int(4));
```
由于需要让Python连接数据库MySQL,所以，需要首先安装Python连接MySQL的模块PyMySQL，请在Linux终端中执行如下命令：
```shell
$ sudo apt-get update\
$ sudo apt-get install python3-pip
$ pip3 -V
$ sudo pip3 install PyMySQL
```
NetworkWordCountStatefulDB.py
``` python
#!/usr/bin/env python3
from __future__ import print_function
import sys
import pymysql
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

if __name__=="__main__":
    if len(sys.argv)!=3
    print("Usage: NetworkWordCountStateful <hostname> <port>",file=sys.stderr)
    exit(-1)
    sc=SparkContext(appName="PythonStreamingStatefulNetworkWordCount")
    ssc=StreamingContext(sc,1)
    ssc.checkpoint("file:///usr/local/spark/mycode/streaming/stateful")
    # RDD with initial state (key, value) pairs
    initialStateRDD=sc.parallelize([u'hello',1),(u'world',1)])
    
    def updateFunc(new_values,last_sum):
        return sum(new_values)+(last_sum or 0)
    
    lines=ssc.socketTextStream(sys.argv[1],int(sys.argv[2]))
    running_counts=lines.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).updateStateByKey(updateFunc,initialRDD=initialStateRDD)
    running_counts.pprint()
    
    def dbfunc(records):
        db=pymyslq.connect("localhost","root","123456","spark")
        cursor=db.cursor()
        def doinsert(p):
            sql="insert into wordcount(word,count) vlaues('%s','%s')%(str(p[0]),str(p[1]))"
            try:
                cursor.execute(sql)
                db.commit()
            except:
                db.rollback()
       for item in records:
            doinsert(item)
            
    def func(rdd):
        repartitionedRDD=rdd.repartition(3)
        repartitionedRDD.foreachPartition(dbfunc)
        
    running_counts.foreachRDD(func)
    ssc.start()
    ssc.awaitTermination()
```
## 7. Structured Streaming
### 7.1 概述
#### 7.1.1 概述
* Structured Streaming的关键思想是将实时数据流视为一张正在不断添加数据的表
* 可以把流数据等同于在一个静态表上的批处理查询，Spark会在不断添加数据的无界输入表上运行计算，并进行增量查询
* 在无界表上对输入的查询将生成结果表，系统每隔一定的周期会触发对无界表的计算并更新结果表
#### 7.1.2 两种处理模型
（1）微批处理
* Structured Streaming默认使用微批处理执行模型，这意味着Spark流计算引擎会定期检查流数据源，并对自上一批次后到达的新数据执行批量查询

* 数据到达和得到处理并输出结果之间的延时超过100毫秒

（2）持续处理

* Spark从2.3.0版本开始引入了持续处理的试验性功能，可以实现流计算的毫秒级延迟

* 在持续处理模式下，Spark不再根据触发器来周期性启动任务，而是启动一系列的连续读取、处理和写入结果的长时间运行的任务
#### 7.1.3 Structured Streaming和Spark SQL、Spark Streaming关系
* Structed Streaming处理的数据跟Spark Streaming一样，也是源源不断的数据流，区别在于，Spark Streaming采用的数据抽象是DStream(本质上就是一系列RDD），而Structed Streaming采用的数据抽象是DataFrame。
* Structed Streaming可以使用Spark SQL的DataFrame/Dataset来处理数据流。虽然Spark SQL也是采用DataFrame作为数据抽象，但是，Spark SQL只能处理静态的数据，而Structured Streaming可以处理结构化的数据流。这样，Structured Streaming就将Spark SQL和Spark Streaming二者的特性结合了起来。
* Structured Streaming可以对DataFrame/Dataset应用前面章节提到的各种操作，包括select、where、groupBy、map、filter、flatMap等。
* Spark Streaming只能实现秒级的实时响应，而Structured Streaming由于采用了全新的设计方式，采用微处理模型时可以实现100毫秒级别的实时响应，采用持续处理模型时可以支持毫秒级的实时响应。
### 7.2 编写Structured Streaming程序的基本步骤
编写Structed Streaming程序的基本步骤包括：

<font color='red'>实例任务</font>：一个包含很多行英文语句的数据流源源不断到达，Structured Streaming程序对每行英文语句进行拆

分，并统计每个单词出现的频率

* 导入pyspark模块
  导入pySpark模块，代码如下：

  ```python
  from pyspark.sql import SparkSession
  from pyspark.sql.function import split
  from pyspark.sql.funcitons import explode
  ```

由于程序中需要用到拆分字符串和展开数组内的所有单词的功能，所以引用了来自 `pyspark.functions`里面的`split`和`explode`函数
* 创建SparkSession对象
  创建一个SparkSession对象，代码如下：

  ```python
  if __name__=="__main__":
      spark=SparkSession.builder.appName("StructureNetWordCount").getOrCreate()
      spark.sparkContext.setLogLevel("WARN")
  ```
* 创建输入数据源
  创建一个输入数据源，从“监听在本机（localhost）的9999端口上的服务”那里接收文本数据，具体语句下：

  ```python
  lines=spark.readStream.format("socket").option("host","localhost").option("port",9999).load()
  ```
* 定义流计算过程
  有了输入数据源以后，接着需要定义相关的查询语句，具体如下：

  ```python
   words=lines.select(explode(split(lines.value," ")).alias("word"))
   wordCounts=words.groupBy("word").count()
  ```
* 启动流计算并输出结果
  定义完查询语句后，下面就可以开始真正执行流计算，具体语句如下：

  ```python
  query=wordCounts.writeStream.outputMode("complete").format("console").trigger(processingTime="8 seconds").start()
  query.awaitTermination()
  ```

把代码写入文件StructuredNetworkWordCount.py

在执行StructuredNetworkWordCount.py之前，需要启动HDFS。

启动HDFS的命令如下：

```shell
$ cd /usr/local/hadoop
$ sbin/start-dfs.sh
```

新建一个终端（记作“数据源终端”），输入如下命令：
```shell
$ nc -lk 9999
```
再新建一个终端（记作“流计算终端”），执行如下命令：
```shell
$ cd /usr/local/spark/mycode/structuredstreaming/
$ cd /usr/local/spark/bin/spark-submit StructuredNetworkWordCount.py
```
为了模拟文本数据流，可以在“数据源终端”内用键盘不断敲入一行行英文语句，nc程序会把这些数据发送给StructureNetworkWordCount.py程序进行处理，比如输入如下数据：
```shell
apahce spark
apache hadoop
```
则在“流计算终端”窗口内会输出类似以下的结果信息：
```shell
----------------------------------------------------
Batch: 0
----------------------------------------------------
+------+------+
| word|count|
+------+------+
|apache|   1|
| spark|   1|
+------+------+
```
```shell
----------------------------------------------------
Batch: 1
----------------------------------------------------
+------+------+
| word|count|
+------+------+
|apache|   2|
| spark|   1|
|hadoop|    1|
+------+------+
```
### 7.3 输入源
#### 7.3.1 File源

* File源（或称为“文件源”）以文件流的形式读取某个目录中的文件，支持的文件格式为csv、json、orc、parquet、text等。
* 需要注意的是，文件放置到给定目录的操作应当是原子性的，即不能长时间在给定目录内打开文件写入内容，而是应当采取大部分操作系统都支持的、通过写入到临时文件后移动文件到给定目录的方式来完成。

<font color='red'>一个实例</font>
这里一个JSON格式文件的处理来演示File源的使用方法，主要包括以下两个步骤：

* 创建程序生成JSON格式的File源测试数据

* 创建程序对数据进行统计
  

**1. 创建程序生成JSON格式的File源测试数据**

 为了演示JSON格式文件的处理，这里随机生成一些JSON格式的文件来进行测试。代码文件spark_ss_filesource_generate.py内容如下：

  ```python
  #!/usr/bin/env python3
  # _*_ coding: utf-8 _*_
  
  # 导入需要用到的模块
  import os
  import shutil
  import random
  import time
  TEST_DATA_TEMP_DIR='/tmp/'
  TEST_DATA_DIR='/tmp/testdata/'
  ACTION_DEF=['login','logout','purchase']
  DISTRICT_DEF=['fujian','beijing','shanghai','guangzhou']
  JSON_LINE_PATTERN='{{"eventTime":{},"aciton":"{}","district":"{}"}}\n'
  # 测试的环境搭建，判断文件夹是否存在，如果存在则删除旧数据，并建立文件夹
  def test_setUp():
      if os.path.exists(TEST_DATA_DIR):
          shutil.rmtree(TEST_DATA_DIR,ignore_errors=True)
      os.mkdir(TEST_DATA_DIR)
  # 测试环境的恢复，对文件夹进行清理
  def test_tearDown():
      if os.path.exists(TEST_DATA_DIR):
          shutil.rmtree(TEST_DATA_DIR,ignore_errors=True)
  # 生成测试文件
  def write_and_move(filename,data):
      with open(TEST_DATA_TEMP_DIR+filename,"wt",encoding="utf-8") as f:
          f.write(data)
      shutil.move(TEST_DATA_TEMP_DIR+filename,TEST_DATA_DIR+filename)
  if __name__=="__main__":
      test_setUp()
      
      for i in range(1000):
          filename='e-mail-{}.json'.format(i)
          content=""
          rndcount=list(range(100))
          random.shutffle(rndcount)
          for _ in rndcount:
              content+=JSON_LINE_PATTERN.format(
                  str(int(time.time())),
                  random.choice(ACTION_DEF),
                  random.choice(DISTRICT_DEF))
          write_and_move(filename,content)
          
          time.sleep(1)
          
      test_tearDown()
  ```

>  这段程序首先建立测试环境，清空测试数据所在的目录，接着使用for循环一千次来生成一千个文件，文件名为“e-mail-数字.json”，文件内容是不超过100行的随机JSON行，行的格式是类似如下：
{"eventTime":1546939167,"aciton":"logout","district":"fujian"}

**2. 创建程序对数据进行统计**

spark_ss_filesource.py，其代码内容如下：

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# 导入需要用到的模块
import os
import shutil
from pprint import pprint

from pyspark.sql import SparkSession
from pyspark.sql.functions import window.asc
from pyspark.sql.types import StructType, StructField
from pyspark.sql.types import TimestampType, StringType
# 定义JSON文件的路径常量
TEST_DATA_DIR_SPARK="file:///tmp/testdata/"
if __name__=="__main__":
    # 定义模式，为时间戳类型的eventTime、字符串类型的操作和省份组成
    schema=StructType([
        StructField("eventTime",TimestampType(),True),
        StructField("aciton",StringType(),True),
        StructField("district",StringType(),True)])
        
    spark=SparkSession.builder.appName("StructuredEMallPurchaseCount").getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    lines=spark.readStream.format("json").schema(schema).option("maxFilesPerTrigger",100).load(TEST_DATA_DIR_SPARK)
# 定义窗口
windowDuration='1 minutes'
windowedCounts=lines.filter("action='purchase'").groupBy('district',window('eventTime',windowDuration)).count().sort(asc('window'))
query=windowedCounts.writeStream.outputMode("complete").format("console").option('truncate','false').trigger(processingTime="10 seconds").start()
query.awaitTermination()
```
**3. 测试运行程序**

程序运行过程需要访问HDFS，因此，需要启动HDFS，命令如下：

```shell
$ cd /usr/local/hadoop
$ sbin/start-dfs.sh
```
新建一个终端，执行如下命令生成测试数据：
```shell
$ cd /usr/local/spark/mycode/structuredstreaming/file
$ python3 spark_ss_filesource_generate.py
```
新建一个终端，执行如下命令运行数据统计程序：
```shell
$ cd /usr/local/spark/mycode/structuredstreaming/file
$ /usr/local/spark/bin/spark-submit spark_ss_filesource.py
```
运行程序以后，可以看到类似如下的输出结果：

![image.png-129.4kB][50]

#### 7.3.2 Kafka源

Kafka源是流处理最理想的输入源，因为它可以保证实时和容错

<font color='red'>实例演示</font>

在这个实例中，使用生产者程序每0.1秒生成一个包含2个字母的单词，并写入Kafka的名称为“wordcount-topic”的主题(Topic)内。Spark的消费者程序通过订阅wordcount-topic，会源源不断收到单词，并且每隔8秒钟对收到的单词进行一次词频统计，把统计结果输出到Kafka的主题wordcount-result-topic内，同时，通过2个监控程序检查Spark处理的输入和输出结果。

**1. 启动Kafka**

在Linux系统中新建一个终端（记作“Zookeeper终端”），输入下面命令启动Zookeepr服务：

```shell
$ cd /usr/local/kafka
$ bin/zookeeper-server-start.sh config/zookeeper.properties
```

不要关闭这个终端窗口，一旦关闭，Zookeeper服务就停止了。另外打开第二个终端（记作“Kafka终端”），然后输入下面命令启动Kafka服务：

```shell
$ cd /usr/local/kafka
$ bin/kafka-server-start.sh config/server.properties
```

不要关闭这个终端窗口，一旦关闭，Kafka服务就停止了。
再新开一个终端（记作“监控输入终端”），执行如下命令监控Kafka收到的文本：

```shell
$ cd /usr/local/kafka
$ bin/kafka-console-consumer.sh > --bootstrap-server localhost:9092 --topic wordcount-topic
```

再新开一个终端（记作“监控输出终端”），执行如下命令监控输出的结果文本：

```shell
$ cd /usr/local/kafka
$ bin/kafka-console-consumer.sh > --bootstrap-server localhost:9092 --topic wordcount-result-topic
```

**2. 编写生产者（Producer）程序**

代码文件spark_ss_kafka_producer.py内容如下：

```shell
#!/usr/bin/env python3
import string
import random
import time

from kafka import KafkaProduce

if __name__=="__main__":
    producer=KafkaProducer(bootstrap_servers=['localhost:9092'])
    while True:
        s2=(random.choice(string.ascii_lowercase) for _ in range(2))
        word="".join(s2)
        value=bytearray(word,'utf-8')
        producer.send('wordcount-topic',value=value).get(timeout=10)
        time.sleep(0.1)
```
如果还没有安装Python3的Kafka支持，需要按照如下操作进行安装：

* 首先确认有没有安装pip3，如果没有，使用如下命令安装：

  ```shell
  $ sudo apt-get install pip3
  ```

* 安装kafka-python模块，命令如下：

  ```shell
  $ sudo pip3 install kafka-python
  ```

* 然后在终端中执行如下命令运行生产者程序：

  ```shell
  $ cd /usr/local/spark/mycode/structuredstreaming/kafka/
  $ python3 spark_ss_kafka_producer.py
  ```

> 生产者程序执行以后，在“监控输入终端”的窗口内就可以看到持续输出包含2个字母的单词

**3. 编写消费者（Consumer）程序**

* 代码文件spark_ss_kafka_consumer.py内容如下：

  ```python
  #!/usr/bin/env python3
  
  from pyspark.sql import SparkSession
  
  if __name__=="__main__":
      spark=SparkSession.builder.appName("StructuredKafkaWordCount").getOrCreate()
      spark.sparkContext.setLogLevel('WARN')
      lines=spark.readStream.format("kafka").option("kafka.bootstrap.servers","localhost:9092").option("subscribe",'wordcount-topic').load().selectExpr("CAST(value AS STRING)")
      wordCounts=lines.groupBy("value").count()
      query=wordCounts.selectExpr("CAST(value AS STRING) as key","CONCAT(CAST(value AS STRING),':',CAST(count AS STRING)) as value").
      writeStream
      .outputMode("complete")
      .format("kafka")
      .option("kafka.boostrap.servers","localhost:9092")
      .option("topic","wordcount-result-topic")
      .option("checkpointLocation","file:///tmp/kafka-sink-cp")
      .trigger(processingTime="8 seconds").start()
      query.awaitTermination()
  ```

* 在终端中执行如下命令运行消费者程序：

  ```python
  $ cd /usr/local/spark/mycode/structuredstreaming/kafka/
  $ /usr/local/spark/bin/spark-submit 
  > --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0 > spark_ss_kafka_consumer.py
  ```

* 消费者程序运行起来以后，可以在“监控输出终端”看到类似如下的输出结果：

  ```python
  sq:3
  bl:6
  lo:8
  ...
  ```

#### 7.3.3 Socket源
Socket源从一个本地或远程主机的某个端口服务上读取数据，数据的编码为UTF8。因为Socket源使用内存读取到的所有数据，并且远端服务不能保证数据在出错后可以使用检查点或者当前已处理的偏移量来重放数据，所以，它无法提供端到端的容错保障。Socket源一般仅用于测试或学习用途。

Socket源的实例可以参考7.2节的StructuredNetworkWordCount.py。

#### 7.3.4 Rate源
Rate源可每秒生成特定个数的数据行，每个数据行包括时间戳和值字段。时间戳是消息发送的时间，值是从开始到当前消息发送的总个数，从0开始。Rate源一般用来作为调试或性能基准测试。

代码文件spark_ss_rate.py内容如下：

```python
# !/usr/bin/env/ python3
from pyspark.sql import SparkSession

if __name__=="__main__":
    spark=SparkSession.builder.appName("TestRateStreamSource").getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    lines=spark.readStream.format("rate").option('rowPerSecond',5).load()
    print(liens.schema)
    query=lines.writeStream.outputMode("update").format("console").option('truncate','false').start()
    query.awaitTermination()
```
在Linux终端中执行如下命令执行spark_ss_rate.py:
```shell
$ cd /usr/local/spark/mycode/structuredstreaming/rate/
# /usr/local/spark/bin/spark-submit spark_ss_rate.py
```
上述命令执行后，会得到类似如下的结果：
```shell
StructType(List(StructField(timestamp,TimestampType,true),StructField(value,LongType,true)))
-------------------------------------------
Batch: 0
-------------------------------------------
+--------+------+
|timestamp|value|
+--------+------+
+--------+------+
------------------------------------------
Batch:1
------------------------------------------
+------------------------+------+
|timestamp      |value|
+------------------------+------+
|2018-10-01 15:42:38.595|0  |
|2018-10-01 15:42:38.795|1  |
|2018-10-01 15:42:38.995|2  |
|2018-10-01 15:42:39.195|2  |
|2018-10-01 15:42:39.395|3  |
+------------------------+------+
```
### 7.4 输出操作
#### 7.4.1 启动流计算
DataFrame/Dataset的.writeStream()方法将会返回DataStreamWriter接口，接口通过.start()真正启动流计算，并将DataFrame/Dataset写入到外部的输出接收器，DataStreamWriter接口有以下几个主要函数：

* format：接收器类型。
* outputMode：输出模式，指定写入接收器的内容，可以是Append模式、Complete模式或Update模式
* queryName：查询的名称，可选，用于标识查询的唯一名称。
* trigger：触发间隔，可选，设定触发间隔，如果未指定，则系统将在上一次处理完成后立即检查新数据的可用性。如果由于先前的处理尚未完成导致超过触发间隔，则系统将在处理完成后立即触发新的查询。

#### 7.4.2 输出模式
输出模式用于指定写入接收器的内容，主要有以下几种：
* Append模式：只有结果表中自上次触发间隔后增加的新行，才会被写入外部存储器。这种模式一般适用于“不希望更改结果表中现有行的内容”的使用场景。
* Complete模式：已更新的完整的结果表可被写入外部存储器。
* Update模式：只有自上次触发间隔后结果表中发生更新的行，才会被写入外部存储器。这种模式与Complete模式相比，输出较少，如果结果表的部分行没有更新，则不会输出任何内容。当查询不包括聚合时，这个模式等同于Append模式。
#### 7.4.3 输出接收器
系统内置的输出接收器包括File接收器、Kafka接收器、Foreach接收器、Console接收器、Memory接收器等，其中，Console接收器和Memory接收器仅用于调试用途。有些接收器由于无法保证输出的持久性，导致其不是容错的。
以File接收器为例，这里把7.2节的实例修改为使用File接收器，修改后的代码文件为StructuredNetworkWordCountFileSink.py

```python
#!/usr/bin/env python3
from pyspark.sql import SparkSession
from pyspark.sql.functions import split
from pyspark.sql.functions import explode
from pyspark.sql.functions import length
if __name__ == "__main__":
 spark = SparkSession.builder.appName("StructuredNetworkWordCountFileSink").getOrCreate()
 spark.sparkContext.setLogLevel('WARN')
 lines = spark.readStream.format("socket").option("host", "localhost").option("port", 9999).load()
 words = lines.select(explode(split(lines.value, " ")).alias("word"))
 all_length_5_words = words.filter(length("word") == 5)
 query = all_length_5_words.writeStream
 .outputMode("append") 
 .format("parquet") 
 .option("path", "file:///tmp/filesink") 
 .option("checkpointLocation", "file:///tmp/file-sink-cp") 
 .trigger(processingTime="8 seconds") 
 .start()
 query.awaitTermination()
```
在Linux系统中新建一个终端（记作“数据源终端”），输入如下命令：
```shell
$ nc -lk 9999
```
再新建一个终端（记作“流计算终端”），执行如下命令执行StructuredNetworkWordCountFileSink.py:
```shell
$ cd /usr/local/spark/mycode/structuredstreaming
$ /usr/local/spark/bin/spark-submit
StructuredNetworkWordCountFileSink.py
```
为了模拟文本数据流，可以在数据源终端内用键盘不断敲入一行行英文语句，并且让其中部分英语单词长度等于5
由于程序执行后不会在终端输出信息，这时可新建一个终端，执行如下命令查看File接收器保存的位置：

```shell
$ cd /tmp/filesink
$ ls
```
可以看到以parquet格式保存的类似入下的文件列表：
```shell
part-00000-2bd184d2-e9b0-4110-9018-a7f2d14602a9-c000.snappy.parquet
part-00000-36eed4ab-b8c4-4421-adc6-76560699f6f5-c000.snappy.parquet
part-00000-dde601ad-1b49-4b78-a658-865e54d28fb7-c000.snappy.parquet
part-00001-eedddae2-fb96-4ce9-9000-566456cd5e8e-c000.snappy.parquet
_spark_metadata
```
可以使用strings命令查看文件内的字符串，具体如下：
```shell
$ strings part-00003-89584d0a-db83-467b-84d8-53d43baa4755-c000.snappy.parquet
```
### 7.5 容错处理（自学）
### 7.6 迟到数据处理（自学）
### 7.7 查询的管理好监控（自学）



## 8. Spark MLib

### 8.1 Spark MLlib简介

#### 8.1.1 什么是机器学习

机器学习利用数据或以往的经验，以此优化计算机程序的性能标准

机器学习强调三个关键词：算法、经验、性能

#### 8.1.2 基于大数据的机器学习

* 机器学习算法涉及大量迭代计算
* 基于磁盘的MapReduce不适合进行大量迭代计算
* 基于内存的Spark比较适合进行大量迭代计算

#### 8.1.3 Spark 机器学习库MLlib

* pyspark的即席查询是一个关键。算法工程师可以边写代码边运行，边看结果。

* MLib只集成了能在集群上并行执行并运行良好的并行算法，也包括较新研究出来的算法，如果是小规模数据，最好采用单节点机器学习算法库(比如Weka)
* MLlib是包括<font color='red'>分类、回归、聚类、协同过滤、降维</font>等学习算法和工具组成的机器学习库，同时还包括底层的优化原语和高层的流水线(Pipeline)API，具体如下：
* <font color='red'>算法工具：</font>略
* <font color='red'>特征化工具：</font>特征提取、转化、降维和选择工具
* <font color='red'>流水线：</font>用于构建、评估和优化机器学习工具流的工具
* <font color='red'>持久性：</font>保存和加载算法、模型和管道
* <font color='red'>实用工具：</font>线性代数、统计、数据处理等工具

机器学习库从1.2版本后，被分为2个包：

* <font color='red'>spark.mllib：</font>包含基于RDD的原始算法API
* <font color='red'>spark.ml：</font>提供了基于DataFrame高层次的API和机器学习工作流式API套件

MLlib目前支持如下四种机器学习问题：

|            | 离散数据                                                     | 连续数据                                                     |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 监督学习   | Classification、LogisticRegression(with Elastic-Net)、SVM、DecisionTree、RandomForest、GBT、NaiveBayes、MultilayerPerceptron、OneVsRest | Regression、LinearRegression(with Elastic-Net)、DecisionTree、RandomFores、GBT、AFTSurvivalRegression、IsotonicRegression |
| 无监督学习 | Clustering、KMeans、GaussianMixture、LDA、PowerIterationClustering、BisectingKMeans | Dimensionality、Reduction、matrix factorization、PCA、SVD、ALS、WLS |

### 8.2 机器学习工作流

#### 8.2.1 机器学习流水线概念

* <font color='red'>DataFrame：</font>该数据集即是Spark SQL中的**DataFrame数据集**，被ML Pipeline用作数据存储，比如，DataFrame中的列可以是存储的文本、特征向量、真实标签和预测标签等，它可以容纳各种数据类型，比RDD多包含了schema信息，类似于传统数据库中的二维表

* <font color='red'>Transformer：</font>翻译为**转换器**，它是将DataFrame转化为新的DataFrame算法。比如一个模型就是一个Transformer，它可以将测试数据打上标签，技术上，Transformer实现了一个方法transform()，它给DataFrame附加列，从而变成另一个DataFrame
* <font color='red'>Estimator：</font>翻译成**估计器**或**评估器**，它是训练方法的概念抽象，在流水线中通常用于操作DataFrame数据并生成一个Transformer。技术上来说，Estimator 实现了一个方法fit()，它可以调用fit()，通过训练数据从而得到一个Transformer(即模型)
* <font color='red'>Parameter：</font>被用来设置Transformer或者Estimator的参数，所有的转换器和评估器均可共享用于指定参数的公共API。ParamMap是一组(参数，值)对
* <font color='red'>PipeLine：</font>翻译为**流水线**或者**管道**，流水线将多个工作流阶段（转换器和估计器）连接在一起，形成机器学习的工作流，并获得结果输出

#### 8.2.2 流水线工作过程

构建一个Pipeline流水线步骤如下：

1. 定义Pipeline中的各个<font color='red'>流水线阶段</font>PipelineStage(包括转换器和评估器)，如指标提取和转换模型训练等

2. 按照具体的处理逻辑有序地组织PipelineStages，并创建一个Pipeline

   ```shell
   >>> pipeline=Pipeline([stage1,stage2,stage3])
   ```

3. 将训练数据集作为输入参数，调用Pipeline实例的fit方法，开始以流的方式处理源训练数据，返回一个**PipelineModel**类实例，被用来预测测试数据的标签

* 流水线的各个阶段按顺序展开，输入的DataFrame在它通过每个阶段时被<font color='red'>转换</font>

> 值得注意的是，流水线自身也是估计器，运行fit()方法后，将产生一个PipelineModel，它是一个转换器，该管道模型在测试数据时使用。

#### 8.2.3 构建一个机器学习流水线

下面以<font color='red'>逻辑斯蒂回归为例</font>

**任务描述**

查找所有包含“spark"的句子，即将包含"spark"的句子的标签设为1，没有”spark"的句子的标签设为0

1. 需要使用<font color='red'>**SparkSession**对象</font>

* spark2.0版本以上会自动创建一个名为spark的SparkSession对象，若需手动创建，可借助伴生对象builder()方法，如下：

  ```python
  from pyspark.sql import SparkSession
  spark=SparkSession.builder.master("local").appName("WordCount").getOrCreate()
  ```

* pyspark.ml依赖numpy包，装包命令如下：

  ```shell
  sudo pip3 installl numpy
  ```

2. 引入要包含的包并构建训练数据集

   ```python
   from pyspark.ml import Pipeline
   from pyspark.ml.classification import LogisticRegression
   from pyspark.ml.feature import HashingTF, Tokenizer
   # Prepare training documents from a list of (id, text, label) tuples.
   training = spark.createDataFrame(
    [(0, "a b c d e spark", 1.0),
    (1, "b d", 0.0),
    (2, "spark f g h", 1.0),
    (3, "hadoop mapreduce", 0.0)],
    ["id", "text", "label"])
   ```

3. 定义Pipeline中的各个流水线阶段PipelineStage，包括转换器和评估器(tokenizer，hashingTF和Ir)

   ```python 
   tokenizer = Tokenizer(inputCol="text", outputCol="words")
   hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
   lr = LogisticRegression(maxIter=10, regParam=0.001)
   ```

4. 按照具体的处理逻辑有序地组织PipelineStages，并创建一个Pipeline。

   ```python
   pipeline=Pipeline(stages=[tokenizer,hasingTF,Ir])
   ```

* 现在构建的Pipeline本质上是一个Estimator，运行fit()后，产生一个PipelineModel，是一个Transformer

  ```python
  model=pipeline.fit(training)
  ```

  > 可以看到，model的类型是一个PipelineModel，这个流水线模型将在测试数据时使用

5. 构建测试数据

   ```python
   test = spark.createDataFrame(
    [(4, "spark i j k"),
    (5, "l m n"),
    (6, "spark hadoop spark"),
    (7, "apache hadoop")],
    ["id", "text"])
   ```

6. 调用之前训练好的PipelineModel的transform()方法，让测试数据按顺序通过拟合的流水线，生成预测结果

   ```python
   prediction = model.transform(test)
   selected = prediction.select("id", "text", "probability", "prediction")
   for row in selected.collect():
    rid, text, prob, prediction = row
    print("(%d, %s) --> prob=%s, prediction=%f" % (rid, text, str(prob), prediction))
   # 结果
   (4, spark i j k) --> prob=[0.155543713844,0.844456286156], prediction=1.000000
   (5, l m n) --> prob=[0.830707735211,0.169292264789], prediction=0.000000
   (6, spark hadoop spark) --> prob=[0.0696218406195,0.93037815938], prediction=1.000000
   (7, apache hadoop) --> prob=[0.981518350351,0.018481649649], prediction=0.000000
   ```

### 8.3 特征抽取、转化和选择

#### 8.3.1 特征提取

* <font color='red'>"词频-逆向文件频率"</font>（TF-IDF）是一种在文本挖掘中广泛使用的特征向量化方法，它可以体现一个文档中词语在语料库中的重要程度。

* 词语由t表示，文档由d表示，语料库由D表示，词频TF(t,d)是词语t在文档d中出现的次数，文件频率DF(t,D)是包含词语的文档的个数

* TF-IDF就是数值化文档信息，衡量词语能提供多少信息以区分文档。其定义如下：
  $$
  IDF(t,D)=log\frac{|D|+1}{DF(t,D)+1}
  $$
  TF-IDF度量值表示如下：
  $$
  TFIDF(t,d,D)=TF(t,d)\cdot IDF(t,D)
  $$

在Spark ML库中，TF-IDF被分为两部分：

* TF(+hashing)：HashingTF是一个Transformer，在文本处理中，将这些词条的集合转化为固定长度的特征向量，该算法在哈希的同时会统计各个词条的词频。
* IDF：IDF是一个Estimator，在一个数据集上应用它的fit()方法，产生一个IDFModel，该IDFModel接收特征向量（由HashingTF 产生），然后计算每一个词在文档中出现的频次。IDF会减少那些在语料库中出现频次较高的词的权重。

**过程描述:**

* 我们以一组句子开始
* 首先使用分解器Tokenizer把句子划分为单个词语
* 对每一个句子（词袋），使用HashingTF将句子转换为特征向量
* 最后使用IDF重新调整特征向量（这种转换通常可以提高使用文本特征的性能）

1. 导入TF-IDF所需要的包：

   ```python
   >>> from pyspark.ml.feature import HashingTF,IDF,Tokenizer
   ```

2. 创建一个简单的DataFrame，每一个句子代表一个文档

   ```
   >>> sentenceData = spark.createDataFrame([
   (0, "I heard about Spark and I love Spark"),
   (0, "I wish Java could use case classes"),
   (1, "Logisticregression models are neat")])
   .toDF("label", "sentence")
   ```

3. 得到文档集合后，即可用tokenizer对句子分词

   ```Python
   >>> tokenizer = Tokenizer(inputCol="sentence", outputCol="words")
   >>> wordsData = tokenizer.transform(sentenceData)
   >>> wordsData.show()
   +-----+--------------------+--------------------+
   |label| sentence| words|
   +-----+--------------------+--------------------+
   | 0|I heard about Spa...|[i, heard, about,...|
   | 0|I wish Java could...|[i, wish, java, c...|
   | 1|Logistic regressi...|[logistic, regres...|
   +-----+--------------------+--------------------+
   ```

4. 得到分词后的文档序列后，即可使用HasingTF的transform()把句子哈希成特征向量，这里设置哈希表的桶数为2000

   ```python
   >>> hashingTF = HashingTF(inputCol="words", outputCol="rawFeatures", numFeatures=2000)
   >>> featurizedData = hashingTF.transform(wordsData)
   >>> featurizedData.select("words","rawFeatures").show(truncate=False)
   +---------------------------------------------+-----------------------------+
   |words |rawFeatures |
   +---------------------------------------------+-----------------------------+
   |[i, heard, about, spark, and, i, love,spark]
   |(2000,[240,333,1105,1329,1357,1777],[1.0,1.0,2.0,2.0,1.0,1.0]) |
   |[i, wish, java, could, use, case, classes]
   |(2000,[213,342,489,495,1329,1809,1967],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|
   |[logistic, regression, models, are, neat]
   |(2000,[286,695,1138,1193,1604],[1.0,1.0,1.0,1.0,1.0]) |
   +---------------------------------------------+----------------------- -----+
   ```

5. 调用IDF方法重新构造特征向量的规模，生成的变量idf是一个评估器，应用fit()方法后，会产生一个IDFModel(名称为idfModel)

   ```python
   >>> idf = IDF(inputCol="rawFeatures", outputCol="features")
   >>> idfModel = idf.fit(featurizedData)
   ```

6. 调用IDFModel的`transform()`方法，可以得到每一个单词对应的TF-IDF度量值。

   ```python
   >>> rescaledData = idfModel.transform(featurizedData)
   >>> rescaledData.select("features", "label").show(truncate=False)
   +------------------------------------------------------------------------------+-----+
   |features|label|
   +------------------------------------------------------------------------------+-----+
   |(2000,[240,333,1105,1329,1357,1777],[0.6931471805599453,0.6931471805599453,1.386294361119890
   6,0.5753641449035617,0.6931471805599453,0.6931471805599453]) |0 |
   |(2000,[213,342,489,495,1329,1809,1967],[0.6931471805599453,0.6931471805599453,0.693147180559
   9453,0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453])|0|
   |(2000,[286,695,1138,1193,1604],[0.6931471805599453,0.6931471805599453,0.6931471805599453,0.69
   31471805599453,0.6931471805599453]) |1 |
   +------------------------------------------------------------------------------+-----+
   ```

#### 8.3.2 特征转换：标签和索引的转化

* 在机器学习中，为处理方便，经常需要将字符串标签转为<font color='red'>整数索引</font>，`StringIndexer`、`IndexToString`、`OneHotEncoder`、`VectorIndexer`均提供了特征转换功能。

> 值得注意的是，用于特征转换的转换器也是属于ML Pipeline模型的一部分，可用来构建机器学习流水线，如`StringIndexer`，存储着进行标签数值化过程的相关超参数，是一个`Estimator`，调用`fit()`方法，即可生成相应的模型。

**StringIndexer**

* 可以把一列类别型特征进行数值化编码，索引从0开始，构建的顺序为标签的频率，优先编码频率较大的标签，若输入的是数值型，则会首先转化为字符型，再进行编码

1. 引入所需使用的类

   ```python 
   >>> from pyspark.ml.feature import StringIndexer
   ```

2. 构建1个 `DataFrame`，设置 `StringIndexer` 的输入列和输出列的名字

   ```python
   >>> df = spark.createDataFrame([(0, "a"), (1, "b"), (2, "c"),
   (3, "a"), (4, "a"), (5, "c")],["id", "category"])
   >>> indexer = StringIndexer(inputCol="category",
   outputCol="categoryIndex")
   ```

3. 通过 `fit()` 方法进行模型训练，再对原数据集进行处理，通过 `indexed.show()` 展示

   ```python
   >>> model = indexer.fit(df)
   >>> indexed = model.transform(df)
   >>> indexed.show()
   +---+--------+-------------+
   | id|category|categoryIndex|
   +---+--------+-------------+
   | 0| a| 0.0|
   | 1| b| 2.0|
   | 2| c| 1.0|
   | 3| a| 0.0|
   | 4| a| 0.0|
   | 5| c| 1.0|
   +---+--------+-------------+
   ```

**IndexToString **

* 与 `StringIndexer` 相对应，`IndexToString` 的作用是把标签索引的一列重新映射回原有的字符型标签，一般会和<font color='red'>**StringIndexer**</font>配合，先用 ` StringIndexer` 将标签转化成标签索引，进行模型训练，然后在预测标签的时候再把标签索引转化成原有的字符标签

  ```python
  >>> from pyspark.ml.feature import IndexToString, StringIndexer
  >>> toString = IndexToString(inputCol="categoryIndex",outputCol="originalCategory")
  >>> indexString = toString.transform(indexed)
  >>> indexString.select("id", "originalCategory").show()
  +---+----------------+
  | id|originalCategory|
  +---+----------------+
  | 0| a|
  | 1| b|
  | 2| c|
  | 3| a|
  | 4| a|
  | 5| c|
  +---+----------------+
  ```

**VectorIndexer **

* 之前介绍的 `StringIndexer` 只针对单类别特征，倘若所有特征都已经被组织在一个向量中，又想对其中某些单个分量进行处理时，Spark ML提供了 `VectorIndexer` 类来解决向量数据集中的类别型特征转换
* 通过为其提供 `maxCategories` 超参数，它可以自动识别哪些特征是类别型的，并且将原始值转换为类别索引。它基于不同特征值的数量来识别哪些特征需要被类别化，那些取值可能性最多不超过 `maxCategories` 的特征需要会被认为是类别型的

1. 引入所需要的类，并构建数据集

   ```python
   >>> from pyspark.ml.feature import VectorIndexer
   >>> from pyspark.ml.linalg import Vector, Vectors
   >>> df = spark.createDataFrame([ (Vectors.dense(-1.0, 1.0, 1.0),),(Vectors.dense(-1.0, 3.0, 1.0),), (Vectors.dense(0.0, 5.0, 1.0), )], ["features"])
   ```

2. 构建  `VectorIndexer` 转换器，设置输入和输出列，并进行模型训练

   ```python
   >>> indexer = VectorIndexer(inputCol="features", outputCol="indexed",
   maxCategories=2)
   >>> indexerModel = indexer.fit(df)
   ```

3. 通过 `VectorIndexerModel`的 `categoryMaps` 成员来获得被转换的特征及其映射，这里可以看到，共有两个特征被转换，分别是0号和2号。

   ```python
   >>> categoricalFeatures =indexerModel.categoryMaps.keys()
   >>> print ("Choose"+str(len(categoricalFeatures))+ "categorical features:"+str(categoricalFeatures)) Chose 2 categorical features: [0, 2]
   ```

4. 把模型应用于原有的数据，并打印结果

   ```python
   >>> indexed = indexerModel.transform(df)
   >>> indexed.show()
   +--------------+-------------+
   | features| indexed|
   +--------------+-------------+
   |[-1.0,1.0,1.0]|[1.0,1.0,0.0]|
   |[-1.0,3.0,1.0]|[1.0,3.0,0.0]|
   | [0.0,5.0,1.0]|[0.0,5.0,0.0]|
   +--------------+-------------+
   ```

#### 8.3.3 特征选择

### 8.4 分类与回归

#### 8.4.1 逻辑斯蒂回归分类器

逻辑斯蒂回归属于对数线性模型，因变量既可以是二分类的，也可以是多分类的。

**任务描述**

* iris数据集下载地址：http://dblab.xmu.edu.cn/blog/wpcontent/uploads/2017/03/iris.txt

* 数据集包含150个数据，分为3类，每类50个，每个数据包含4个属性，为便于理解，主要用后2个属性(花瓣的长度和宽度)进行分类。

**步骤**

1. 导入本地向量 `Vector`和 `Vectors`，导入所需要的类

   ```python
   >>> from pyspark.ml.linalg import Vector,Vectors
   >>> from pyspark.sql import Row,functions
   >>> from pyspark.ml.evaluation import MulticlassClassificationEvaluator
   >>> from pyspark.ml import Pipeline
   >>> from pyspark.ml.feature import IndexToString, StringIndexer, \
   ... VectorIndexer,HashingTF, Tokenizer
   >>> from pyspark.ml.classification import LogisticRegression, \
   ... LogisticRegressionModel,BinaryLogisticRegressionSummary,LogisticRegression
   ```

2. 定制函数，返回一个指定的数据，读取文本文件，第一个 `map`把每行的数据用","隔开，该数据集前四列为4个特征，最后一个为分类，将特征存储在 `Vector` 中，创建一个Iris模式的 RDD，再转化成dataframe，最后调用`show()` 方法来查看一下部分数据。

   ```python
   >>> def f(x):
   ... rel = {}
   ... rel['features']=Vectors.dense(float(x[0]),float(x[1]),float(x[2]),float(x[3]))
   ... rel['label'] = str(x[4])
   ... return rel
   >>> data = spark.sparkContext. \
   ... textFile("file:///usr/local/spark/iris.txt"). \
   ... map(lambda line: line.split(',')). map(lambda p: Row(**f(p))). \
   ... toDF()
   >>> data.show()
   +-----------------+-----------+
   | features| label|
   +-----------------+-----------+
   |[5.1,3.5,1.4,0.2]|Iris-setosa|
   |[4.9,3.0,1.4,0.2]|Iris-setosa|
   |[4.7,3.2,1.3,0.2]|Iris-setosa|
   |[4.6,3.1,1.5,0.2]|Iris-setosa|
   ………
   +-----------------+-----------+
   only showing top 20 rows
   ```

3. 分别获取标签列和特征列，进行索引并进行重命名

   ```python
   >>> labelIndexer = StringIndexer().setInputCol("label").setOutputCol("indexedLabel").\
   ... fit(data)
   >>> featureIndexer = VectorIndexer().setInputCol("features"). \
   ... setOutputCol("indexedFeatures"). \
   ... fit(data)
   ```

4. 设置 `LogisticRegression` 算法的参数。这里设置了循环次数为100次，规范化项为0.3等，具体可以设置的参数，可以通过 `explainParams()` 来获取，还能看到程序已经设置的参数的结果。

   ```python
   >>> lr = LogisticRegression(). \
   ... setLabelCol("indexedLabel"). \
   ... setFeaturesCol("indexedFeatures"). \
   ... setMaxIter(100). \
   ... setRegParam(0.3). \
   ... setElasticNetParam(0.8)
   >>> print("LogisticRegression parameters:\n" +lr.explainParams())
   ```

5. 设置一个 `IndexToString` 的转换器，把预测的类别重新转化成字符型的。构建一个机器学习流水线，设置各个阶段。上一个阶段的输出将是本阶段的输入。

   ```python
   >>> labelConverter = IndexToString(). \
   ... setInputCol("prediction"). \
   ... setOutputCol("predictedLabel"). \
   ... setLabels(labelIndexer.labels)
   >>> lrPipeline = Pipeline(). \
   ... setStages([labelIndexer, featureIndexer, lr, labelConverter])
   ```

6. 把数据集按照7:3进行划分。Pipeline本质上是一个评估器，当Pipeline调用 `fit()` 的时候就产生了一个 PipelineModel，它是一个转换器。然后，这个PipelineModel 就可以调用 `transform()` 来进行预测，生成一个新的DataFrame，即利用训练得到的模型对测试集进行验证。

   ```python
   >>> trainingData, testData = data.randomSplit([0.7, 0.3])
   >>> lrPipelineModel = lrPipeline.fit(trainingData)
   >>> lrPredictions = lrPipelineModel.transform(testData)
   ```

7. 输出预测结果，`select` 选择要输出的列，`collect` 获取所有行的数据，用 `foreach` 把每行打印出来。

   ```python
   >>> preRel = lrPredictions.select("predictedLabel","label","features","probability").\
       collect()
   >>> for item in preRel:
       print(str(item['label'])+','+ \
       str(item['features'])+'-->prob='+ \
       str(item['probability'])+',predictedLabel'+ \
       str(item['predictedLabel']))
   ```
   
8. 对训练的模型进行评估。创建一个 `MulticlassClassificationEvaluator` 实例，用 `setter` 方法把预测分类的列名和真实分类的列名进行设置，然后计算预测准确率。

   ```python
   >>> evaluator = MulticlassClassificationEvaluator(). \
       setLabelCol("indexedLabel"). \
       setPredictionCol("prediction")
   >>> lrAccuracy = evaluator.evaluate(lrPredictions)
   >>> lrAccuracy
   0.7774712643678161 #模型预测的准确率
   ```

9. 可以通过 `model` 来获取训练得到的逻辑斯蒂模型。`lrPipelineModel` 是一个 `PipelineModel`，因此，可以通过调用它的 `stages` 方法来获取模型，具体如下：

   ```python
   >>> lrModel = lrPipelineModel.stages[2]
   >>> print ("Coefficients: \n " + str(lrModel.coefficientMatrix)+ \
       "\nIntercept: "+str(lrModel.interceptVector)+ \
       "\n numClasses: "+str(lrModel.numClasses)+ \
       "\n numFeatures: "+str(lrModel.numFeatures))
   Coefficients:
   3 X 4 CSRMatrix
   (1,3) 0.4332
   (2,2) -0.2472
   (2,3) -0.1689
   Intercept: [-0.11530503231364186,-0.63496556499483,0.750270597308472]
   numClasses: 3
   numFeatures: 4
   ```

#### 8.4.2 决策树分类器

决策树是一种基本的分类与回归方法，决策树中的每个节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。学习时，利用训练数据，依据损失函数最小化原则建立决策树模型，预测时，对新的数据，利用决策树模型进行分类。

决策树学习通常包含3个步骤：特征选择、决策树的生成和决策树的剪枝

我们同样以irirs数据为例进行分析

1. 导入需要的包

   ```python 
   >>> from pyspark.ml.classification import DecisionTreeClassificationModel
   >>> from pyspark.ml.classification import DecisionTreeClassifier
   >>> from pyspark.ml import Pipeline,PipelineModel
   >>> from pyspark.ml.evaluation import MulticlassClassificationEvaluator
   >>> from pyspark.ml.linalg import Vector,Vectors
   >>> from pyspark.sql import Row
   >>> from pyspark.ml.feature import IndexToString,StringIndexer,VectorIndexer
   ```

2.  读取文本文件，我们把特征存储在Vector中，创建一个Iris模式的RDD，然后转化为dataframe

   ```python
   >>> def f(x):
       rel = {}
       rel['features']=Vectors.dense(float(x[0]),float(x[1]),float(x[2]),float(x[3]))
       rel['label'] = str(x[4])
       return rel
   >>> data = spark.sparkContext.textFile("file:///usr/local/spark/iris.txt"). \
       map(lambda line: line.split(',')). \
       map(lambda p: Row(**f(p))). \
       toDF()
   ```

3.  进一步处理特征和标签，把数据集随机分成训练集和测试集，其中训练集占70%

   ```python
   >>> labelIndexer = StringIndexer(). \
       setInputCol("label"). \
       setOutputCol("indexedLabel"). \
       fit(data)
   >>> featureIndexer = VectorIndexer(). \
       setInputCol("features"). \
       setOutputCol("indexedFeatures"). \
       setMaxCategories(4). \
       fit(data)
   >>> labelConverter = IndexToString(). \
       setInputCol("prediction"). \
       setOutputCol("predictedLabel"). \
       setLabels(labelIndexer.labels)
   >>> trainingData, testData = data.randomSplit([0.7, 0.3])
   
   ```

4. 创建决策树模型DecisionTreeClassiefier，通过setter的方法来设置决策树的参数，也可以用ParamMap来设置。这里仅需要设置特征列(FeaturesCol)和待预测列(LabelCol)。具体可以设置的参数通过explainParam()来获取。

   ```python
   >>> dtClassifier = DecisionTreeClassifier().setLabelCol("indexedLabel").setFeaturesCol("indexedFeatures")
   ```

5. 构建机器学习流水线(Pipeline)，在训练数据集上调用fit()进行模型训练，并在测试数据集上调用transform()方法进行预测。

   ```python
   >>> dtPipeline = Pipeline(). \
   .setStages([labelIndexer, featureIndexer, dtClassifier, labelConverter])
   >>> dtPipelineModel = dtPipeline.fit(trainingData)
   >>> dtPredictions = dtPipelineModel.transform(testData)
   >>> dtPredictions.select("predictedLabel", "label", "features").show(20)
   +---------------+---------------+-----------------+
   | predictedLabel| label| features|
   +---------------+---------------+-----------------+
   | Iris-setosa| Iris-setosa|[4.4,3.0,1.3,0.2]|
   | Iris-setosa| Iris-setosa|[4.6,3.4,1.4,0.3]|
   | Iris-setosa| Iris-setosa|[4.9,3.1,1.5,0.1]|
   | Iris-setosa| Iris-setosa|[5.0,3.2,1.2,0.2]|
   >>> evaluator = MulticlassClassificationEvaluator(). \
       setLabelCol("indexedLabel"). \
       setPredictionCol("prediction")
   >>> dtAccuracy = evaluator.evaluate(dtPredictions)
   >>> dtAccuracy
   0.9726976552103888 #模型的预测准确率
   ```

   





















































[1]: http://static.zybuluo.com/wangwenhui/47orcev79huab934cdup6rs3/image.png
[2]: http://static.zybuluo.com/wangwenhui/g4cieeu4w5j7qp3397tygq8y/image.png
[3]: http://static.zybuluo.com/wangwenhui/d75sl8ets9w0wkgfpgv46kiq/image.png
[4]: http://static.zybuluo.com/wangwenhui/zzyict28lpdu2fd45bn22liv/image.png
[5]: http://static.zybuluo.com/wangwenhui/v3n3n4l12p0r6wprcvm0jmuz/image.png
[6]: http://static.zybuluo.com/wangwenhui/8ytkhvevov3a5f71nuhc8bh2/image.png
[7]: http://static.zybuluo.com/wangwenhui/eone31ivugdg5mjio6l64sye/image.png
[8]: http://static.zybuluo.com/wangwenhui/9iqgzadpnjxawnmhwos7ppto/image.png
[9]: http://static.zybuluo.com/wangwenhui/flqbf9xfpso67m5jtrhxz9cp/image.png
[10]: http://static.zybuluo.com/wangwenhui/nxftefkahbq78gbq6bn0gxhi/image.png
[11]: http://static.zybuluo.com/wangwenhui/8nxfxgec87pli8izeaz1rzu5/image.png
[12]: http://static.zybuluo.com/wangwenhui/q1jk9a511rt2o2my5cukjrn8/image.png
[13]: http://static.zybuluo.com/wangwenhui/470icvt99jn18ymnbfxdw5km/image.png
[14]: http://static.zybuluo.com/wangwenhui/pvb0ghqt0c5ais4zp4r1jc9o/image.png
[15]: http://static.zybuluo.com/wangwenhui/exy4901xm5ip7mrd4fv1f4zk/image.png
[16]: http://static.zybuluo.com/wangwenhui/xz3zw6vdq0hz59l3csers13z/image.png
[17]: http://static.zybuluo.com/wangwenhui/ucvp5jski6v4v19wslrfatpi/image.png
[18]: http://static.zybuluo.com/wangwenhui/piz43qp3xyn5u1n4r0mxa8nz/image.png
[19]: http://static.zybuluo.com/wangwenhui/7ezyrfk3c0c6whgt0c5548ow/image.png
[20]: http://static.zybuluo.com/wangwenhui/zm51eebyug7egccni7jv740j/image.png
[21]: http://static.zybuluo.com/wangwenhui/w59u9nwruvpq9bbhdzt5t395/image.png
[22]: http://static.zybuluo.com/wangwenhui/yc7dsuj77tfx8uwziwbxfd9b/image.png
[23]: http://static.zybuluo.com/wangwenhui/ssfoq1jef37o0ugsr2zzwj6g/image.png
[24]: http://static.zybuluo.com/wangwenhui/btlecuwij3h9i0z6qcsjcnbn/image.png
[25]: http://static.zybuluo.com/wangwenhui/zfqe1h3eta4hblkw4nlw656v/image.png
[26]: http://static.zybuluo.com/wangwenhui/95fi7e7ou6xm09ak45rzkgjj/image.png
[27]: http://static.zybuluo.com/wangwenhui/9fo0ibo9ceqg8w5yghum7ju5/image.png
[28]: http://static.zybuluo.com/wangwenhui/lsglp5rohotzsq7ao2qrnrox/image.png
[29]: http://static.zybuluo.com/wangwenhui/sy1xgvkrvp1whr7yvkah1of1/image.png
[30]: http://static.zybuluo.com/wangwenhui/r3vo47akcbim0z1lwhvkk8eq/image.png
[31]: http://static.zybuluo.com/wangwenhui/83o04aisa3n6peco38jjuo43/%E7%BB%98%E5%9B%BE1.png
[32]: http://static.zybuluo.com/wangwenhui/rtiqivv0x5v251ngat9n5lso/%E7%BB%98%E5%9B%BE1.png
[33]: http://static.zybuluo.com/wangwenhui/y4eug884o88fpdx0qyk1lxjf/image.png
[34]: http://static.zybuluo.com/wangwenhui/jsd22gvwxc6vywavwe954wvp/image.png
[35]: http://static.zybuluo.com/wangwenhui/03lhib3tzt7kjn0mrvvtwh4l/image.png
[36]: http://static.zybuluo.com/wangwenhui/9we51ij3o5d0ff6h2314t0b9/image.png
[37]: http://static.zybuluo.com/wangwenhui/cw2otkyb2kqt02twew0ft75k/image.png
[38]: http://static.zybuluo.com/wangwenhui/c1h5sjp53euzz0w4den4l24c/image.png
[39]: http://static.zybuluo.com/wangwenhui/t7qhyb2f4ub5l9fg6xradilh/image.png
[40]: http://static.zybuluo.com/wangwenhui/tujqbt0wdlpkmbxk1qq9uwdo/image.png
[41]: http://static.zybuluo.com/wangwenhui/a6shfp5719hpto29vudtmd2w/image.png
[42]: http://static.zybuluo.com/wangwenhui/pcym85v5ucdf2sz6k4yi0o8z/image.png
[43]: http://static.zybuluo.com/wangwenhui/xdqlrmaq0wdqmbs9lv1bgb07/image.png
[44]: http://static.zybuluo.com/wangwenhui/n3db1qj8l8ep8k4pe0hny3g3/image.png
[45]: http://static.zybuluo.com/wangwenhui/q7u5n4fn15wc873cxuct2hlm/image.png
[46]: http://static.zybuluo.com/wangwenhui/05shutsppcnk4zgli8q6v1fi/image.png
[47]: http://static.zybuluo.com/wangwenhui/gl1bq8jix281rinvwpo7o15j/image.png
[48]: http://static.zybuluo.com/wangwenhui/q6yt9lgc1f154yi4a7jvnwin/image.png
[49]: http://static.zybuluo.com/wangwenhui/159lo88kjqnge1fjjqj1hzqq/%E7%BB%98%E5%9B%BE1.png
[50]: http://static.zybuluo.com/wangwenhui/e958xt689wxysg65swax8tn3/image.png